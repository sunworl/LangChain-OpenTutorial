{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Ollama Deep Researcher (Deepseek-R1)\n",
        "\n",
        "- Author: [Youngin Kim](https://github.com/Normalist-K)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial explores how to **build a fully local AI-powered research agent** using **Ollama** and **Deepseek-R1** , an open-source large language model. The research agent is designed based on **Iterative Demonstration-Based Retrieval-Augmented Generation (IterDRAG)** , a methodology that enhances complex query resolution through **iterative query decomposition, retrieval, and synthesis** . By leveraging this structured approach, we can enable AI to **autonomously refine queries, retrieve relevant documents, and synthesize  high-quality research outputs** â€” all while running **entirely on your local machine** .\n",
        "\n",
        "**DISCLAIMER** : This tutorial code is heavily based on **Ollama Deep Researcher** [link](https://github.com/langchain-ai/ollama-deep-researcher).\n",
        "\n",
        "### Key Technologies\n",
        "- **[Ollama](https://ollama.com)** : A local runtime for efficiently running open-source LLMs.\n",
        "- **[Deepseek-R1](https://github.com/deepseek-ai/DeepSeek-R1)** : A powerful open-source model optimized for reasoning and research.\n",
        "- **[IterDRAG (Iterative Demonstration-Based RAG)](https://arxiv.org/html/2410.04343v1#:~:text=To%20tackle%20this%20issue%2C%20we,used%20to%20generate%20intermediate%20answers.)** : A retrieval and generation method that improves AI-driven research by **breaking down complex queries into manageable sub-queries, retrieving relevant context, and synthesizing iterative answers** .\n",
        "\n",
        "---\n",
        "\n",
        "### **What Youâ€™ll Learn**\n",
        "ðŸ”¹ **How to set up Ollama & Deepseek-R1 for local AI research**  \n",
        "ðŸ”¹ **How to optimize Deepseek-R1 models**  \n",
        "ðŸ”¹ **How to implement an IterDRAG-based research workflow**  \n",
        "\n",
        "\n",
        "By the end of this tutorial, youâ€™ll be able to **build a fully local, AI-enhanced research agent** that applies **IterDRAG principles** to enable **incremental knowledge refinement, retrieval-aware generation, and dynamic query optimization**â€”all while maintaining full **privacy, speed, and control** over the research process. ðŸš€\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Getting Started with Ollama and DeepSeek-R1](#getting-started-with-ollama-and-deepseek-r1)\n",
        "- [Using ChatOllama with DeepSeek-R1](#using-chatollama-with-deepseek-r1)\n",
        "- [Using DeepSeek-R1](#using-deepseek-r1)\n",
        "- [Ollama Deep Researcher powered by IterDRAG](#ollama-deep-researcher-powered-by-iterdrag)\n",
        "- [Defining the Deep Researcher Nodes](#defining-the-deep-researcher-nodes)\n",
        "- [Running the Deep Researcher Graph](#running-the-deep-researcher-graph)\n",
        "\n",
        "### References\n",
        "\n",
        "- [LangChain](https://blog.langchain.dev/)\n",
        "- [ollama-deep-researcher](https://github.com/langchain-ai/ollama-deep-researcher)\n",
        "- [Ollama](https://ollama.com)\n",
        "- [Deepseek-R1](https://github.com/deepseek-ai/DeepSeek-R1)\n",
        "- [IterDRAG (Iterative Demonstration-Based RAG)](https://arxiv.org/html/2410.04343v1#:~:text=To%20tackle%20this%20issue%2C%20we,used%20to%20generate%20intermediate%20answers.)\n",
        "- [substratus's blog](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm)\n",
        "- [Language Models & Co.](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1?utm_campaign=post&utm_medium=web)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langgraph\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-ollama\",\n",
        "        \"langchain_community\",\n",
        "        \"tavily-python\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can set API keys in a ` .env` file or set them manually.\n",
        "\n",
        "[Note] If youâ€™re not using the ` .env` file, no worries! Just enter the keys directly in the cell below, and youâ€™re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "327c2c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"LangGraph-Ollama-Deep-Researcher\",\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6bf75b1",
      "metadata": {},
      "source": [
        "## Getting Started with Ollama and DeepSeek-R1\n",
        "\n",
        "Ollama allows us to run **Deepseek-R1** (or other models) **directly on a local machine** , removing the need for cloud-based APIs.  \n",
        "- The model can be accessed via:  \n",
        "  âœ… **`ollama` command-line interface**  \n",
        "  âœ… **LangChainâ€™s `ChatOllama` class** , enabling structured AI workflows  \n",
        "- Supports multiple output formats, including **text, JSON, and multimodal outputs**  \n",
        "\n",
        "By using **Ollama** to serve Deepseek-R1, we can execute this **entire workflow locally** , ensuring **privacy, efficiency, and full control** over the research process.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Install Ollama\n",
        "\n",
        "Ollama is available for macOS, Linux, and Windows. You can download and install it from the official website:\n",
        "\n",
        "ðŸ”— **[Download Ollama](https://ollama.com/download)**\n",
        "\n",
        "Once installed, verify that Ollama is working by running the following command in your terminal:\n",
        "\n",
        "```sh\n",
        "$ ollama --version\n",
        "```\n",
        "If the installation was successful, this command should return the installed Ollama version.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Download DeepSeek-R1 Models\n",
        "\n",
        "After installing Ollama, you can download a DeepSeek-R1 model. These models vary in size, so itâ€™s important to check your GPU memory before selecting one.\n",
        "\n",
        "To pull a model, use:\n",
        "```sh\n",
        "$ ollama pull deepseek-r1:8b\n",
        "```\n",
        "\n",
        "This command will download the 8 billion parameter model (8B), which we tested on a MacBook Pro M1 (16GB RAM) and confirmed to be working properly.\n",
        "\n",
        "ðŸ’¡ If you experience performance issues, consider using a smaller model, depending on your hardware.\n",
        "\n",
        "---\n",
        "\n",
        "### Check GPU Memory Requirements\n",
        "\n",
        "[Ollama's DeepSeek-R1 models](https://ollama.com/library/deepseek-r1:8b) support Q4_K_M quantization, which reduces the required GPU memory by compressing the model to 4-bit precision.\n",
        "\n",
        "You can use the table below to determine if your hardware can support a particular model.\n",
        "For most users, 4-bit precision (Q4_K_M) is the recommended setting.\n",
        "\n",
        "ðŸ“Œ Calculation Formula:\n",
        "Refer to [this blog post](https://www.substratus.ai/blog/calculating-gpu-memory-for-llm) for details on how GPU memory is estimated.\n",
        "\n",
        "| Model Parameters | 16-bit Precision | 8-bit Precision | 4-bit Precision |\n",
        "|:----------------:|:----------------:|:---------------:|:---------------:|\n",
        "| 1.5 billion      | ~3.6 GB          | ~1.8 GB         | ~0.9 GB         |\n",
        "| 7 billion        | ~16.8 GB         | ~8.4 GB         | ~4.2 GB         |\n",
        "| 8 billion        | ~19.2 GB         | ~9.6 GB         | ~4.8 GB         |\n",
        "| 14 billion       | ~33.6 GB         | ~16.8 GB        | ~8.4 GB         |\n",
        "| 32 billion       | ~76.8 GB         | ~38.4 GB        | ~19.2 GB        |\n",
        "| 70 billion       | ~168.0 GB        | ~84.0 GB        | ~42.0 GB        |\n",
        "| 671 billion      | ~1610.4 GB       | ~805.2 GB       | ~402.6 GB       |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678a3f5a",
      "metadata": {},
      "source": [
        "## Using ChatOllama with DeepSeek-R1\n",
        "\n",
        "In this section, we'll explore how to utilize the `ChatOllama` class with the **deepseek-r1** model to generate web search queries in JSON format. Additionally, we'll delve into the use of `<think>` tags in **deepseek-r1** to structure the model's reasoning process.\n",
        "\n",
        "This tutorial does not cover the basics of **Ollama** and `ChatOllama` . \n",
        "\n",
        "If you need more information, please refer to the following tutorial: **\"/04-MODEL/10-Ollama.ipynb\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad46ec7f",
      "metadata": {},
      "source": [
        "### Generating Web Search Queries in JSON Format\n",
        "\n",
        "To generate structured search queries, we can prompt **deepseek-r1** to output responses in JSON format. This ensures that the generated queries are well-structured and easy to parse in automated workflows.\n",
        "\n",
        "When requesting search queries, you can specify the format explicitly, ensuring that the AI returns a properly formatted JSON object. This is useful for integrating AI-generated search queries into research pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9efddf35",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'how does cat communicate with each other',\n",
              " 'aspect': 'communication',\n",
              " 'rationale': 'Understanding how cats communicate can help in better interacting with them and providing appropriate care.'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "search_query_prompt = \"\"\"Your goal is to generate targeted web search query.\n",
        "\n",
        "The query will gather information related to a specific topic.\n",
        "\n",
        "Topics: {topics}\n",
        "\n",
        "Return your query as a JSON object:\n",
        "{{\n",
        "    \"query\": \"string\",\n",
        "    \"aspect\": \"string\",\n",
        "    \"retionale\": \"string\"\n",
        "}}\n",
        "\"\"\"\n",
        "\n",
        "llm_json_mode = ChatOllama(model=\"deepseek-r1:8b\", temperature=0.6, format=\"json\")\n",
        "\n",
        "msg = llm_json_mode.invoke(search_query_prompt.format(topics=\"cat\"))\n",
        "query = json.loads(msg.content)\n",
        "query"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa0d9b7",
      "metadata": {},
      "source": [
        "## Using DeepSeek-R1\n",
        "\n",
        "This section provides an overview of **how to use DeepSeek-R1 effectively** and follow the **recommended best practices** outlined by its authors. We'll also introduce the **\\<think> tags** , explain their purpose, and mention a workaround for removing them from agent outputs.\n",
        "\n",
        "In this tutorial, we will not cover DeepSeek's **architecture or training methodology** .  \n",
        "\n",
        "If you need more information, please refer to [this blog](https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1?utm_campaign=post&utm_medium=web)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9eb718",
      "metadata": {},
      "source": [
        "### Recommended Usage Guidelines\n",
        "To achieve optimal performance with DeepSeek-R1, [the model's authors](https://github.com/deepseek-ai/DeepSeek-R1?tab=readme-ov-file#usage-recommendations) recommend the following configurations:\n",
        "\n",
        "- **Set temperature between 0.5 and 0.7** (0.6 is recommended) to ensure coherent responses and prevent repetition.\n",
        "- **Do not use system prompts.** Instead, provide all instructions within the user prompt.\n",
        "- **For mathematical problems, explicitly instruct the model** to reason step by step and format the final answer using `\\boxed{}` .\n",
        "- **Encourage structured reasoning** by ensuring the model starts its response with `<think>\\n` , as it may sometimes omit its reasoning process.\n",
        "\n",
        "Following these recommendations will help maintain **consistency and accuracy** when using DeepSeek-R1.\n",
        "\n",
        "---\n",
        "\n",
        "Below are code examples comparing cases **without** and **with** the recommended usage guidelines.  \n",
        "\n",
        "Not following the guidelines does not necessarily lead to poor results, but these examples are provided to illustrate the impact of using best practices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e6952bc4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'<think>\\n\\n</think>\\n\\nThe capital of South Korea is **Seoul** (ì„œìš¸).'\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "\n",
        "llm = ChatOllama(model=\"deepseek-r1:8b\", temperature=1.5)\n",
        "msg = llm.invoke(\"What is capital of Korea\")\n",
        "pprint(msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "066e7115",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'<think>\\n\\n</think>\\n\\nThe capital of Korea is **Seoul**.'\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOllama(model=\"deepseek-r1:8b\", temperature=0.6)\n",
        "msg = llm.invoke(\"What is capital of Korea\")\n",
        "pprint(msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "06cdeb68",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'cat vs dog behavior comparison',\n",
              " 'aspect': 'behavioral differences',\n",
              " 'rationale': 'To understand how cats and dogs differ in their behaviors and characteristics.'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "query_writer_instructions = \"\"\"Your goal is to generate a targeted web search query.\n",
        "The query will gather information related to a specific topic.\n",
        "\n",
        "<TOPIC>\n",
        "{research_topic}\n",
        "</TOPIC>\n",
        "\n",
        "<FORMAT>\n",
        "Format your response as a JSON object with ALL three of these exact keys:\n",
        "   - \"query\": The actual search query string\n",
        "   - \"aspect\": The specific aspect of the topic being researched\n",
        "   - \"rationale\": Brief explanation of why this query is relevant\n",
        "</FORMAT>\n",
        "\n",
        "<EXAMPLE>\n",
        "Example output:\n",
        "{{\n",
        "    \"query\": \"machine learning transformer architecture explained\",\n",
        "    \"aspect\": \"technical architecture\",\n",
        "    \"rationale\": \"Understanding the fundamental structure of transformer models\"\n",
        "}}\n",
        "</EXAMPLE>\n",
        "\n",
        "Provide your response in JSON format:\"\"\"\n",
        "\n",
        "\n",
        "# Format the prompt\n",
        "query_writer_instructions_formatted = query_writer_instructions.format(\n",
        "    research_topic=\"cat vs. dog\"\n",
        ")\n",
        "\n",
        "# Generate a query\n",
        "llm_json_mode = ChatOllama(\n",
        "    model=\"deepseek-r1:8b\",\n",
        "    temperature=0.6,\n",
        "    format=\"json\",\n",
        ")\n",
        "result = llm_json_mode.invoke(\n",
        "    [\n",
        "        SystemMessage(content=query_writer_instructions_formatted),\n",
        "        HumanMessage(content=f\"Generate a query for web search:\"),\n",
        "    ]\n",
        ")\n",
        "query = json.loads(result.content)\n",
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "645fddb0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'cat vs dog comparison',\n",
              " 'aspect': 'animal behavior',\n",
              " 'rationale': 'To understand the differences in behavior and characteristics between cats and dogs.'}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result = llm_json_mode.invoke(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=f\"{query_writer_instructions_formatted} \\n\\n Generate a query for web search:\"\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "query = json.loads(result.content)\n",
        "query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5662b330",
      "metadata": {},
      "outputs": [],
      "source": [
        "math_problem = \"\"\"Problem:\n",
        "There are 35 candies and 4 children. Each child must receive the same number of candies, and all candies must be distributed.\n",
        "\t1.\tHow many candies does each child get?\n",
        "\t2.\tHow many candies are left after the distribution?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef622843",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('<think>\\n'\n",
            " 'First, I need to determine how many candies each child can receive when '\n",
            " 'distributing 35 candies among 4 children equally.\\n'\n",
            " '\\n'\n",
            " \"To do this, I'll divide the total number of candies by the number of \"\n",
            " \"children: 35 Ã· 4 = 8.75. Since it's not possible to have a fraction of a \"\n",
            " 'candy, I recognize that we need to adjust the distribution to ensure all '\n",
            " 'candies are used up and each child receives an equal whole number.\\n'\n",
            " '\\n'\n",
            " 'To resolve this, one child will receive 9 candies while the remaining three '\n",
            " 'children will receive 8 candies each. This way, the total distributed is 9 + '\n",
            " '3 Ã— 8 = 35 candies, which uses up all available candies.\\n'\n",
            " '\\n'\n",
            " \"Next, I'll address how many candies are left after the distribution. Since \"\n",
            " 'all 35 candies have been given out to the children, there are no leftovers '\n",
            " 'remaining.\\n'\n",
            " '</think>\\n'\n",
            " '\\n'\n",
            " '**Solution:**\\n'\n",
            " '\\n'\n",
            " 'We need to distribute **35 candies** among **4 children** such that each '\n",
            " 'child receives the same number of candies, and all candies are used up.\\n'\n",
            " '\\n'\n",
            " '1. **Calculating Candies per Child:**\\n'\n",
            " '   \\n'\n",
            " '   To find how many candies each child gets, divide the total number of '\n",
            " 'candies by the number of children:\\n'\n",
            " '   \\n'\n",
            " '   \\\\[\\n'\n",
            " '   \\\\frac{35 \\\\text{ candies}}{4 \\\\text{ children}} = 8.75 \\\\text{ candies '\n",
            " 'per child}\\n'\n",
            " '   \\\\]\\n'\n",
            " '   \\n'\n",
            " \"   Since we can't have a fraction of a candy, we need to adjust the \"\n",
            " 'distribution so that each child receives an equal whole number of candies '\n",
            " 'and all candies are used up.\\n'\n",
            " '\\n'\n",
            " '2. **Determining Equal Distribution:**\\n'\n",
            " '   \\n'\n",
            " '   - One child will receive **9 candies**.\\n'\n",
            " '   - The remaining three children will each receive **8 candies**.\\n'\n",
            " '   \\n'\n",
            " '   This ensures:\\n'\n",
            " '   \\n'\n",
            " '   \\\\[\\n'\n",
            " '   1 \\\\times 9 + 3 \\\\times 8 = 9 + 24 = 35 \\\\text{ candies}\\n'\n",
            " '   \\\\]\\n'\n",
            " '   \\n'\n",
            " '   All candies are distributed, and the distribution is as equal as '\n",
            " 'possible.\\n'\n",
            " '\\n'\n",
            " '3. **Candies Left After Distribution:**\\n'\n",
            " '   \\n'\n",
            " '   Since all **35 candies** have been given out to the children, there are '\n",
            " 'no candies left.\\n'\n",
            " '   \\n'\n",
            " '   \\\\[\\n'\n",
            " '   \\\\boxed{0}\\n'\n",
            " '   \\\\]\\n'\n",
            " '\\n'\n",
            " '**Final Answers:**\\n'\n",
            " '\\n'\n",
            " '1. Each child receives \\\\(\\\\boxed{8}\\\\) candies (with one child receiving an '\n",
            " 'additional candy).\\n'\n",
            " '2. No candies are left after distribution, so the answer is '\n",
            " '\\\\(\\\\boxed{0}\\\\).')\n"
          ]
        }
      ],
      "source": [
        "msg = llm.invoke(math_problem)\n",
        "pprint(msg.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a9c5d522",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('<think>\\n'\n",
            " 'I need to determine how many candies each child receives when there are 35 '\n",
            " 'candies and 4 children.\\n'\n",
            " '\\n'\n",
            " \"First, I'll divide the total number of candies by the number of children: 35 \"\n",
            " 'Ã· 4 = 8.75.\\n'\n",
            " '\\n'\n",
            " \"Since each child must receive an equal whole number of candies, I can't have \"\n",
            " \"a fraction of a candy. Therefore, I'll round down to the nearest whole \"\n",
            " 'number, which is 8 candies per child.\\n'\n",
            " '\\n'\n",
            " \"Next, I'll calculate how many candies are left after distributing 8 candies \"\n",
            " 'to each child: 35 - (4 Ã— 8) = 35 - 32 = 3 candies remaining.\\n'\n",
            " '</think>\\n'\n",
            " '\\n'\n",
            " '**Solution:**\\n'\n",
            " '\\n'\n",
            " 'We need to distribute **35 candies** among **4 children** such that each '\n",
            " 'child receives the same number of candies, and all candies are distributed.\\n'\n",
            " '\\n'\n",
            " '### Step 1: Determine How Many Candies Each Child Receives\\n'\n",
            " 'To find out how many candies each child gets, divide the total number of '\n",
            " 'candies by the number of children:\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Candies per child} = \\\\frac{\\\\text{Total Candies}}{\\\\text{Number of '\n",
            " 'Children}} = \\\\frac{35}{4} = 8.75\\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " 'Since each child must receive a whole number of candies, we **round down** '\n",
            " 'to the nearest whole number:\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Candies per child} = 8\\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " '### Step 2: Calculate the Remaining Candies After Distribution\\n'\n",
            " 'Subtract the total number of candies distributed from the original total:\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Remaining Candies} = \\\\text{Total Candies} - (\\\\text{Number of '\n",
            " 'Children} \\\\times \\\\text{Candies per Child}) \\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Remaining Candies} = 35 - (4 \\\\times 8) = 35 - 32 = 3\\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " '### Final Answer:\\n'\n",
            " '1. **Each child receives** \\\\(\\\\boxed{8}\\\\) **candies.**\\n'\n",
            " '2. **There are** \\\\(\\\\boxed{3}\\\\)** candies left after the distribution.**')\n"
          ]
        }
      ],
      "source": [
        "math_problem_instruction = f\"\"\"Answer the below problem. \n",
        "Please reason step by step, and put your final answer within \\\\boxed{{}}.\n",
        "{math_problem}\n",
        "\"\"\"\n",
        "\n",
        "msg = llm.invoke(math_problem_instruction)\n",
        "pprint(msg.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d391e006",
      "metadata": {},
      "source": [
        "### **What Are `<think>` Tags?**\n",
        "DeepSeek-R1 utilizes **structured reasoning** through `<think>` tags. These tags encapsulate the modelâ€™s internal thought process before delivering a final answer.\n",
        "\n",
        "The `<think>` tags exist because of the **way DeepSeek-R1 was trained**:\n",
        "-  **Reinforcement Learning (RL) for Reasoning Tasks:** During training, the model was rewarded for explicitly stating its thought process within `<think>` tags.\n",
        "\n",
        "---\n",
        "\n",
        "### **Handling `<think>` Tags in Outputs**\n",
        "While `<think>` tags enhance interpretability, there are cases where you may **want to remove them** for cleaner output in applications.\n",
        "\n",
        "The following code is a hack to remove the `<think>` tags from the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6773b7d0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('**Solution:**\\n'\n",
            " '\\n'\n",
            " 'We need to distribute **35 candies** among **4 children** such that each '\n",
            " 'child receives the same number of candies, and all candies are distributed.\\n'\n",
            " '\\n'\n",
            " '### Step 1: Determine How Many Candies Each Child Receives\\n'\n",
            " 'To find out how many candies each child gets, divide the total number of '\n",
            " 'candies by the number of children:\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Candies per child} = \\\\frac{\\\\text{Total Candies}}{\\\\text{Number of '\n",
            " 'Children}} = \\\\frac{35}{4} = 8.75\\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " 'Since each child must receive a whole number of candies, we **round down** '\n",
            " 'to the nearest whole number:\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Candies per child} = 8\\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " '### Step 2: Calculate the Remaining Candies After Distribution\\n'\n",
            " 'Subtract the total number of candies distributed from the original total:\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Remaining Candies} = \\\\text{Total Candies} - (\\\\text{Number of '\n",
            " 'Children} \\\\times \\\\text{Candies per Child}) \\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " '\\\\[\\n'\n",
            " '\\\\text{Remaining Candies} = 35 - (4 \\\\times 8) = 35 - 32 = 3\\n'\n",
            " '\\\\]\\n'\n",
            " '\\n'\n",
            " '### Final Answer:\\n'\n",
            " '1. **Each child receives** \\\\(\\\\boxed{8}\\\\) **candies.**\\n'\n",
            " '2. **There are** \\\\(\\\\boxed{3}\\\\)** candies left after the distribution.**')\n"
          ]
        }
      ],
      "source": [
        "think_output = msg.content\n",
        "\n",
        "while \"<think>\" in think_output and \"</think>\" in think_output:\n",
        "    start = think_output.find(\"<think>\")\n",
        "    end = think_output.find(\"</think>\") + len(\"</think>\")\n",
        "    think_output = think_output[:start] + think_output[end:]\n",
        "    think_output = think_output.strip()\n",
        "\n",
        "pprint(think_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fccc2087",
      "metadata": {},
      "source": [
        "## Ollama Deep Researcher powered by IterDRAG\n",
        "\n",
        "### Why IterDRAG for Research?\n",
        "\n",
        "Research queries often require **multi-hop reasoning** , meaning a single AI-generated answer is insufficient for complex topics. **IterDRAG addresses this challenge** by:  \n",
        "\n",
        "âœ… **Decomposing complex research topics** into structured sub-queries  \n",
        "âœ… **Iteratively retrieving, summarizing, and refining** relevant documents  \n",
        "âœ… **Generating intermediate answers** for each sub-query before final synthesis  \n",
        "âœ… **Scaling up knowledge extraction** by incorporating multiple iterations of retrieval  \n",
        "\n",
        "\n",
        "This tutorial introduces a **modular research agent** designed to iteratively refine research queries and synthesize structured reports. The research process follows an **IterDRAG-based methodology** , ensuring thorough information retrieval and refinement through multiple iterations.\n",
        "\n",
        "---\n",
        "\n",
        "### How the Ollama Deep Researcher Works\n",
        "1. **Query Generation** â†’ The AI **generates an initial web search query** based on the research topic.\n",
        "2. **Web Research** â†’ The agent **fetches relevant documents** using a search API (Tavily or Perplexity).\n",
        "3. **Summarization** â†’ Retrieved documents are **summarized into a structured format** for further analysis.\n",
        "4. **Reflection & Knowledge Gap Detection** â†’ The agent **analyzes knowledge gaps** and **generates a follow-up query** if needed.\n",
        "5. **Iterative Research Loop** â†’ Steps 2â€“4 are **repeated up to a set number of times** to refine the research.\n",
        "6. **Final Report Generation** â†’ Once the loop limit is reached, the agent **compiles all gathered insights and sources into a structured report** .\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "622a6e75",
      "metadata": {},
      "source": [
        "### Configuration & State management\n",
        "\n",
        "Before constructing the graph, letâ€™s first define the configuration and state. \n",
        "\n",
        "For the configuration,\n",
        "- `Configuration` manages customizable settings such as the number of research iterations ( `max_web_research_loops` ) and the choice of the local LLM model (e.g., `deepseek-r1:8b` ).\n",
        "- The `from_runnable_config` method allows dynamic configuration loading from environment variables or provided settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f29e6de1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dataclasses import dataclass, fields\n",
        "from typing import Any, Optional\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_opentutorial.messages import random_uuid\n",
        "\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class Configuration:\n",
        "    \"\"\"The configurable fields for the research assistant.\"\"\"\n",
        "\n",
        "    max_web_research_loops: int = 3\n",
        "    local_llm: str = \"deepseek-r1:8b\"  # 1.5b, 7b, 8b, 14b, 32b, 70b, 671b\n",
        "\n",
        "    @classmethod\n",
        "    def from_runnable_config(\n",
        "        cls, config: Optional[RunnableConfig] = None\n",
        "    ) -> \"Configuration\":\n",
        "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
        "        configurable = (\n",
        "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
        "        )\n",
        "        values: dict[str, Any] = {\n",
        "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
        "            for f in fields(cls)\n",
        "            if f.init\n",
        "        }\n",
        "        return cls(**{k: v for k, v in values.items() if v})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3f1a10",
      "metadata": {},
      "source": [
        "For state management, \n",
        "- `SummaryState` keeps track of the research process, including the topic, search queries, gathered sources, and the final summary.\n",
        "- It also defines `SummaryStateInput` and `SummaryStateOutput` to structure input and output data clearly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d76f9e3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import operator\n",
        "from dataclasses import dataclass, field\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class SummaryState:\n",
        "    research_topic: str = field(default=None)  # Report topic\n",
        "    search_query: str = field(default=None)  # Search query\n",
        "    web_research_results: Annotated[list, operator.add] = field(default_factory=list)\n",
        "    sources_gathered: Annotated[list, operator.add] = field(default_factory=list)\n",
        "    research_loop_count: int = field(default=0)  # Research loop count\n",
        "    running_summary: str = field(default=None)  # Final report\n",
        "\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class SummaryStateInput:\n",
        "    research_topic: str = field(default=None)  # Report topic\n",
        "\n",
        "\n",
        "@dataclass(kw_only=True)\n",
        "class SummaryStateOutput:\n",
        "    running_summary: str = field(default=None)  # Final report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b69a4bc",
      "metadata": {},
      "source": [
        "## Defining the Deep Researcher Nodes\n",
        "\n",
        "Now, letâ€™s build the graph.\n",
        "\n",
        "First, weâ€™ll define the five core nodes that make up the **Ollama Deep Researcher** . These nodes represent key stages of the research process. Once the nodes are defined, weâ€™ll connect them with edges to form a structured graph workflow that guides the research execution step by step.\n",
        "\n",
        "Here are the five core nodes:\n",
        "\n",
        "- `generate_query` : Generates an initial web search query based on the research topic.  \n",
        "- `web_research` : Searches the web using the generated query and retrieves relevant information.  \n",
        "- `summarize_sources` : Summarizes the gathered sources into a structured format.  \n",
        "- `reflect_on_summary` : Identifies knowledge gaps and formulates a follow-up query if needed.  \n",
        "- `finalize_summary` : Compiles all findings into a well-structured final research report.  \n",
        "\n",
        "![graph](./assets/14-langgraph-ollama-deep-researcher-deepseek.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3936f317",
      "metadata": {},
      "source": [
        "### Node to generate a query for web search\n",
        "\n",
        "**Generating the Search Query**  \n",
        "- The agent **constructs an optimized web search query** using `ChatOllama` in JSON format.  \n",
        "- It ensures that the generated query is structured for efficient retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29e67035",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "\n",
        "query_writer_instructions = \"\"\"Your goal is to generate a targeted web search query.\n",
        "The query will gather information related to a specific topic.\n",
        "\n",
        "<TOPIC>\n",
        "{research_topic}\n",
        "</TOPIC>\n",
        "\n",
        "<FORMAT>\n",
        "Format your response as a JSON object with ALL three of these exact keys:\n",
        "   - \"query\": The actual search query string\n",
        "   - \"aspect\": The specific aspect of the topic being researched\n",
        "   - \"rationale\": Brief explanation of why this query is relevant\n",
        "</FORMAT>\n",
        "\n",
        "<EXAMPLE>\n",
        "Example output:\n",
        "{{\n",
        "    \"query\": \"machine learning transformer architecture explained\",\n",
        "    \"aspect\": \"technical architecture\",\n",
        "    \"rationale\": \"Understanding the fundamental structure of transformer models\"\n",
        "}}\n",
        "</EXAMPLE>\n",
        "\n",
        "Provide your response in JSON format:\"\"\"\n",
        "\n",
        "\n",
        "def generate_query(state: SummaryState, config: RunnableConfig):\n",
        "    \"\"\"Generate a query for web search\"\"\"\n",
        "\n",
        "    # Format the prompt\n",
        "    query_writer_instructions_formatted = query_writer_instructions.format(\n",
        "        research_topic=state.research_topic\n",
        "    )\n",
        "\n",
        "    # Generate a query\n",
        "    configurable = Configuration.from_runnable_config(config)\n",
        "    llm_json_mode = ChatOllama(\n",
        "        model=configurable.local_llm,\n",
        "        temperature=0.6,\n",
        "        format=\"json\",\n",
        "    )\n",
        "    result = llm_json_mode.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=f\"{query_writer_instructions_formatted}\\n\\nGenerate a query for web search:\"\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    query = json.loads(result.content)\n",
        "\n",
        "    return {\"search_query\": query[\"query\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faede782",
      "metadata": {},
      "source": [
        "### Node to conduct web search using Tavily\n",
        "\n",
        "**Conducting Web Research**  \n",
        "- The system fetches information using either **Tavily** or **Perplexity** search APIs.  \n",
        "- The search results are **formatted and deduplicated** to ensure high-quality input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "73cca13d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import traceable\n",
        "from tavily import TavilyClient\n",
        "\n",
        "\n",
        "def deduplicate_and_format_sources(\n",
        "    search_response, max_tokens_per_source, include_raw_content=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Takes either a single search response or list of responses from search APIs and formats them.\n",
        "    Limits the raw_content to approximately max_tokens_per_source.\n",
        "    include_raw_content specifies whether to include the raw_content from Tavily in the formatted string.\n",
        "\n",
        "    Args:\n",
        "        search_response: Either:\n",
        "            - A dict with a 'results' key containing a list of search results\n",
        "            - A list of dicts, each containing search results\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with deduplicated sources\n",
        "    \"\"\"\n",
        "    # Convert input to list of results\n",
        "    if isinstance(search_response, dict):\n",
        "        sources_list = search_response[\"results\"]\n",
        "    elif isinstance(search_response, list):\n",
        "        sources_list = []\n",
        "        for response in search_response:\n",
        "            if isinstance(response, dict) and \"results\" in response:\n",
        "                sources_list.extend(response[\"results\"])\n",
        "            else:\n",
        "                sources_list.extend(response)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"Input must be either a dict with 'results' or a list of search results\"\n",
        "        )\n",
        "\n",
        "    # Deduplicate by URL\n",
        "    unique_sources = {}\n",
        "    for source in sources_list:\n",
        "        if source[\"url\"] not in unique_sources:\n",
        "            unique_sources[source[\"url\"]] = source\n",
        "\n",
        "    # Format output\n",
        "    formatted_text = \"Sources:\\n\\n\"\n",
        "    for i, source in enumerate(unique_sources.values(), 1):\n",
        "        formatted_text += f\"Source {source['title']}:\\n===\\n\"\n",
        "        formatted_text += f\"URL: {source['url']}\\n===\\n\"\n",
        "        formatted_text += (\n",
        "            f\"Most relevant content from source: {source['content']}\\n===\\n\"\n",
        "        )\n",
        "        if include_raw_content:\n",
        "            # Using rough estimate of 4 characters per token\n",
        "            char_limit = max_tokens_per_source * 4\n",
        "            # Handle None raw_content\n",
        "            raw_content = source.get(\"raw_content\", \"\")\n",
        "            if raw_content is None:\n",
        "                raw_content = \"\"\n",
        "                print(f\"Warning: No raw_content found for source {source['url']}\")\n",
        "            if len(raw_content) > char_limit:\n",
        "                raw_content = raw_content[:char_limit] + \"... [truncated]\"\n",
        "            formatted_text += f\"Full source content limited to {max_tokens_per_source} tokens: {raw_content}\\n\\n\"\n",
        "\n",
        "    return formatted_text.strip()\n",
        "\n",
        "\n",
        "def format_sources(search_results):\n",
        "    \"\"\"Format search results into a bullet-point list of sources.\n",
        "\n",
        "    Args:\n",
        "        search_results (dict): Tavily search response containing results\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted string with sources and their URLs\n",
        "    \"\"\"\n",
        "    return \"\\n\".join(\n",
        "        f\"* {source['title']} : {source['url']}\" for source in search_results[\"results\"]\n",
        "    )\n",
        "\n",
        "\n",
        "@traceable\n",
        "def tavily_search(query, include_raw_content=True, max_results=3):\n",
        "    \"\"\"Search the web using the Tavily API.\n",
        "\n",
        "    Args:\n",
        "        query (str): The search query to execute\n",
        "        include_raw_content (bool): Whether to include the raw_content from Tavily in the formatted string\n",
        "        max_results (int): Maximum number of results to return\n",
        "\n",
        "    Returns:\n",
        "        dict: Search response containing:\n",
        "            - results (list): List of search result dictionaries, each containing:\n",
        "                - title (str): Title of the search result\n",
        "                - url (str): URL of the search result\n",
        "                - content (str): Snippet/summary of the content\n",
        "                - raw_content (str): Full content of the page if available\"\"\"\n",
        "\n",
        "    tavily_client = TavilyClient()\n",
        "    return tavily_client.search(\n",
        "        query, max_results=max_results, include_raw_content=include_raw_content\n",
        "    )\n",
        "\n",
        "\n",
        "def web_research(state: SummaryState, config: RunnableConfig):\n",
        "    \"\"\"Gather information from the web\"\"\"\n",
        "\n",
        "    # Search the web\n",
        "    search_results = tavily_search(\n",
        "        state.search_query, include_raw_content=True, max_results=1\n",
        "    )\n",
        "    search_str = deduplicate_and_format_sources(\n",
        "        search_results, max_tokens_per_source=1000, include_raw_content=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"sources_gathered\": [format_sources(search_results)],\n",
        "        \"research_loop_count\": state.research_loop_count + 1,\n",
        "        \"web_research_results\": [search_str],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "941aaf42",
      "metadata": {},
      "source": [
        "### Node to summarize the web search results\n",
        "\n",
        "**Summarizing the Findings**  \n",
        "- The AI **extracts key insights** from retrieved documents.  \n",
        "- It **removes redundant information** and maintains coherence across iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "681f8679",
      "metadata": {},
      "outputs": [],
      "source": [
        "summarizer_instructions = \"\"\"\n",
        "<GOAL>\n",
        "Generate a high-quality summary of the web search results and keep it concise / related to the user topic.\n",
        "</GOAL>\n",
        "\n",
        "<REQUIREMENTS>\n",
        "When creating a NEW summary:\n",
        "1. Highlight the most relevant information related to the user topic from the search results\n",
        "2. Ensure a coherent flow of information\n",
        "\n",
        "When EXTENDING an existing summary:                                                                                                                 \n",
        "1. Read the existing summary and new search results carefully.                                                    \n",
        "2. Compare the new information with the existing summary.                                                         \n",
        "3. For each piece of new information:                                                                             \n",
        "    a. If it's related to existing points, integrate it into the relevant paragraph.                               \n",
        "    b. If it's entirely new but relevant, add a new paragraph with a smooth transition.                            \n",
        "    c. If it's not relevant to the user topic, skip it.                                                            \n",
        "4. Ensure all additions are relevant to the user's topic.                                                         \n",
        "5. Verify that your final output differs from the input summary.                                                                                                                                                            \n",
        "< /REQUIREMENTS >\n",
        "\n",
        "< FORMATTING >\n",
        "- Start directly with the updated summary, without preamble or titles. Do not use XML tags in the output.  \n",
        "< /FORMATTING >\"\"\"\n",
        "\n",
        "\n",
        "def summarize_sources(state: SummaryState, config: RunnableConfig):\n",
        "    \"\"\"Summarize the gathered sources\"\"\"\n",
        "\n",
        "    # Existing summary\n",
        "    existing_summary = state.running_summary\n",
        "\n",
        "    # Most recent web research\n",
        "    most_recent_web_research = state.web_research_results[-1]\n",
        "\n",
        "    # Build the human message\n",
        "    if existing_summary:\n",
        "        human_message_content = (\n",
        "            f\"<User Input> \\n {state.research_topic} \\n <User Input>\\n\\n\"\n",
        "            f\"<Existing Summary> \\n {existing_summary} \\n <Existing Summary>\\n\\n\"\n",
        "            f\"<New Search Results> \\n {most_recent_web_research} \\n <New Search Results>\"\n",
        "        )\n",
        "    else:\n",
        "        human_message_content = (\n",
        "            f\"<User Input> \\n {state.research_topic} \\n <User Input>\\n\\n\"\n",
        "            f\"<Search Results> \\n {most_recent_web_research} \\n <Search Results>\"\n",
        "        )\n",
        "\n",
        "    # Run the LLM\n",
        "    configurable = Configuration.from_runnable_config(config)\n",
        "    llm = ChatOllama(model=configurable.local_llm, temperature=0.6)\n",
        "    result = llm.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=f\"{summarizer_instructions}\\n\\n{human_message_content}\"\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    running_summary = result.content\n",
        "\n",
        "    # TODO: This is a hack to remove the <think> tags w/ Deepseek models\n",
        "    # It appears very challenging to prompt them out of the responses\n",
        "    while \"<think>\" in running_summary and \"</think>\" in running_summary:\n",
        "        start = running_summary.find(\"<think>\")\n",
        "        end = running_summary.find(\"</think>\") + len(\"</think>\")\n",
        "        running_summary = running_summary[:start] + running_summary[end:]\n",
        "\n",
        "    return {\"running_summary\": running_summary}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee801bd",
      "metadata": {},
      "source": [
        "### Node to generate a refined query reflecting a existing summary\n",
        "\n",
        "**Refining Through Reflection**  \n",
        "- The model **analyzes knowledge gaps** in the existing summary.  \n",
        "- If necessary, it **generates a refined follow-up query** to gather missing information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "197f9dae",
      "metadata": {},
      "outputs": [],
      "source": [
        "reflection_instructions = \"\"\"You are an expert research assistant analyzing a summary about {research_topic}.\n",
        "\n",
        "<GOAL>\n",
        "1. Identify knowledge gaps or areas that need deeper exploration\n",
        "2. Generate a follow-up question that would help expand your understanding\n",
        "3. Focus on technical details, implementation specifics, or emerging trends that weren't fully covered\n",
        "</GOAL>\n",
        "\n",
        "<REQUIREMENTS>\n",
        "Ensure the follow-up question is self-contained and includes necessary context for web search.\n",
        "</REQUIREMENTS>\n",
        "\n",
        "<FORMAT>\n",
        "Format your response as a JSON object with these exact keys:\n",
        "- knowledge_gap: Describe what information is missing or needs clarification\n",
        "- follow_up_query: Write a specific question to address this gap\n",
        "</FORMAT>\n",
        "\n",
        "<EXAMPLE>\n",
        "Example output:\n",
        "{{\n",
        "    \"knowledge_gap\": \"The summary lacks information about performance metrics and benchmarks\",\n",
        "    \"follow_up_query\": \"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\"\n",
        "}}\n",
        "</EXAMPLE>\n",
        "\n",
        "Provide your analysis in JSON format:\"\"\"\n",
        "\n",
        "\n",
        "def reflect_on_summary(state: SummaryState, config: RunnableConfig):\n",
        "    \"\"\"Reflect on the summary and generate a follow-up query\"\"\"\n",
        "\n",
        "    # Generate a query\n",
        "    configurable = Configuration.from_runnable_config(config)\n",
        "    llm_json_mode = ChatOllama(\n",
        "        model=configurable.local_llm, temperature=0.6, format=\"json\"\n",
        "    )\n",
        "    content = f\"\"\"{reflection_instructions.format(research_topic=state.research_topic)}\n",
        "    Identify a knowledge gap and generate a follow-up web search query based on our existing knowledge: {state.running_summary}\n",
        "    \"\"\"\n",
        "    result = llm_json_mode.invoke(\n",
        "        [\n",
        "            HumanMessage(content=content),\n",
        "        ]\n",
        "    )\n",
        "    follow_up_query = json.loads(result.content)\n",
        "\n",
        "    # Get the follow-up query\n",
        "    query = follow_up_query.get(\"follow_up_query\")\n",
        "\n",
        "    # JSON mode can fail in some cases\n",
        "    if not query:\n",
        "\n",
        "        # Fallback to a placeholder query\n",
        "        return {\"search_query\": f\"Tell me more about {state.research_topic}\"}\n",
        "\n",
        "    # Update search query with follow-up query\n",
        "    return {\"search_query\": follow_up_query[\"follow_up_query\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e441e856",
      "metadata": {},
      "source": [
        "### Node to finalize summary\n",
        "\n",
        "**Finalizing the Summary**  \n",
        "- The node **compiles all research findings** into a structured report.  \n",
        "- Sources are formatted into a **bulleted list for transparency** ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "dd10f185",
      "metadata": {},
      "outputs": [],
      "source": [
        "def finalize_summary(state: SummaryState):\n",
        "    \"\"\"Finalize the summary\"\"\"\n",
        "\n",
        "    # Format all accumulated sources into a single bulleted list\n",
        "    all_sources = \"\\n\".join(source for source in state.sources_gathered)\n",
        "    state.running_summary = (\n",
        "        f\"## Summary\\n\\n{state.running_summary}\\n\\n ### Sources:\\n{all_sources}\"\n",
        "    )\n",
        "    return {\"running_summary\": state.running_summary}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2620faf",
      "metadata": {},
      "source": [
        "### Building the Deep Researcher Graph\n",
        "\n",
        "Now we'll create the deep researcher graph that orchestrates the research workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "1cffb15b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing_extensions import Literal\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "def route_research(\n",
        "    state: SummaryState, config: RunnableConfig\n",
        ") -> Literal[\"finalize_summary\", \"web_research\"]:\n",
        "    \"\"\"Route the research based on the follow-up query\"\"\"\n",
        "\n",
        "    configurable = Configuration.from_runnable_config(config)\n",
        "    if state.research_loop_count <= configurable.max_web_research_loops:\n",
        "        return \"web_research\"\n",
        "    else:\n",
        "        return \"finalize_summary\"\n",
        "\n",
        "\n",
        "# Add nodes and edges\n",
        "builder = StateGraph(\n",
        "    SummaryState,\n",
        "    input=SummaryStateInput,\n",
        "    output=SummaryStateOutput,\n",
        "    config_schema=Configuration,\n",
        ")\n",
        "builder.add_node(\"generate_query\", generate_query)\n",
        "builder.add_node(\"web_research\", web_research)\n",
        "builder.add_node(\"summarize_sources\", summarize_sources)\n",
        "builder.add_node(\"reflect_on_summary\", reflect_on_summary)\n",
        "builder.add_node(\"finalize_summary\", finalize_summary)\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"generate_query\")\n",
        "builder.add_edge(\"generate_query\", \"web_research\")\n",
        "builder.add_edge(\"web_research\", \"summarize_sources\")\n",
        "builder.add_edge(\"summarize_sources\", \"reflect_on_summary\")\n",
        "builder.add_conditional_edges(\"reflect_on_summary\", route_research)\n",
        "builder.add_edge(\"finalize_summary\", END)\n",
        "\n",
        "# Create memory\n",
        "memory = MemorySaver()\n",
        "\n",
        "graph = builder.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9bc4f2bd",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain_opentutorial.graphs import visualize_graph\n",
        "\n",
        "# Visualize the graph\n",
        "visualize_graph(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0c5651",
      "metadata": {},
      "source": [
        "## Running the Deep Researcher Graph\n",
        "\n",
        "This code runs a research agent using the invoke_graph function. \n",
        "\n",
        "It initializes the agent with the DeepSeek-R1 (8B) model, sets a research topic, and configures the agent to perform up to 3 web research iterations. The agent then executes the research workflow, gathering information, summarizing findings, and refining queries automatically. ðŸš€\n",
        "\n",
        "[NOTE] This process takes approximately 3 to 5 minutes on an M1 Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2d75b9df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mgenerate_query\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32msearch_query\u001b[0m:\n",
            "DeepSeek-R1 model overview\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mweb_research\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "* DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1 : https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1\n",
            "\u001b[1;32mresearch_loop_count\u001b[0m:\n",
            "1\n",
            "Sources:\n",
            "\n",
            "Source DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1:\n",
            "===\n",
            "URL: https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1\n",
            "===\n",
            "Most relevant content from source: Weâ€™ll then move on to DeepSeek-R1, how itâ€™s reasoning works, and some prompt engineering best practices for reasoning models. DeepSeek-R1-Zero stands out from most other state-of-the-art models because it was trained using only reinforcement learning (RL), no supervised fine-tuning (SFT). The training process for DeepSeek-R1-Zero involved presenting the model with various reasoning tasks, ranging from math problems to abstract logic challenges. To train DeepSeek-R1-Zero to generate structured chain of thought sequences, the researchers used the following prompt training template, replacing {{prompt}} with the reasoning question. Next weâ€™ll look at a table comparing DeepSeek-R1-Zeroâ€™s performance across multiple reasoning datasets against OpenAIâ€™s reasoning models. Reinforcement learning-only training: R1-Zero demonstrates the feasibility of RL-alone approaches for building high-performing reasoning models\n",
            "===\n",
            "Full source content limited to 1000 tokens: DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1\n",
            "\n",
            "BlogLog in\n",
            "BlogLog inBook DemoSign up\n",
            "DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1\n",
            "Last updated onÂ \n",
            "January 23, 2025\n",
            "Table of Contents\n",
            "â€\n",
            "â€DeepSeek is a Chinese AI company â€œdedicated to making AGI a realityâ€ and open-sourcing all its models. They started in 2023, but have been making waves over the past month or so, and especially this past week with the release of their two latest reasoning models: DeepSeek-R1-Zero and the more advanced DeepSeek-R1, also known as DeepSeek Reasoner.\n",
            "Theyâ€™ve released not only the models but also the code and evaluation prompts for public use, along with a detailed paper outlining their approach.\n",
            "Aside from creating 2 highly performant models that are on par with OpenAIâ€™s o1 model, the paper has a lot of valuable information around reinforcement learning, chain of thought reasoning, prompt engineering with reasoning models, and more.\n",
            "Weâ€™ll start by focusing on the training process of DeepSeek-R1-Zero, which uniquely relied solely on reinforcement learning, instead of traditional supervised learning. Weâ€™ll then move on to DeepSeek-R1, how itâ€™s reasoning works, and some prompt engineering best practices for reasoning models.\n",
            "Training DeepSeek-R1-Zero: A reinforcement learning-only approach\n",
            "DeepSeek-R1-Zero stands out from most other state-of-the-art models because it was trained using only reinforcement learning (RL), no supervised fine-tuning (SFT). This challenges the current conventional approach and opens up new opportunities to train reasoning models with less human intervention and effort.\n",
            "DeepSeek-R1-Zero is the first open-source model to validate that advanced reasoning capabilities can be developed purely through RL.\n",
            "Without pre-labeled datasets, the model learns through trial and error, refining its behavior, parameters, and weights based solely on feedback from the solutions it generates.\n",
            "DeepSeek-R1-Zero is the base model for DeepSeek-R1.\n",
            "The RL process for DeepSeek-R1-Zero\n",
            "The training process for DeepSeek-R1-Zero involved presenting the model with various reasoning tasks, ranging from math problems to abstract logic challenges. The model generated outputs and was evaluated based on its performance.\n",
            "DeepSeek-R1-Zero received feedback through a reward system that helped guide its learning process:\n",
            "\n",
            "Accuracy rewards: Evaluates whether the output is correct. Used for when there are deterministic results (math problems).\n",
            "Format rewards: Encouraged the model to structure its reasoning within <think> and </think> tags\n",
            "\n",
            "Training prompt template\n",
            "To train DeepSeek-R1-Zero to generate structured chain of thought sequences, the researchers used the following prompt training template, replacing {{prompt}} with the reasoning question. You can access it in PromptHub here.\n",
            "â€\n",
            "\n",
            "â€\n",
            "This template prompted the model to explicitly outline its thought process within <think> tags before delivering the final answer in <answer> tags.\n",
            "The power of RL in reasoning\n",
            "With this training process DeepSeek-R1-Zero began to produce sophisticated reasoning chains.\n",
            "Through thousands of training steps, DeepSeek-R1-Zero evolved to solve increasingly complex problems. It learned to:\n",
            "\n",
            "Generate long reasoning chains that enabled deeper and more structured problem-solving\n",
            "Perform self-verification to cross-check its own answers (more on this later)\n",
            "Correct its own mistakes, showcasing emergent self-reflective behaviors\n",
            "\n",
            "â€\n",
            "DeepSeek R1-Zero performance\n",
            "While DeepSeek-R1-Zero is primarily a precursor to DeepSeek-R1, it still achieved high performance on several benchmarks. Letâ€™s dive into some of the experiments ran.\n",
            "Accuracy improvements during training\n",
            "â€\n",
            "\n",
            "â€\n",
            "\n",
            "Pass@1 accuracy began at 15.6% and by the end of the training it improved to 71.0%, comparable to OpenAIâ€™s o1-0912 model\n",
            "The red solid line represents performance with majority voting (similar to ensembling and self-consistency techniques), which increased accuracy further to 86.7%, surpassi... [truncated]\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36msummarize_sources\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mrunning_summary\u001b[0m:\n",
            "\n",
            "\n",
            "DeepSeek-R1-Zero, developed by the Chinese AI company DeepSeek, is a cutting-edge reasoning model trained exclusively using reinforcement learning (RL), breaking away from traditional supervised methods. This approach allowed it to learn advanced reasoning skills through trial and error without relying on labeled datasets or human intervention. The model excelled in tasks ranging from math problems to abstract logic challenges, generating structured chains of thought guided by a specific prompt template. DeepSeek-R1-Zero demonstrated remarkable accuracy, improving from 15.6% to 71% during training, comparable to OpenAI's o1-0912 model. With the addition of majority voting techniques, its accuracy surged to 86.7%, showcasing its potential in structured problem-solving and self-reflective behaviors.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mreflect_on_summary\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32msearch_query\u001b[0m:\n",
            "What are the specific mathematical, logical, or real-world problem types that DeepSeek-R1-Zero demonstrated its reasoning skills on?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mweb_research\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "* The Mathematics Behind DeepSeek-R1 | by Harjot Kaur | Jan, 2025 ... : https://pub.towardsai.net/the-mathematics-behind-deepseek-r1-954102f9b9c6\n",
            "\u001b[1;32mresearch_loop_count\u001b[0m:\n",
            "2\n",
            "Sources:\n",
            "\n",
            "Source The Mathematics Behind DeepSeek-R1 | by Harjot Kaur | Jan, 2025 ...:\n",
            "===\n",
            "URL: https://pub.towardsai.net/the-mathematics-behind-deepseek-r1-954102f9b9c6\n",
            "===\n",
            "Most relevant content from source: The Mathematics Behind DeepSeek-R1 | by Harjot Kaur | Jan, 2025 | Towards AI DeepSeek-R1 represents a new frontier in AI training â€” one that prioritizes reinforcement learning (RL) over human imitation and mathematical reasoning over conversational fluency. This article breaks down the mathematical foundations that make DeepSeek-R1 unique, using step-by-step explanations of its training pipeline, key algorithms, and optimization techniques. i) Reinforcement Learning (RL) on the Base Model â€” This step trains the initial model, DeepSeek-R1-Zero, without relying on human-labelled data. Unlike traditional AI models that start with supervised fine-tuning (SFT) and then apply reinforcement learning for refinement, DeepSeek uses RL-first training, which means it learns to optimize its reasoning independently before being exposed to human-generated data. Published in Towards AI -----------------------\n",
            "===\n",
            "Full source content limited to 1000 tokens: The Mathematics Behind DeepSeek-R1 | by Harjot Kaur | Jan, 2025 | Towards AI\n",
            "Open in app\n",
            "Sign up\n",
            "Sign in\n",
            "\n",
            "Write\n",
            "\n",
            "Sign up\n",
            "Sign in\n",
            "\n",
            "Member-only story\n",
            "The Mathematics Behind DeepSeek-R1\n",
            "A Technical Breakdown!\n",
            "\n",
            "\n",
            "Harjot Kaur\n",
            "Â·Follow\n",
            "Published in\n",
            "Towards AI\n",
            "Â·\n",
            "7 min read\n",
            "Â·\n",
            "1 hour ago\n",
            "\n",
            "--\n",
            "\n",
            "Listen\n",
            "Share\n",
            "Introduction\n",
            "DeepSeek-R1 represents a new frontier in AI training â€” one that prioritizes reinforcement learning (RL) over human imitation and mathematical reasoning over conversational fluency. This article breaks down the mathematical foundations that make DeepSeek-R1 unique, using step-by-step explanations of its training pipeline, key algorithms, and optimization techniques.\n",
            "\n",
            "Source: https://news.sky.com/story/what-is-deepseek-the-low-cost-chinese-ai-firm-that-has-turned-the-tech-world-upside-down-13298039\n",
            "1. Training Pipeline Overview\n",
            "DeepSeek-R1â€™s training consists of three major phases:\n",
            "i) Reinforcement Learning (RL) on the Base Model â€” This step trains the initial model, DeepSeek-R1-Zero, without relying on human-labelled data.\n",
            "ii) Multi-Stage RL with Cold-Start Data â€” The model is fine-tuned with a small, high-quality dataset and further refined using reinforcement learning.\n",
            "iii) Distillation to Smaller Models â€” Knowledge from the large model is transferred to smaller models using supervised learning.\n",
            "Unlike traditional AI models that start with supervised fine-tuning (SFT) and then apply reinforcement learning for refinement, DeepSeek uses RL-first training, which means it learns to optimize its reasoning independently before being exposed to human-generated data.\n",
            "\n",
            "--\n",
            "\n",
            "--\n",
            "\n",
            "\n",
            "\n",
            "Follow\n",
            "Published in Towards AI -----------------------\n",
            "72K Followers\n",
            "Â·Last published just now\n",
            "The leading AI community and content platform focused on making AI accessible to all. Check out our new course platform: https://academy.towardsai.net/courses/beginner-to-advanced-llm-dev\n",
            "Follow\n",
            "\n",
            "\n",
            "Follow\n",
            "Written by Harjot Kaur ----------------------\n",
            "67 Followers\n",
            "Â·29 Following\n",
            "Iâ€™m a non-engineer on a mission to simplify AI. I share accessible insights to help beginners break into this field. Let's learn and demystify AI together!\n",
            "Follow\n",
            "No responses yet\n",
            "\n",
            "What are your thoughts?\n",
            "Cancel\n",
            "Respond\n",
            "Respond\n",
            "Also publish to my profile\n",
            "Help\n",
            "Status\n",
            "About\n",
            "Careers\n",
            "Press\n",
            "Blog\n",
            "Privacy\n",
            "Terms\n",
            "Text to speech\n",
            "Teams\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36msummarize_sources\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mrunning_summary\u001b[0m:\n",
            "\n",
            "\n",
            "DeepSeek-R1-Zero, developed by the Chinese AI company DeepSeek, is a cutting-edge reasoning model trained exclusively using reinforcement learning (RL), breaking away from traditional supervised methods. This approach allowed it to learn advanced reasoning skills through trial and error without relying on labeled datasets or human intervention. The model excelled in tasks ranging from math problems to abstract logic challenges, generating structured chains of thought guided by a specific prompt template. DeepSeek-R1-Zero demonstrated remarkable accuracy, improving from 15.6% to 71% during training, comparable to OpenAI's o1-0912 model. With the addition of majority voting techniques, its accuracy surged to 86.7%, showcasing its potential in structured problem-solving and self-reflective behaviors.\n",
            "\n",
            "The training process of DeepSeek-R1 involves three major phases: (i) Reinforcement Learning (RL) on the Base Model, which trains the initial model without human-labelled data; (ii) Multi-Stage RL with Cold-Start Data, where the model is fine-tuned with a small, high-quality dataset and further refined using reinforcement learning; and (iii) Distillation to Smaller Models, where knowledge from the large model is transferred to smaller models using supervised learning. Unlike traditional AI models that start with supervised fine-tuning (SFT) and then apply reinforcement learning for refinement, DeepSeek uses RL-first training, allowing the model to learn to optimize its reasoning independently before being exposed to human-generated data.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mreflect_on_summary\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32msearch_query\u001b[0m:\n",
            "What are the specific tasks and domains in which DeepSeek-R1 has been tested or applied, and are there any real-world case studies or applications highlighting its capabilities?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mweb_research\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "* DeepSeek-R1: Features, Use Cases, and Comparison with OpenAI : https://www.mygreatlearning.com/blog/deepseek-r1-features-use-cases/\n",
            "\u001b[1;32mresearch_loop_count\u001b[0m:\n",
            "3\n",
            "Sources:\n",
            "\n",
            "Source DeepSeek-R1: Features, Use Cases, and Comparison with OpenAI:\n",
            "===\n",
            "URL: https://www.mygreatlearning.com/blog/deepseek-r1-features-use-cases/\n",
            "===\n",
            "Most relevant content from source: Great Learning Blog AI and Machine Learning DeepSeek-R1: Features, Use Cases, and Its Comparison with OpenAI AspectDeepSeek-R1OpenAIBenchmark PerformanceHigher pass rates on AIME, MATH-500, and coding challengesCompetitive but generally lower pass rates in the same testsAIME Pass Rate95%89%MATH-500 Pass Rate93%88%Coding Challenge Pass Rate92%85%Reinforcement Learning ApproachPure RL training for optimized task-specific learningPrimarily supervised fine-tuningMixture of Experts (MoE)Advanced MoE architecture, activates relevant parametersDoes not utilize MoE; full model activation requiredExplainabilityBuilt-in tools for explainable AI (XAI) enhancing transparencyLimited transparency in decision-making processesCost EfficiencySignificantly more cost-effective, optimized for resource useHigher operational costs due to intensive resource requirementsCustomizabilitySeamless integration with TensorFlow, PyTorch, and pre-trained modulesFlexible but often requires more effort for fine-tuningStrengthsCost-effective, transparent, high performance in benchmarksStronger in natural language tasks, more versatile across domainsWeaknessesLags behind in complex reasoning and creative tasksMore expensive and less efficient in specialized benchmarksGeneralization Across DomainsStrong in specific benchmarks (e.g., coding, math)Excellent generalization across a wide range of domains\n",
            "===\n",
            "Full source content limited to 1000 tokens: DeepSeek-R1: Features, Use Cases, and Its Comparison with OpenAI\n",
            "Skip to content\n",
            "\n",
            "Blog\n",
            "Search for: \n",
            "\n",
            "Browse Topics Menu Toggle\n",
            "AI and Machine Learning\n",
            "Data Science and Business Analytics\n",
            "IT/Software Development\n",
            "Digital Marketing\n",
            "Business Management\n",
            "Career Development\n",
            "Cybersecurity\n",
            "Cloud Computing\n",
            "Design Thinking\n",
            "Study Abroad\n",
            "Research and Studies\n",
            "\n",
            "\n",
            "Free Courses Menu Toggle\n",
            "IT & Software Free Courses\n",
            "Interview Preparation Free Courses\n",
            "Data Science Free Courses\n",
            "Artificial Intelligence Free Courses\n",
            "Machine Learning Free Courses\n",
            "Digital Marketing Free Courses\n",
            "Management Free Courses\n",
            "Cyber Security Free Courses\n",
            "Cloud Computing Free Courses\n",
            "Big Data Free Courses\n",
            "\n",
            "\n",
            "Study Abroad Menu Toggle\n",
            "Study in USA\n",
            "Study In Germany\n",
            "\n",
            "\n",
            "Career Menu Toggle\n",
            "Career Paths\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Blog\n",
            "Search for: \n",
            "Main Menu\n",
            "\n",
            "Articles\n",
            "Tutorials\n",
            "Interview Questions\n",
            "Free Courses\n",
            "Videos\n",
            "Projects\n",
            "Career Guide\n",
            "\n",
            "Great Learning Blog AI and Machine Learning DeepSeek-R1: Features, Use Cases, and Its Comparison with OpenAI\n",
            "Recommended AI Courses\n",
            "\n",
            "MIT No Code AI and Machine Learning Program\n",
            "Learn Artificial Intelligence & Machine Learning from University of Texas. Get a completion certificate and grow your professional career.\n",
            "4.70 â˜… (4,175 Ratings)\n",
            ": 12 Weeks\n",
            "View Program\n",
            "\n",
            "AI and ML Program from UT Austin\n",
            "Enroll in the PG Program in AI and Machine Learning from University of Texas McCombs. Earn PG Certificate and and unlock new opportunities\n",
            "4.73 â˜… (1,402 Ratings)\n",
            ": 7 months\n",
            "View Program\n",
            "Table of Contents\n",
            "\n",
            "What is DeepSeek-R1?\n",
            "DeepSeek-R1 Takes the Lead Over OpenAI in the U.S. App Store\n",
            "Unique Features of DeepSeek-R1\n",
            "Comparison of Performance Metrics Against OpenAI's Models\n",
            "Use Cases of DeepSeek-R1\n",
            "Future Prospects of DeepSeek-R1\n",
            "Conclusion\n",
            "\n",
            "DeepSeek-R1: Features, Use Cases, and Its Comparison with OpenAI\n",
            "Is DeepSeek-R1 the future of AI, or does OpenAI still hold the crown? Explore DeepSeek-R1â€™s innovative features, practical use cases, and compare its performance with OpenAIâ€™s models to make an informed decision.\n",
            "By Great Learning Editorial Team Published on Jan 28, 2025\n",
            "\n",
            "Table of contents\n",
            "\n",
            "What is DeepSeek-R1?\n",
            "DeepSeek-R1 Takes the Lead Over OpenAI in the U.S. App Store\n",
            "Unique Features of DeepSeek-R1\n",
            "Comparison of Performance Metrics Against OpenAI's Models\n",
            "Use Cases of DeepSeek-R1\n",
            "Future Prospects of DeepSeek-R1\n",
            "Conclusion\n",
            "\n",
            "Is DeepSeek-R1 the future of AI, or is OpenAI still the go-to choice?\n",
            "With advancements in AI happening at lightning speed, itâ€™s crucial to understand what makes DeepSeek-R1 a noteworthy contender.\n",
            "In this article, weâ€™ll break down the core features and use cases and compare the performance of DeepSeek-R1 with OpenAI to help you make an informed decision.\n",
            "What is DeepSeek-R1?\n",
            "DeepSeek-R1 is an innovative reasoning model designed to surpass the boundaries of what AI can accomplish, offering superior performance on numerous critical tasks.\n",
            "It employs large-scale reinforcement learning (RL) and multi-phase training to deliver high-calibre results.\n",
            "The modelâ€™s variants, including DeepSeek-R1-Zero, extend its capabilities even further.\n",
            "One of DeepSeekâ€™s boldest moves is its decision to open-source not just its main model but also six smaller distilled variants, which vary between 1.5 billion and 70 billion parameters.\n",
            "These models are released under the MIT license, delivering researchers & developers the freedom to adapt, enhance, as well as commercialize their innovations.\n",
            "Suggested: What is Generative AI?\n",
            "DeepSeek-R1 Takes the Lead Over OpenAI in the U.S. App Store\n",
            "DeepSeek recently surpassed OpenAIâ€™s ChatGPT in the Apple App Store, claiming the top spot for free apps in the U.S. as of January 2025.\n",
            "This follows the launch of DeepSeek-R1, an AI model that outperforms OpenAIâ€™s o1 in several benchmarks, including AIME and MATH-500.\n",
            "DeepSeek-R1â€™s success is credited to its innovative reasoning capabilities and cost-effective development, estimated at just $6 million, far less than its competitors.\n",
            "With its open-source model, DeepSeek is gaining traction among develope... [truncated]\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36msummarize_sources\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mrunning_summary\u001b[0m:\n",
            "\n",
            "\n",
            "**Summary of DeepSeek-R1 Model**\n",
            "\n",
            "The DeepSeek-R1 model represents a significant advancement in artificial intelligence, offering innovative capabilities that set it apart from competitors like OpenAI. Here's an overview of its key features and implications:\n",
            "\n",
            "1. **Innovative Architecture**: \n",
            "   - The model employs large-scale reinforcement learning (RL) and a multi-phase training approach, which includes RL-first training, fine-tuning with cold-start data, and distillation into smaller models.\n",
            "\n",
            "2. **Performance Metrics**:\n",
            "   - DeepSeek-R1 has demonstrated superior performance in benchmarks such as AIME and MATH-500, outperforming OpenAI's ChatGPT in the Apple App Store rankings. It achieved higher pass rates in these critical tasks.\n",
            "\n",
            "3. **Unique Features**:\n",
            "   - **Cost Efficiency**: Developed at a fraction of the cost compared to competitors (approximately $6 million), making it more accessible for various applications.\n",
            "   - **Explainability**: The model's decisions can be understood, enhancing trust and transparency in its operations.\n",
            "   - **Customization**: Allows for extensive adaptation and enhancement through open-source availability under the MIT license.\n",
            "\n",
            "4. **Comparison with OpenAI**:\n",
            "   - While OpenAI models are known for their performance, DeepSeek-R1 offers a more budget-friendly option with comparable or superior results in specific tasks. This makes it an attractive choice for cost-conscious users and developers.\n",
            "\n",
            "5. **Use Cases**:\n",
            "   - The model is applicable across various domains, including education, professional fields, research, and product development, due to its versatility and high performance in critical thinking tasks.\n",
            "\n",
            "6. **Open-Source Availability**:\n",
            "   - DeepSeek has released six distilled variants of the model under the MIT license, facilitating collaboration and customization among researchers and developers.\n",
            "\n",
            "In conclusion, DeepSeek-R1 is poised as a strong contender in AI, offering cutting-edge performance at a fraction of the cost, with significant potential for various applications across different industries.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mreflect_on_summary\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32msearch_query\u001b[0m:\n",
            "What specific tasks or benchmarks have shown DeepSeek-R1 to be more effective than OpenAI's models?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mweb_research\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "* Can DeepSeek R1 Take On OpenAI o1? Benchmarks Say Yes : https://www.techopedia.com/can-deepseek-r1-take-on-openai-o1\n",
            "\u001b[1;32mresearch_loop_count\u001b[0m:\n",
            "4\n",
            "Sources:\n",
            "\n",
            "Source Can DeepSeek R1 Take On OpenAI o1? Benchmarks Say Yes:\n",
            "===\n",
            "URL: https://www.techopedia.com/can-deepseek-r1-take-on-openai-o1\n",
            "===\n",
            "Most relevant content from source: Can DeepSeek R1 Take On OpenAI o1? Can DeepSeek R1 Take On OpenAI o1? New to the playing field is DeepSeek, which released a powerful large language model (LLM) in January 2025 that displays â€œremarkable reasoning capabilitiesâ€ and performance comparable to OpenAI o1 on AI reasoning tasks. DeepSeek R1 is an open source AI model, which according to benchmarks performs on par with OpenAI across mathematics, coding, and reasoning tasks. DeepSeek R1 is a powerful open-source large language model (LLM) launched in January 2025 that competes with OpenAI o1 in AI benchmarks. DeepSeek R1 performs similarly to OpenAI o1 in reasoning, math, and coding tasks, but is much cheaper to use, costing just 3% of o1â€™s API price.\n",
            "===\n",
            "Full source content limited to 1000 tokens: Can DeepSeek R1 Take On OpenAI o1? Benchmarks Say Yes - Techopedia\n",
            "Skip to the content\n",
            "\n",
            "\n",
            "Dictionary\n",
            "Trending Terms\n",
            "Cellular\n",
            "Binary\n",
            "Viral\n",
            "Podcast\n",
            "Web\n",
            "Website\n",
            "App\n",
            "Online\n",
            "Media\n",
            "Soft Copy\n",
            "\n",
            "\n",
            "Techopedia Terms\n",
            "#\n",
            "A\n",
            "B\n",
            "C\n",
            "D\n",
            "E\n",
            "F\n",
            "G\n",
            "H\n",
            "I\n",
            "J\n",
            "K\n",
            "L\n",
            "M\n",
            "N\n",
            "O\n",
            "P\n",
            "Q\n",
            "R\n",
            "S\n",
            "T\n",
            "U\n",
            "V\n",
            "W\n",
            "X\n",
            "Y\n",
            "Z\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Artificial Intelligence\n",
            "Cryptocurrency\n",
            "Cybersecurity\n",
            "Data Management\n",
            "Networking\n",
            "\n",
            "English\n",
            "Languages \n",
            "English Deutsch EspaÃ±ol FranÃ§ais Italiano Nederlands Norsk Svenska í•œêµ­ì–´ æ—¥æœ¬èªž\n",
            "\n",
            "SUGGESTED SEARCHES\n",
            "Is Toronto the Next Silicon Valley? Borderless AI CEO Suggests â€˜Yesâ€™ InterviewHow ChatGPT is Revolutionizing Smart Contract and Blockchain BlockchainWhen is Pi Network's Expected Launch Date? Pioneer's Mainnet 'Lands in Q1 2025' Blockchain\n",
            "\n",
            "\n",
            "\n",
            "All Articles\n",
            "Artificial Intelligence\n",
            "Machine Learning\n",
            "\n",
            "Can DeepSeek R1 Take On OpenAI o1? Benchmarks Say Yes\n",
            "\n",
            "by Technology Writer\n",
            "Tim Keary\n",
            "\n",
            "Tim Keary\n",
            "Technology Writer\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tim Keary is a technology writer and reporter covering AI, cybersecurity, and enterprise technology. Before joining Techopedia full-time in 2023, his work appeared on VentureBeat,â€¦\n",
            "All Articles by Tim Keary\n",
            "Fact Checked by Eddie Wrenn\n",
            "\n",
            "Eddie Wrenn\n",
            "Senior Content Editor\n",
            "\n",
            "\n",
            "\n",
            "Eddie is Techopedia's Senior Editor who has previously worked in local, national, and international newsrooms in the UK and Australia, including Mail Online and Sydney'sâ€¦\n",
            "All Articles by Eddie Wrenn\n",
            "Updated on 24 January 2025\n",
            "\n",
            " Email Facebook X Whatsapp LinkedIn Telegram Reddit\n",
            "Why Trust Techopedia\n",
            "We uphold a strict editorial policy. Our content, created by leading industry experts, is reviewed by a team of seasoned editors to ensure compliance with the highest standards in reporting and publishing.\n",
            "Ad Disclosure\n",
            " When you buy through affiliate links in our content, we may earn a commission at no extra cost to you. Learn how our funding model works. By using this website you agree to our terms and conditions and privacy policy.\n",
            "\n",
            "As time goes on, artificial intelligence is increasingly going open source.\n",
            "New to the playing field is DeepSeek, which released a powerful large language model (LLM) in January 2025 that displays â€œremarkable reasoning capabilitiesâ€ and performance comparable to OpenAI o1 on AI reasoning tasks.\n",
            "DeepSeek, a Chinese company founded by Liang Wenfang in 2023, demonstrates with its model that open-source AI isnâ€™t just capable of competing with proprietary AI products but can even surpass them in key areas, especially in the hot topic of price.\n",
            "But can DeepSeek take on OpenAI? If R1 is any indicator, this Hangzhou-based startup will be a key player in the generative AI market for the foreseeable future. Hereâ€™s why.\n",
            "Key Takeaways\n",
            "\n",
            "In January 2025 DeepSeek Launched R1, a powerful LLM with 671 billion parameters,\n",
            "The model uses techniques like reinforcement learning and chain-of-thought reasoning to improve the accuracy of responses.\n",
            "DeepSeekâ€™s model is so powerful that it displays performance comparable to OpenAI o1 on reasoning tasks.\n",
            "Experts have reacted positively to R1â€™s open-source approach and low overall cost.\n",
            "\n",
            "Table of Contents Table of Contents\n",
            "\n",
            "\n",
            "Key Takeaways\n",
            "\n",
            "\n",
            "Everything We Know About DeepSeek R1 So Far\n",
            "\n",
            "\n",
            "R1 vs o1: Key Performance Metrics\n",
            "\n",
            "\n",
            "Could R1 Threaten OpenAI?\n",
            "\n",
            "\n",
            "Expert Reactions to DeepSeek R1 So Far\n",
            "\n",
            "\n",
            "The Bottom Line\n",
            "\n",
            "\n",
            "FAQs\n",
            "\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "Show Full Guide\n",
            "\n",
            "Table of Contents\n",
            "\n",
            "\n",
            "Key Takeaways\n",
            "\n",
            "\n",
            "Everything We Know About DeepSeek R1 So Far\n",
            "\n",
            "\n",
            "R1 vs o1: Key Performance Metrics\n",
            "\n",
            "\n",
            "Show Full Guide5.  Could R1 Threaten OpenAI?\n",
            "\n",
            "\n",
            "Expert Reactions to DeepSeek R1 So Far\n",
            "\n",
            "\n",
            "The Bottom Line\n",
            "\n",
            "\n",
            "FAQs\n",
            "\n",
            "\n",
            "References\n",
            "\n",
            "\n",
            "Everything We Know About DeepSeek R1 So Far\n",
            "DeepSeek R1 is an open source AI model, which according to benchmarks performs on par with OpenAI across mathematics, coding, and reasoning tasks. The model is available via the companyâ€™s official website or via API.\n",
            "According to DeepSeekâ€™s research paper, R1 has been trained using techniques like reinforcement learning and chain of thought reasoning to increase the accuracy of the modelâ€™s responses.\n",
            "Advertisements\n",
            "One of the core sel... [truncated]\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36msummarize_sources\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mrunning_summary\u001b[0m:\n",
            "\n",
            "\n",
            "**DeepSeek-R1 Model Summary**\n",
            "\n",
            "The DeepSeek-R1 model is an open-source large language model developed by the Chinese company DeepSeek, founded in 2023 by Liang Wenfang. Launched in January 2025, R1 has demonstrated remarkable reasoning capabilities and performance comparable to OpenAI's GPT-4 (o1) in AI reasoning tasks.\n",
            "\n",
            "**Architecture and Training Techniques**\n",
            "\n",
            "DeepSeek-R1 is equipped with 671 billion parameters, making it a powerful tool for generating accurate responses. The model employs techniques such as reinforcement learning and chain-of-thought reasoning to enhance its precision and effectiveness. These methods allow R1 to not only match but sometimes surpass the performance of OpenAI's o1 in areas like mathematics, coding, and logical reasoning.\n",
            "\n",
            "**Performance Metrics**\n",
            "\n",
            "In benchmarks, DeepSeek-R1 has shown comparable results to OpenAI o1 across various tasks. Its ability to handle complex problem-solving and generate coherent, contextually appropriate responses positions it as a strong competitor in the generative AI market. The model's performance is particularly notable in mathematical computations and coding challenges.\n",
            "\n",
            "**Cost Advantage**\n",
            "\n",
            "One of R1's significant strengths is its affordability compared to OpenAI's models. This cost-effectiveness makes R1 accessible to a broader range of applications, including education, research, and industry, where budget constraints are often a limiting factor.\n",
            "\n",
            "**Impact on AI Market**\n",
            "\n",
            "The release of R1 underscores the potential of open-source AI models to compete with and even surpass proprietary alternatives like OpenAI's products. This approach not only democratizes access to advanced AI technologies but also challenges traditional monopolies in the AI sector, fostering innovation and competition.\n",
            "\n",
            "In conclusion, DeepSeek-R1 represents a significant milestone in AI development, offering both powerful performance and accessible solutions. Its impact on the market is expected to be profound, influencing future advancements in generative AI.\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mreflect_on_summary\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32msearch_query\u001b[0m:\n",
            "What are the key architectural and technical details of the DeepSeek-R1 model, including the types of neural networks, layer configurations, and training methodologies employed?\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "ðŸ”„ Node: \u001b[1;36mfinalize_summary\u001b[0m ðŸ”„\n",
            "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
            "\u001b[1;32mrunning_summary\u001b[0m:\n",
            "## Summary\n",
            "\n",
            "\n",
            "\n",
            "**DeepSeek-R1 Model Summary**\n",
            "\n",
            "The DeepSeek-R1 model is an open-source large language model developed by the Chinese company DeepSeek, founded in 2023 by Liang Wenfang. Launched in January 2025, R1 has demonstrated remarkable reasoning capabilities and performance comparable to OpenAI's GPT-4 (o1) in AI reasoning tasks.\n",
            "\n",
            "**Architecture and Training Techniques**\n",
            "\n",
            "DeepSeek-R1 is equipped with 671 billion parameters, making it a powerful tool for generating accurate responses. The model employs techniques such as reinforcement learning and chain-of-thought reasoning to enhance its precision and effectiveness. These methods allow R1 to not only match but sometimes surpass the performance of OpenAI's o1 in areas like mathematics, coding, and logical reasoning.\n",
            "\n",
            "**Performance Metrics**\n",
            "\n",
            "In benchmarks, DeepSeek-R1 has shown comparable results to OpenAI o1 across various tasks. Its ability to handle complex problem-solving and generate coherent, contextually appropriate responses positions it as a strong competitor in the generative AI market. The model's performance is particularly notable in mathematical computations and coding challenges.\n",
            "\n",
            "**Cost Advantage**\n",
            "\n",
            "One of R1's significant strengths is its affordability compared to OpenAI's models. This cost-effectiveness makes R1 accessible to a broader range of applications, including education, research, and industry, where budget constraints are often a limiting factor.\n",
            "\n",
            "**Impact on AI Market**\n",
            "\n",
            "The release of R1 underscores the potential of open-source AI models to compete with and even surpass proprietary alternatives like OpenAI's products. This approach not only democratizes access to advanced AI technologies but also challenges traditional monopolies in the AI sector, fostering innovation and competition.\n",
            "\n",
            "In conclusion, DeepSeek-R1 represents a significant milestone in AI development, offering both powerful performance and accessible solutions. Its impact on the market is expected to be profound, influencing future advancements in generative AI.\n",
            "\n",
            " ### Sources:\n",
            "* DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1 : https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1\n",
            "* The Mathematics Behind DeepSeek-R1 | by Harjot Kaur | Jan, 2025 ... : https://pub.towardsai.net/the-mathematics-behind-deepseek-r1-954102f9b9c6\n",
            "* DeepSeek-R1: Features, Use Cases, and Comparison with OpenAI : https://www.mygreatlearning.com/blog/deepseek-r1-features-use-cases/\n",
            "* Can DeepSeek R1 Take On OpenAI o1? Benchmarks Say Yes : https://www.techopedia.com/can-deepseek-r1-take-on-openai-o1\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "from langchain_opentutorial.messages import invoke_graph\n",
        "\n",
        "topic = \"Give me a summary of DeepSeek-r1 model.\"\n",
        "\n",
        "inputs = {\"research_topic\": topic}\n",
        "\n",
        "config = RunnableConfig(\n",
        "    configurable={\n",
        "        \"thread_id\": random_uuid(),\n",
        "        \"local_llm\": \"deepseek-r1:8b\",\n",
        "        \"max_web_research_loops\": 3,\n",
        "    },\n",
        ")\n",
        "\n",
        "invoke_graph(graph, inputs, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "78424a7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "final_state = graph.get_state(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e65092a",
      "metadata": {},
      "source": [
        "Display completed research summary in markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6a2e94b8",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "## Summary\n",
              "\n",
              "\n",
              "\n",
              "**DeepSeek-R1 Model Summary**\n",
              "\n",
              "The DeepSeek-R1 model is an open-source large language model developed by the Chinese company DeepSeek, founded in 2023 by Liang Wenfang. Launched in January 2025, R1 has demonstrated remarkable reasoning capabilities and performance comparable to OpenAI's GPT-4 (o1) in AI reasoning tasks.\n",
              "\n",
              "**Architecture and Training Techniques**\n",
              "\n",
              "DeepSeek-R1 is equipped with 671 billion parameters, making it a powerful tool for generating accurate responses. The model employs techniques such as reinforcement learning and chain-of-thought reasoning to enhance its precision and effectiveness. These methods allow R1 to not only match but sometimes surpass the performance of OpenAI's o1 in areas like mathematics, coding, and logical reasoning.\n",
              "\n",
              "**Performance Metrics**\n",
              "\n",
              "In benchmarks, DeepSeek-R1 has shown comparable results to OpenAI o1 across various tasks. Its ability to handle complex problem-solving and generate coherent, contextually appropriate responses positions it as a strong competitor in the generative AI market. The model's performance is particularly notable in mathematical computations and coding challenges.\n",
              "\n",
              "**Cost Advantage**\n",
              "\n",
              "One of R1's significant strengths is its affordability compared to OpenAI's models. This cost-effectiveness makes R1 accessible to a broader range of applications, including education, research, and industry, where budget constraints are often a limiting factor.\n",
              "\n",
              "**Impact on AI Market**\n",
              "\n",
              "The release of R1 underscores the potential of open-source AI models to compete with and even surpass proprietary alternatives like OpenAI's products. This approach not only democratizes access to advanced AI technologies but also challenges traditional monopolies in the AI sector, fostering innovation and competition.\n",
              "\n",
              "In conclusion, DeepSeek-R1 represents a significant milestone in AI development, offering both powerful performance and accessible solutions. Its impact on the market is expected to be profound, influencing future advancements in generative AI.\n",
              "\n",
              " ### Sources:\n",
              "* DeepSeek R-1 Model Overview and How it Ranks Against OpenAI's o1 : https://www.prompthub.us/blog/deepseek-r-1-model-overview-and-how-it-ranks-against-openais-o1\n",
              "* The Mathematics Behind DeepSeek-R1 | by Harjot Kaur | Jan, 2025 ... : https://pub.towardsai.net/the-mathematics-behind-deepseek-r1-954102f9b9c6\n",
              "* DeepSeek-R1: Features, Use Cases, and Comparison with OpenAI : https://www.mygreatlearning.com/blog/deepseek-r1-features-use-cases/\n",
              "* Can DeepSeek R1 Take On OpenAI o1? Benchmarks Say Yes : https://www.techopedia.com/can-deepseek-r1-take-on-openai-o1"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(final_state.values[\"running_summary\"])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-kr-lwwSZlnu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

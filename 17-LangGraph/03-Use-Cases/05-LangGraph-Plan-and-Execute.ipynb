{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "OqKumYqaNrPC",
      "metadata": {
        "id": "OqKumYqaNrPC"
      },
      "source": [
        "# Plan-and-Execute\n",
        "\n",
        "- Author: [ranian963](https://github.com/ranian963)\n",
        "- Peer Review:\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/17-LangGraph/03-Use-Cases/05-langgraph-plan-and-execute.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/17-LangGraph/03-Use-Cases/05-langgraph-plan-and-execute.ipynb)\n",
        "\n",
        "## Overview\n",
        "This tutorial introduces how to create a `plan-and-execute` style agent and explains, step by step, how to implement it using [LangGraph](https://langchain-ai.github.io/langgraph/).  \n",
        "The `plan-and-execute` approach is useful for tackling complex tasks by first establishing a long-term plan, then executing each step of that plan, and revising it as needed.\n",
        "\n",
        "![](./assets/05-langgraph-plan-and-execute.png)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Plan-and-Execute Definition](#plan-and-execute-definition)\n",
        "- [Defining Tools](#defining-tools)\n",
        "- [Defining the Execution Agent](#defining-the-execution-agent)\n",
        "- [State Definition](#state-definition)\n",
        "- [Plan Step](#plan-step)\n",
        "- [Re-plan Step](#re-plan-step)\n",
        "- [Creating the Graph](#creating-the-graph)\n",
        "- [Graph Visualization](#graph-visualization)\n",
        "- [Running the Graph](#running-the-graph)\n",
        "- [Checking the Final Report](#checking-the-final-report)\n",
        "\n",
        "### References\n",
        "\n",
        "- [LangChain](https://blog.langchain.dev/)\n",
        "- [LangGraph](https://langchain-ai.github.io/langgraph/)  \n",
        "- [Plan-and-Solve Paper](https://arxiv.org/abs/2305.04091)  \n",
        "- [Baby-AGI Project](https://github.com/yoheinakajima/babyagi)  \n",
        "- [ReAct Paper](https://arxiv.org/abs/2210.03629)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rhUZI_8lN_7x",
      "metadata": {
        "id": "rhUZI_8lN_7x"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Ir6WsaC1OB8y",
      "metadata": {
        "id": "Ir6WsaC1OB8y"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "WhSXCH7kODHR",
      "metadata": {
        "id": "WhSXCH7kODHR"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_openai\",\n",
        "        \"langgraph\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vzgjAj2mOKEJ",
      "metadata": {
        "id": "vzgjAj2mOKEJ"
      },
      "source": [
        "You can set API keys in a `.env` file or set them manually.\n",
        "\n",
        "[Note] If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RmZFhLKBOMrQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmZFhLKBOMrQ",
        "outputId": "32cb3dd1-d3ad-44a9-e97e-45d3d1c84473"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"05-LangGraph-Plan-and-Execute\",\n",
        "            \"TAVILY_API_KEY\": \"\",\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c95e9bf",
      "metadata": {
        "id": "5c95e9bf"
      },
      "source": [
        "## Plan-and-Execute Definition\n",
        "\n",
        "A `plan-and-execute` approach is characterized by:\n",
        "- Long-Term Planning: Before performing a complex task, it first establishes a high-level plan.\n",
        "- Step-by-Step Execution and Replanning: It carries out the plan in stages, checking at each step whether the plan is still valid and updating it if necessary.\n",
        "\n",
        "This method is inspired by the [Plan-and-Solve Peper](https://arxiv.org/abs/2305.04091) and the [Baby-AGI Project](https://github.com/yoheinakajima/babyagi). Unlike the more traditional [ReAct Style](https://arxiv.org/abs/2210.03629), which focuses on short-term reasoning one step at a time, `plan-and-execute` explicitly emphasizes long-term planning.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "1. **Clear Long-Term Structure**: Even powerful LLMs can struggle to handle extended plans in a single pass. By explicitly defining a long-term plan, the process becomes more robust.\n",
        "2. **Efficient Model Usage**: A larger or more powerful model can be used for the planning phase, while a smaller or lighter model can handle the execution phase, optimizing resource utilization.\n",
        "\n",
        "The sections below explain how to implement a `plan-and-execute` agent in LangGraph, step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6afbb11",
      "metadata": {
        "id": "c6afbb11"
      },
      "source": [
        "### Defining the Model Name for the Examples\n",
        "\n",
        "We will define the model name to be used in these demonstrations.\n",
        "\n",
        "> **Note**  \n",
        "> 1. Since `MODEL_NAME` appears frequently, we declare it as a separate variable.  \n",
        "> 2. It is recommended to run this with a model such as `gpt-4o` (or another GPT-4-level model). If you use a smaller model like `gpt-40-mini`, you may encounter frequent replanning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ruCr1MZjZBs3",
      "metadata": {
        "id": "ruCr1MZjZBs3"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gpt-4o\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0856a092",
      "metadata": {
        "id": "0856a092"
      },
      "source": [
        "## Defining Tools\n",
        "\n",
        "We first define the tools to be used.\n",
        "\n",
        "In this simple example, we will use the built-in search tool provided by `Tavily`. Of course, it is equally straightforward to create your own custom tools as needed.\n",
        "\n",
        "For more details, refer to the [Tools](https://langchain-opentutorial.gitbook.io/langchain-opentutorial/15-agent/01-tools) documentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dc76788",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dc76788",
        "outputId": "4b0e493e-8618-42e5-8d8b-2c7fd0860286"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Create an instance of TavilySearchResults with k=3 for retrieving up to 3 search results\n",
        "tavily_web_search = TavilySearchResults(k=3)\n",
        "\n",
        "tools = [tavily_web_search]\n",
        "tools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d230b5c",
      "metadata": {
        "id": "9d230b5c"
      },
      "source": [
        "## Defining the Execution Agent\n",
        "\n",
        "We now create the `execution agent` responsible for performing tasks.\n",
        "\n",
        "In this example, the same `execution agent` will be used for each task, but that is not mandatory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f70cf9fa",
      "metadata": {
        "id": "f70cf9fa"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Define the prompt\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant.\",\n",
        "        ),\n",
        "        (\"human\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Define the LLM\n",
        "llm = ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
        "\n",
        "# Create ReAct agent\n",
        "agent_executor = create_react_agent(llm, tools, state_modifier=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab1307f9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1307f9",
        "outputId": "8342cc09-24ef-488c-be7e-a7b0b302a24c"
      },
      "outputs": [],
      "source": [
        "# Agent execution\n",
        "agent_executor.invoke({\"messages\": [(\"user\", \"Tell me about LangChain\")]})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f6efaa",
      "metadata": {
        "id": "05f6efaa"
      },
      "source": [
        "## State Definition\n",
        "- `input`: User’s input  \n",
        "- `plan`: The current plan  \n",
        "- `past_steps`: The plan and results of previous executions  \n",
        "- `response`: The final response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "27c44393",
      "metadata": {
        "id": "27c44393"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "from typing import Annotated, List, Tuple\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "# State definition\n",
        "class PlanExecute(TypedDict):\n",
        "    input: Annotated[str, \"User's input\"]\n",
        "    plan: Annotated[List[str], \"Current plan\"]\n",
        "    past_steps: Annotated[List[Tuple], operator.add]\n",
        "    response: Annotated[str, \"Final response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8c5d026",
      "metadata": {
        "id": "b8c5d026"
      },
      "source": [
        "## Plan Step\n",
        "\n",
        "We will generate a long-term plan using **function calling** . Specifically, we define a `Plan` model and a prompt for the planner that instructs the LLM to produce an itemized plan of steps needed to solve the user's request. We keep each step focused and avoid adding unnecessary detail.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ad37fbe6",
      "metadata": {
        "id": "ad37fbe6"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "# Define Plan model\n",
        "class Plan(BaseModel):\n",
        "    \"\"\"Sorted steps to execute the plan\"\"\"\n",
        "\n",
        "    steps: Annotated[List[str], \"Different steps to follow, should be in sorted order\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "726052a2",
      "metadata": {
        "id": "726052a2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Create a prompt template for planning\n",
        "planner_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "planner = planner_prompt | ChatOpenAI(\n",
        "    model_name=MODEL_NAME, temperature=0\n",
        ").with_structured_output(Plan)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0fd64cd",
      "metadata": {
        "id": "a0fd64cd"
      },
      "source": [
        "We will run `planner` to verify the plan generation result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c782515c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c782515c",
        "outputId": "417c7798-5234-4a26-f678-fb11b99d3dca"
      },
      "outputs": [],
      "source": [
        "# Run planner\n",
        "planner.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            (\n",
        "                \"user\",\n",
        "                \"What are the main pros and cons of LangGraph, and why should we use it?\",\n",
        "            )\n",
        "        ]\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef28ddf5",
      "metadata": {
        "id": "ef28ddf5"
      },
      "source": [
        "## Re-plan Step\n",
        "\n",
        "Based on the results of previous steps, we create a stage that can revise the original plan. If a tool call or execution indicates that additional steps are needed, we update the plan accordingly; otherwise, we finalize the response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c80e74e3",
      "metadata": {
        "id": "c80e74e3"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "\n",
        "class Response(BaseModel):\n",
        "    \"\"\"Response to user.\"\"\"\n",
        "\n",
        "    response: str\n",
        "\n",
        "\n",
        "class Act(BaseModel):\n",
        "    \"\"\"Action to perform.\"\"\"\n",
        "\n",
        "    # The action to perform: 'Response' or 'Plan'. If you want to respond to the user, use Response.\n",
        "    # If you need to use additional tools, use Plan.\n",
        "    action: Union[Response, Plan] = Field(\n",
        "        description=\"Action to perform. If you want to respond to user, use Response.\"\n",
        "        \"If you need to further use tools to get the answer, use Plan.\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Define the prompt for re-planning\n",
        "replanner_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
        "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
        "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
        "\n",
        "Your objective was this:\n",
        "{input}\n",
        "\n",
        "Your original plan was this:\n",
        "{plan}\n",
        "\n",
        "You have currently done the follow steps:\n",
        "{past_steps}\n",
        "\n",
        "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# Create the replanner\n",
        "replanner = replanner_prompt | ChatOpenAI(\n",
        "    model_name=MODEL_NAME, temperature=0\n",
        ").with_structured_output(Act)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88a7250",
      "metadata": {
        "id": "b88a7250"
      },
      "source": [
        "## Creating the Graph\n",
        "\n",
        "We now build the LangGraph workflow by connecting the defined nodes:\n",
        "\n",
        "1. **planner** : Generates the plan.  \n",
        "2. **execute** : Uses the `execution agent` to perform the next step.  \n",
        "3. **replan** : Decides whether to continue with a new plan or provide the final answer.  \n",
        "4. **final_report** : Summarizes all steps and provides a polished final response.\n",
        "\n",
        "After defining the nodes and edges, we compile the graph. You can visualize the workflow to better understand how data moves between these steps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "df12d945",
      "metadata": {
        "id": "df12d945"
      },
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Generate and return a plan based on user input\n",
        "def plan_step(state: PlanExecute):\n",
        "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
        "    # Return the list of steps from the generated plan\n",
        "    return {\"plan\": plan.steps}\n",
        "\n",
        "\n",
        "# Use the agent executor to perform the specified task and return the result\n",
        "def execute_step(state: PlanExecute):\n",
        "    plan = state[\"plan\"]\n",
        "    # Convert the plan to a string, enumerating each step\n",
        "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
        "    task = plan[0]\n",
        "    # Format the current task for the agent\n",
        "    task_formatted = f\"\"\"For the following plan:\n",
        "{plan_str}\\n\\nYou are tasked with executing [step 1. {task}].\"\"\"\n",
        "    # Use the agent executor to perform the task and get the result\n",
        "    agent_response = agent_executor.invoke({\"messages\": [(\"user\", task_formatted)]})\n",
        "    # Return a dictionary containing the previous step and its result\n",
        "    return {\n",
        "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
        "    }\n",
        "\n",
        "\n",
        "# Update the plan or return the final response based on the results of the previous step\n",
        "def replan_step(state: PlanExecute):\n",
        "    output = replanner.invoke(state)\n",
        "    # If the answer should be returned to the user\n",
        "    if isinstance(output.action, Response):\n",
        "        return {\"response\": output.action.response}\n",
        "    # If more steps are needed\n",
        "    else:\n",
        "        next_plan = output.action.steps\n",
        "        if len(next_plan) == 0:\n",
        "            return {\"response\": \"No more steps needed.\"}\n",
        "        else:\n",
        "            return {\"plan\": next_plan}\n",
        "\n",
        "\n",
        "# A function that decides whether to end the agent's execution\n",
        "def should_end(state: PlanExecute):\n",
        "    if \"response\" in state and state[\"response\"]:\n",
        "        return \"final_report\"\n",
        "    else:\n",
        "        return \"execute\"\n",
        "\n",
        "\n",
        "final_report_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.\n",
        "Final report should be written in professional tone.\n",
        "\n",
        "Your objective was this:\n",
        "\n",
        "{input}\n",
        "\n",
        "Your previously done steps(question and answer pairs):\n",
        "\n",
        "{past_steps}\n",
        "\n",
        "Generate a final report in markdown format.\"\"\"\n",
        ")\n",
        "\n",
        "final_report = (\n",
        "    final_report_prompt\n",
        "    | ChatOpenAI(model_name=MODEL_NAME, temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "def generate_final_report(state: PlanExecute):\n",
        "    past_steps = \"\\n\\n\".join(\n",
        "        [\n",
        "            f\"Question: {past_step[0]}\\n\\nAnswer: {past_step[1]}\\n\\n####\"\n",
        "            for past_step in state[\"past_steps\"]\n",
        "        ]\n",
        "    )\n",
        "    response = final_report.invoke({\"input\": state[\"input\"], \"past_steps\": past_steps})\n",
        "    return {\"response\": response}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "16abdcf5",
      "metadata": {
        "id": "16abdcf5"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "\n",
        "# Create the workflow graph\n",
        "workflow = StateGraph(PlanExecute)\n",
        "\n",
        "# Define nodes\n",
        "workflow.add_node(\"planner\", plan_step)\n",
        "workflow.add_node(\"execute\", execute_step)\n",
        "workflow.add_node(\"replan\", replan_step)\n",
        "workflow.add_node(\"final_report\", generate_final_report)\n",
        "\n",
        "# Define edges\n",
        "workflow.add_edge(START, \"planner\")\n",
        "workflow.add_edge(\"planner\", \"execute\")\n",
        "workflow.add_edge(\"execute\", \"replan\")\n",
        "workflow.add_edge(\"final_report\", END)\n",
        "\n",
        "# Conditional edges: use should_end function to decide whether to stop\n",
        "workflow.add_conditional_edges(\n",
        "    \"replan\",\n",
        "    should_end,\n",
        "    {\"execute\": \"execute\", \"final_report\": \"final_report\"},\n",
        ")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile(checkpointer=MemorySaver())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd850b8c",
      "metadata": {
        "id": "bd850b8c"
      },
      "source": [
        "## Graph Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ee7d379",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "0ee7d379",
        "outputId": "592609c4-ae94-4fe0-af40-00b99523b774"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
        "\n",
        "# Image(app.get_graph(xray=True).draw_mermaid_png(output_file_path=\"05-langgraph-plan-and-execute.png\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d266cb",
      "metadata": {
        "id": "b6d266cb"
      },
      "source": [
        "## Running the Graph\n",
        "\n",
        "Finally, we run the entire workflow by providing user input. The workflow proceeds as follows:\n",
        "\n",
        "1. The **Planner** step creates an initial plan.  \n",
        "2. The **Execute** step executes the first item in the plan and returns results.  \n",
        "3. The **Re-plan** step checks if more actions are needed. If so, it updates the plan and goes back to **Execute** ; otherwise, it proceeds to **Final Report** .  \n",
        "4. The **Final Report** step generates a comprehensive markdown summary of all executed steps and the final answer.\n",
        "\n",
        "By following these steps, you can build a `plan-and-execute` agent in LangGraph, enabling structured, multi-step problem-solving with explicit long-term planning and flexible re-planning capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6323963b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6323963b",
        "outputId": "a447df4d-1e5a-4174-bdab-ae12367b6ebb"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "config = RunnableConfig(recursion_limit=50, configurable={\"thread_id\": \"1\"})\n",
        "inputs = {\"input\": \"Please explain about AI Agents.\"}\n",
        "\n",
        "async for event in app.astream(inputs, config=config):\n",
        "    for k, v in event.items():\n",
        "        if k != \"__end__\":\n",
        "            print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QqqgsrSntPd8",
      "metadata": {
        "id": "QqqgsrSntPd8"
      },
      "source": [
        "## Checking the Final Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e139d290",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e139d290",
        "outputId": "7dc8f880-d1e6-4fd3-89d1-e99c82fb4a8d"
      },
      "outputs": [],
      "source": [
        "snapshot = app.get_state(config).values\n",
        "print(snapshot[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "611af101",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "611af101",
        "outputId": "84176145-6e78-4b26-87f3-963791c94df7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(snapshot[\"response\"])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0856a092",
        "9d230b5c",
        "05f6efaa",
        "b8c5d026",
        "ef28ddf5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "langchain-kr-lwwSZlnu-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

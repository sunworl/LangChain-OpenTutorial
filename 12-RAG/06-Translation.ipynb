{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "- Author: [Wonyoung Lee](https://github.com/BaBetterB)\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BaBetterB/LangChain-OpenTutorial/blob/main/12-RAG/06-Translation.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial compares two approaches to translating Chinese text into English using LangChain.\n",
    "\n",
    "The first approach utilizes a single LLM (e.g. GPT-4) to generate a straightforward translation. The second approach employs Retrieval-Augmented Generation (RAG), which enhances translation accuracy by retrieving relevant documents.\n",
    "\n",
    "The tutorial evaluates the translation accuracy and performance of each method, helping users choose the most suitable approach for their needs.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Translation using LLM](#translation-using-llm)\n",
    "- [Translation using RAG](#translation-using-rag)\n",
    "- [Evaluation of translation results](#evaluation-of-translation-results)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain OpenAIEmbeddings API](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)\n",
    "- [NLTK](https://www.nltk.org/)\n",
    "- [TER](https://machinetranslate.org/ter)\n",
    "- [BERTScore](https://arxiv.org/abs/1904.09675)\n",
    "- [FAISS](https://github.com/facebookresearch/faiss)\n",
    "- [Chinese Source](https://cn.chinadaily.com.cn/)\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [ `langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample text and output the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"load_dotenv\",\n",
    "        \"langchain_openai\",\n",
    "        \"faiss-cpu\",\n",
    "        \"sacrebleu\",\n",
    "        \"bert_score\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Translation\",  # title\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration File for Managing API Keys as Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key Information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using LLM\n",
    "\n",
    "Translation using LLM refers to using a large language model (LLM), such as GPT-4, to translate text from one language to another. \n",
    "The model processes the input text and generates a direct translation based on its pre-trained knowledge. This approach is simple, fast, and effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese_text: 人工智能正在改变世界，各国都在加紧研究如何利用这一技术提高生产力。\n",
      "Translation: Artificial intelligence is transforming the world, and countries are intensifying their research on how to leverage this technology to enhance productivity.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create PromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a professional translator.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Please translate the following Chinese document into natural and accurate English.\"\n",
    "            \"Consider the context and vocabulary to ensure smooth and fluent sentences.:.\\n\\n\"\n",
    "            \"**Chinese Original Text:** {chinese_text}\\n\\n**English Translation:**\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "translation_chain = RunnableSequence(prompt, llm)\n",
    "\n",
    "chinese_text = \"人工智能正在改变世界，各国都在加紧研究如何利用这一技术提高生产力。\"\n",
    "\n",
    "response = translation_chain.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "print(\"Chinese_text:\", chinese_text)\n",
    "print(\"Translation:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using RAG \n",
    "\n",
    "Translation using RAG (Retrieval-Augmented Generation) enhances translation accuracy by combining a pre-trained LLM with a retrieval mechanism. This approach first retrieves relevant documents or data related to the input text and then utilizes this additional context to generate a more precise and contextually accurate translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search Implementation Using FAISS\n",
    "\n",
    "In this implementation, we use a vector database to store and retrieve embedded representations of entire sentences. Instead of relying solely on predefined knowledge in the LLM, our approach allows the model to retrieve semantically relevant sentences from the vector database, improving the translation's accuracy and fluency.\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)**\n",
    "\n",
    "FAISS is a library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It is widely used for approximate nearest neighbor (ANN) search in large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result\n",
      "1. 当地球员并非专业人士，而是农民、建筑工人、教师和学生，对足球的热爱将他们凝聚在一起\n",
      "2. ”卡卡说道\n",
      "3. “足球让我们结识新朋友，连接更广阔的世界\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "file_path = \"data/news_cn.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"file not found!!: {file_path}\")\n",
    "\n",
    "loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Vectorizing Sentences Individually\n",
    "sentences = []\n",
    "for doc in docs:\n",
    "    text = doc.page_content\n",
    "    sentence_list = text.split(\"。\")  # Splitting Chinese sentences based on '。'\n",
    "    sentences.extend(\n",
    "        [sentence.strip() for sentence in sentence_list if sentence.strip()]\n",
    "    )\n",
    "\n",
    "\n",
    "# Store sentences in the FAISS vector database\n",
    "vector_store = FAISS.from_texts(sentences, embedding=embeddings)\n",
    "\n",
    "# Search vectors using keywords \"人工智能\"\n",
    "search_results = vector_store.similarity_search(\"人工智能\", k=3)\n",
    "\n",
    "# check result\n",
    "print(\"Search result\")\n",
    "for idx, result in enumerate(search_results, start=1):\n",
    "    print(f\"{idx}. {result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare translation using LLM and translation using RAG.\n",
    "\n",
    "First, write the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# Download the necessary data for sentence tokenization in NLTK (requires initial setup)\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Document Search Function (Used in RAG)\n",
    "def retrieve_relevant_docs(query, vector_store, k=3):\n",
    "    \"\"\"\n",
    "    Searches for relevant documents using vector similarity search.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The keyword or sentence to search for.\n",
    "        vector_store (FAISS): The vector database.\n",
    "        k (int): The number of top matching documents to retrieve (default: 3).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of retrieved document texts.\n",
    "    \"\"\"\n",
    "    search_results = vector_store.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in search_results]\n",
    "\n",
    "\n",
    "# Translation using only LLM\n",
    "def translate_with_llm(chinese_text):\n",
    "    \"\"\"\n",
    "    Translates Chinese text into English using GPT-4o-mini.\n",
    "\n",
    "    Parameters:\n",
    "        chinese_text (str): The input Chinese sentence to be translated.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated English sentence.\n",
    "    \"\"\"\n",
    "    prompt_template_llm = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Translate the following Chinese sentence into English:\",\n",
    "            ),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\"user\", \"Please provide an accurate translation.\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_llm = RunnableSequence(prompt_template_llm, llm)\n",
    "\n",
    "    return translation_chain_llm.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# RAG-based Translation\n",
    "def translate_with_rag(chinese_text, vector_store):\n",
    "    \"\"\"\n",
    "    Translates Chinese text into English using the RAG approach.\n",
    "    It first retrieves relevant documents and then uses them for context-aware translation.\n",
    "\n",
    "    Parameters:\n",
    "        chinese_text (str): The input Chinese sentence to be translated.\n",
    "        vector_store (FAISS): The vector database for document retrieval.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated English sentence with contextual improvements.\n",
    "    \"\"\"\n",
    "    retrieved_docs = retrieve_relevant_docs(chinese_text, vector_store)\n",
    "\n",
    "    # Add retrieved documents as context\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Construct prompt template (Using RAG)\n",
    "\n",
    "    prompt_template_rag = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Below is the Chinese text that needs to be translated into English. Additionally, the following context has been provided from relevant documents that might help you in producing a more accurate and context-aware translation.\",\n",
    "            ),\n",
    "            (\"system\", f\"Context (Relevant Documents):\\n{context}\"),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Please provide a translation that is both accurate and reflects the context from the documents provided.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_rag = RunnableSequence(prompt_template_rag, llm)\n",
    "\n",
    "    # Request translation using RAG\n",
    "\n",
    "    return translation_chain_rag.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# Load Chinese text from a file and split it into sentences, returning them as a list.\n",
    "def chinese_text_from_file_loader(path):\n",
    "    \"\"\"\n",
    "    Loads Chinese text from a file and splits it into individual sentences.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): File path.\n",
    "\n",
    "    Returns:\n",
    "        list: List of Chinese sentences.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    return split_chinese_sentences_from_docs(docs)\n",
    "\n",
    "\n",
    "# Split sentences from a list of documents and return them as a list\n",
    "def split_chinese_sentences_from_docs(docs):\n",
    "    \"\"\"\n",
    "    Extracts sentences from a list of documents.\n",
    "\n",
    "    Parameters:\n",
    "        docs (list): List of document objects.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        sentences.extend(split_chinese_sentences(text))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Use regular expressions to split sentences and punctuation together.\n",
    "# Then, combine the sentences and punctuation back and return them\n",
    "def split_chinese_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits Chinese text into sentences based on punctuation marks (。！？).\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input Chinese text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of separated sentences.\n",
    "    \"\"\"\n",
    "    # Separate sentences and punctuation,\n",
    "    sentence_list = re.split(r\"([。！？])\", text)\n",
    "\n",
    "    # Combine the sentences and punctuation back to restore them.\n",
    "    merged_sentences = [\n",
    "        \"\".join(x) for x in zip(sentence_list[0::2], sentence_list[1::2])\n",
    "    ]\n",
    "\n",
    "    # Remove empty sentences and return the result.\n",
    "    return [sentence.strip() for sentence in merged_sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "def count_chinese_sentences(docs):\n",
    "    \"\"\"\n",
    "    Counts the number of sentences in a given Chinese text.\n",
    "\n",
    "    Parameters:\n",
    "        docs (str or list): Input text data.\n",
    "\n",
    "    Returns:\n",
    "        list: List of split sentences.\n",
    "    \"\"\"\n",
    "    if isinstance(docs, str):\n",
    "        sentences = split_chinese_sentences(docs)\n",
    "\n",
    "    print(f\"Total number of sentences: {len(sentences)}\")\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_english_sentences_from_docs(docs):\n",
    "    \"\"\"\n",
    "    Splits English text into sentences using NLTK's sentence tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input English text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of separated sentences.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        sentences.extend(split_english_sentences(text))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Use NLTK's sent_tokenize() to split sentences accurately.\n",
    "# By default, it recognizes periods (.), question marks (?), and exclamation marks (!) to separate sentences.\n",
    "def split_english_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits English text into sentences using NLTK's sentence tokenizer.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The input English text.\n",
    "\n",
    "    Returns:\n",
    "        list: List of separated sentences.\n",
    "    \"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "\n",
    "def count_paragraphs_and_sentences(docs):\n",
    "    \"\"\"\n",
    "    Counts the number of paragraphs and sentences in a given text.\n",
    "\n",
    "    Parameters:\n",
    "        docs (str): Input text data.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of sentences.\n",
    "    \"\"\"\n",
    "    if isinstance(docs, str):\n",
    "\n",
    "        paragraphs = paragraphs = re.split(r\"\\n\\s*\\n\", docs.strip())\n",
    "        paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "        sentences = [sent for para in paragraphs for sent in sent_tokenize(para)]\n",
    "\n",
    "        print(f\"Total number of paragraphs : {len(paragraphs)}\")\n",
    "        print(f\"Total number of sentences  : {len(sentences)}\")\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the written functions to perform the comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input chinese text\n",
      "Total number of sentences: 15\n",
      "数据领域迎来国家标准。10月8日，国家发改委等部门发布关于印发《国家数据标准体系建设指南》(以下简称《指南》)的通知。为“充分发挥标准在激活数据要素潜能、做强做优做大数字经济等方面的规范和引领作用”，国家发展改革委、国家数据局、中央网信办、工业和信息化部、财政部、国家标准委组织编制了《国家数据标准体系建设指南》。《指南》提出，到2026年底，基本建成国家数据标准体系，围绕数据流通利用基础设施、数据管理、数据服务、训练数据集、公共数据授权运营、数据确权、数据资源定价、企业数据范式交易等方面制修订30项以上数据领域基础通用国家标准，形成一批标准应用示范案例，建成标准验证和应用服务平台，培育一批具备数据管理能力评估、数据评价、数据服务能力评估、公共数据授权运营绩效评估等能力的第三方标准化服务机构。《指南》明确，数据标准体系框架包含基础通用、数据基础设施、数据资源、数据技术、数据流通、融合应用、安全保障等7个部分。数据基础设施方面，标准涉及存算设施中的数据算力设施、数据存储设施，网络设施中的5G网络数据传输、光纤数据传输、卫星互联网数据传输，此外还有流通利用设施。数据流通方面，标准包括数据产品、数据确权、数据资源定价、数据流通交易。融合应用方面，标准涉及工业制造、农业农村、商贸流通、交通运输、金融服务、科技创新、文化旅游(文物)、卫生健康、应急管理、气象服务、城市治理、绿色低碳。安全保障方面，标准涉及数据基础设施安全，数据要素市场安全，数据流通安全。数据资源中的数据治理标准包括数据业务规划、数据质量管理、数据调查盘点、数据资源登记；训练数据集方面的标准包括训练数据集采集处理、训练数据集标注、训练数据集合成。在组织保障方面，将指导建立全国数据标准化技术组织，加快推进急用、急需数据标准制修订工作，强化与有关标准化技术组织、行业、地方及相关社团组织之间的沟通协作、协调联动，以标准化促进数据产业生态建设。同时还将完善标准试点政策配套，搭建数据标准化公共服务平台，开展标准宣贯，选择重点地方、行业先行先试，打造典型示范。探索推动数据产品第三方检验检测，深化数据标准实施评价管理。在人才培养方面，将打造标准配套的数据人才培训课程，形成一批数据标准化专业人才。优化数据国际标准化专家队伍，支持参与国际标准化活动，强化国际交流。\n",
      "\n",
      "Translation using LLM\n",
      "Total number of paragraphs : 1\n",
      "Total number of sentences  : 16\n",
      "The data field welcomes national standards. On October 8, the National Development and Reform Commission and other departments issued a notice regarding the release of the \"Guidelines for the Construction of a National Data Standard System\" (hereinafter referred to as the \"Guidelines\"). To \"fully leverage the role of standards in activating the potential of data elements and strengthening, optimizing, and expanding the digital economy,\" the National Development and Reform Commission, the National Data Bureau, the Central Cyberspace Affairs Commission, the Ministry of Industry and Information Technology, the Ministry of Finance, and the National Standardization Administration organized the preparation of the \"Guidelines.\" The \"Guidelines\" propose that by the end of 2026, a national data standard system will be basically established, with the development and revision of more than 30 basic general national standards in the areas of data circulation and utilization infrastructure, data management, data services, training datasets, public data authorized operation, data rights confirmation, data resource pricing, and enterprise data paradigm transactions. A number of standard application demonstration cases will be formed, a standard verification and application service platform will be built, and a number of third-party standardized service institutions capable of data management capability assessment, data evaluation, data service capability assessment, and public data authorized operation performance assessment will be cultivated. The \"Guidelines\" clarify that the framework of the data standard system includes seven parts: basic general standards, data infrastructure, data resources, data technology, data circulation, integrated applications, and security assurance. In terms of data infrastructure, the standards cover data computing facilities and data storage facilities in computing storage infrastructure, as well as 5G network data transmission, optical fiber data transmission, satellite internet data transmission, and circulation utilization facilities in network infrastructure. Regarding data circulation, the standards include data products, data rights confirmation, data resource pricing, and data circulation transactions. In terms of integrated applications, the standards involve industrial manufacturing, agriculture and rural areas, trade and circulation, transportation, financial services, technological innovation, cultural tourism (cultural relics), health, emergency management, meteorological services, urban governance, and green low-carbon initiatives. Concerning security assurance, the standards address data infrastructure security, data element market security, and data circulation security. The data governance standards within data resources include data business planning, data quality management, data survey and inventory, and data resource registration; standards related to training datasets encompass the collection and processing of training datasets, dataset labeling, and dataset synthesis. In terms of organizational support, guidance will be provided to establish a national data standardization technical organization, accelerate the revision of urgently needed data standards, and strengthen communication and collaboration with relevant standardization technical organizations, industries, localities, and related associations to promote the construction of the data industry ecosystem through standardization. Additionally, policies for standard pilot projects will be improved, a public service platform for data standardization will be established, standard promotion activities will be conducted, and key localities and industries will be selected for pioneering trials to create typical demonstrations. Efforts will be made to explore third-party inspection and testing of data products and deepen the evaluation and management of data standard implementation. In terms of talent cultivation, training courses for data talents that complement the standards will be developed to cultivate a group of professionals in data standardization. The international standardization expert team will be optimized to support participation in international standardization activities and strengthen international exchanges.\n",
      "\n",
      "Translation using RAG\n",
      "Total number of paragraphs : 5\n",
      "Total number of sentences  : 19\n",
      "The data sector is welcoming national standards. On October 8, the National Development and Reform Commission (NDRC) and other departments issued a notice regarding the release of the \"Guidelines for the Construction of a National Data Standard System\" (hereinafter referred to as the \"Guidelines\"). This initiative aims to \"fully leverage the regulatory and guiding role of standards in activating the potential of data elements, strengthening, optimizing, and expanding the digital economy.\" The NDRC, the National Data Bureau, the Cyberspace Administration of China, the Ministry of Industry and Information Technology, the Ministry of Finance, and the National Standardization Administration jointly organized the development of the \"Guidelines.\"\n",
      "\n",
      "The \"Guidelines\" propose that by the end of 2026, a national data standard system will be essentially established. This will involve the formulation and revision of over 30 foundational and general national standards in areas such as data circulation and utilization infrastructure, data management, data services, training data sets, public data authorization operations, data rights confirmation, data resource pricing, and enterprise data paradigm transactions. The aim is to create a number of standard application demonstration cases, establish a standard verification and application service platform, and cultivate a group of third-party standardized service organizations capable of assessing data management capabilities, data evaluation, data service capabilities, and public data authorization operation performance.\n",
      "\n",
      "The \"Guidelines\" clarify that the framework of the data standard system consists of seven components: foundational general standards, data infrastructure, data resources, data technology, data circulation, integrated applications, and security guarantees. In terms of data infrastructure, the standards address data computing facilities and data storage facilities within computing and storage infrastructure, as well as network facilities including 5G data transmission, fiber-optic data transmission, and satellite internet data transmission, along with circulation and utilization facilities. Regarding data circulation, standards encompass data products, data rights confirmation, data resource pricing, and data circulation transactions. In terms of integrated applications, standards relate to industrial manufacturing, agriculture and rural development, commerce and circulation, transportation, financial services, technological innovation, cultural tourism (cultural relics), health care, emergency management, meteorological services, urban governance, and green low-carbon initiatives. Security guarantees cover the security of data infrastructure, the safety of the data elements market, and the security of data circulation.\n",
      "\n",
      "Data governance standards within data resources include data business planning, data quality management, data inventory and survey, and data resource registration. Standards related to training data sets include collection and processing of training data sets, data labeling, and training data set synthesis. In terms of organizational support, there will be guidance to establish a national data standardization technical organization, accelerate the formulation and revision of urgently needed data standards, and strengthen communication and collaboration with relevant standardization technical organizations, industries, localities, and related associations to promote the construction of a data industry ecosystem through standardization. \n",
      "\n",
      "Additionally, there will be improvements to the supporting policies for standard pilot projects, the establishment of a public service platform for data standardization, the promotion of standards, and the selection of key localities and industries for pilot testing to create typical models. The exploration of third-party inspection and testing of data products will be encouraged, along with the deepening of data standard implementation evaluation and management. In terms of talent development, there will be initiatives to create training courses for data talent that complement standards, aiming to cultivate a group of professionals in data standardization. The optimization of the team of international standardization experts in data will be supported to encourage participation in international standardization activities and to strengthen international exchanges.\n"
     ]
    }
   ],
   "source": [
    "# Download the 'punkt_tab' data if it's not available.\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt_tab\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt_tab\")\n",
    "\n",
    "sentences = chinese_text_from_file_loader(\"data/comparison_cn.txt\")\n",
    "\n",
    "\n",
    "\n",
    "chinese_text = \"\"\n",
    "\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "\n",
    "    chinese_text += sentence\n",
    "\n",
    "\n",
    "\n",
    "# LLM\n",
    "\n",
    "\n",
    "llm_translation = translate_with_llm(chinese_text)\n",
    "\n",
    "\n",
    "\n",
    "# RAG\n",
    "\n",
    "\n",
    "rag_translation = translate_with_rag(chinese_text, vector_store)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\ninput chinese text\")\n",
    "count_chinese_sentences(chinese_text)\n",
    "print(chinese_text)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using LLM\")\n",
    "\n",
    "\n",
    "count_paragraphs_and_sentences(llm_translation.content)\n",
    "\n",
    "\n",
    "print(llm_translation.content)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using RAG\")\n",
    "\n",
    "\n",
    "count_paragraphs_and_sentences(rag_translation.content)\n",
    "\n",
    "\n",
    "print(rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of translation results\n",
    "\n",
    "Evaluating machine translation quality is essential to ensure the accuracy and fluency of translated text. In this tutorial, we use two key metrics, TER and BERTScore, to assess the quality of translations produced by both a general LLM-based translation system and a RAG-based translation system.\n",
    "\n",
    "By combining TER and BERTScore, we achieve a comprehensive evaluation of translation quality.\n",
    "TER measures the structural differences and required edits between translations and reference texts.\n",
    "BERTScore captures the semantic similarity between translations and references.\n",
    "This dual evaluation approach allows us to effectively compare LLM and RAG translations, helping determine which method provides more accurate, fluent, and natural translations.\n",
    "\n",
    "\n",
    "**TER (Translation Edit Rate)**\n",
    "\n",
    "TER quantifies how much editing is required to transform a system-generated translation into the reference translation. It accounts for insertions, deletions, substitutions, and Shifts (word reordering).\n",
    "\n",
    "Interpretation:\n",
    "Lower TER indicates a better translation (fewer modifications needed).\n",
    "Higher TER suggests that the translation deviates significantly from the reference\n",
    "\n",
    "**BERTScore - Contextual Semantic Evaluation**\n",
    "\n",
    "BERTScore evaluates translation quality by computing semantic similarity scores between reference and candidate translations. It utilizes contextual embeddings from a pre-trained BERT model, unlike traditional n-gram-based methods that focus solely on word overlap.\n",
    "\n",
    "Interpretation:\n",
    "Higher BERTScore (closer to 1.0) indicates better semantic similarity between the candidate and reference translations.\n",
    "Lower scores indicate less semantic alignment with the reference translation.\n",
    "\n",
    "Since Chinese and English are grammatically very different languages, there can be significant differences in word order and sentence structure. As a result, the TER score may be relatively high, while BERTScore can serve as a more important evaluation metric.\n",
    "\n",
    "By leveraging both TER and BERTScore, we can effectively analyze the strengths and weaknesses of LLM-based and RAG-based translation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Top 1**\n",
      "------------------------------------------------------------\n",
      "Source Text       | 数据领域迎来国家标准。\n",
      "LLM Translation   | The translation of the Chinese sentence \"数据领域迎来国家标准。\" is \"The data field welcomes national standards.\"\n",
      "RAG Translation   | \"The data sector is ushering in national standards.\"\n",
      "TER Score (LLM)   | 1400.0\n",
      "BERTScore (LLM)   | 0.923\n",
      "TER Score (RAG)   | 800.0\n",
      "BERTScore (RAG)   | 0.782\n",
      "------------------------------------------------------------ \n",
      "\n",
      "**Top 2**\n",
      "------------------------------------------------------------\n",
      "Source Text       | 在人才培养方面，将打造标准配套的数据人才培训课程，形成一批数据标准化专业人才。\n",
      "LLM Translation   | In terms of talent development, we will create standardized and supportive training programs for data professionals, forming a group of standardized professionals in data.\n",
      "RAG Translation   | \"In terms of talent development, we will create standardized training courses for data professionals, aiming to cultivate a group of specialized personnel in data standardization.\"\n",
      "TER Score (LLM)   | 2400.0\n",
      "BERTScore (LLM)   | 0.764\n",
      "TER Score (RAG)   | 2500.0\n",
      "BERTScore (RAG)   | 0.772\n",
      "------------------------------------------------------------ \n",
      "\n",
      "**Top 3**\n",
      "------------------------------------------------------------\n",
      "Source Text       | 数据流通方面，标准包括数据产品、数据确权、数据资源定价、数据流通交易。\n",
      "LLM Translation   | In terms of data circulation, the standards include data products, data rights confirmation, data resource pricing, and data circulation transactions.\n",
      "RAG Translation   | In terms of data circulation, the standards include data products, data ownership rights, data resource pricing, and data circulation transactions.\n",
      "TER Score (LLM)   | 2000.0\n",
      "BERTScore (LLM)   | 0.762\n",
      "TER Score (RAG)   | 2000.0\n",
      "BERTScore (RAG)   | 0.761\n",
      "------------------------------------------------------------ \n",
      "\n",
      "**Top 4**\n",
      "------------------------------------------------------------\n",
      "Source Text       | 安全保障方面，标准涉及数据基础设施安全，数据要素市场安全，数据流通安全。\n",
      "LLM Translation   | In terms of security guarantees, the standards involve data infrastructure security, data factor market security, and data circulation security.\n",
      "RAG Translation   | In terms of security guarantees, the standards cover the safety of data infrastructure, the security of the data factor market, and the security of data circulation.\n",
      "TER Score (LLM)   | 1900.0\n",
      "BERTScore (LLM)   | 0.76\n",
      "TER Score (RAG)   | 2600.0\n",
      "BERTScore (RAG)   | 0.761\n",
      "------------------------------------------------------------ \n",
      "\n",
      "**Top 5**\n",
      "------------------------------------------------------------\n",
      "Source Text       | 优化数据国际标准化专家队伍，支持参与国际标准化活动，强化国际交流。\n",
      "LLM Translation   | \"Optimize the team of international standardization experts in data, support participation in international standardization activities, and strengthen international exchanges.\"\n",
      "RAG Translation   | \"Optimize the team of international experts in data standardization, support participation in international standardization activities, and strengthen international communication.\"\n",
      "TER Score (LLM)   | 1900.0\n",
      "BERTScore (LLM)   | 0.758\n",
      "TER Score (RAG)   | 1900.0\n",
      "BERTScore (RAG)   | 0.764\n",
      "------------------------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sacrebleu\n",
    "import bert_score\n",
    "\n",
    "\n",
    "# Download the 'punkt' data if it's not available.\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except LookupError:\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# TER Score Calculation\n",
    "def calculate_ter(reference, candidate):\n",
    "    ter_metric = sacrebleu.metrics.TER()\n",
    "    return round(ter_metric.corpus_score([candidate], [[reference]]).score, 3)\n",
    "\n",
    "\n",
    "# BERTScore Calculation\n",
    "def calculate_bert_score(reference, candidate):\n",
    "    try:\n",
    "        P, R, F1 = bert_score.score([candidate], [reference], lang=\"en\")\n",
    "        return round(F1.mean().item(), 3)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating BERTScore: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "sentences = chinese_text_from_file_loader(\"data/comparison_cn.txt\")\n",
    "\n",
    "# Store sentences in the FAISS vector database\n",
    "vector_store = FAISS.from_texts(sentences, embedding=embeddings)\n",
    "\n",
    "\n",
    "# Execute translation\n",
    "translated_results = []\n",
    "for idx, sentence in enumerate(sentences, start=1):\n",
    "    llm_translation = translate_with_llm(sentence)\n",
    "    rag_translation = translate_with_rag(sentence, vector_store)\n",
    "\n",
    "    # Evaluate translation quality (LLM)\n",
    "    ter_llm = calculate_ter(sentence, llm_translation.content)\n",
    "    bert_llm = calculate_bert_score(sentence, llm_translation.content)\n",
    "\n",
    "    # Evaluate translation quality (RAG)\n",
    "    ter_rag = calculate_ter(sentence, rag_translation.content)\n",
    "    bert_rag = calculate_bert_score(sentence, rag_translation.content)\n",
    "\n",
    "    translated_results.append(\n",
    "        {\n",
    "            \"source_text\": sentence,\n",
    "            \"llm_translation\": llm_translation.content,\n",
    "            \"rag_translation\": rag_translation.content,\n",
    "            \"TER LLM\": ter_llm,\n",
    "            \"BERTScore LLM\": bert_llm,\n",
    "            \"TER RAG\": ter_rag,\n",
    "            \"BERTScore RAG\": bert_rag,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "# Since Chinese and English are grammatically very different languages, there can be significant differences in word order and sentence structure. As a result, the TER score may be relatively high, while BERTScore can serve as a more important evaluation metric.\n",
    "# Sort in descending order based on BERTScore LLM and extract the top 5.\n",
    "top_5_bert_llm = sorted(\n",
    "    translated_results, key=lambda x: x[\"BERTScore LLM\"], reverse=True\n",
    ")[:5]\n",
    "# Display results in a transposed format\n",
    "for idx, result in enumerate(top_5_bert_llm, start=1):\n",
    "    print(f\"**Top {idx}**\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Source Text       | {result['source_text']}\")\n",
    "    print(f\"LLM Translation   | {result['llm_translation']}\")\n",
    "    print(f\"RAG Translation   | {result['rag_translation']}\")\n",
    "    print(f\"TER Score (LLM)   | {result['TER LLM']}\")\n",
    "    print(f\"BERTScore (LLM)   | {result['BERTScore LLM']}\")\n",
    "    print(f\"TER Score (RAG)   | {result['TER RAG']}\")\n",
    "    print(f\"BERTScore (RAG)   | {result['BERTScore RAG']}\")\n",
    "    print(\"-\" * 60, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-9y5W8e20-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "- Author: [Wonyoung Lee](https://github.com/BaBetterB)\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BaBetterB/LangChain-OpenTutorial/blob/main/15-Agent/05-Iteration-HumanInTheLoop.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial compares two approaches to translating Chinese text into English using LangChain.\n",
    "\n",
    "The first approach utilizes a single LLM (e.g. GPT-4) to generate a straightforward translation. The second approach employs Retrieval-Augmented Generation (RAG), which enhances translation accuracy by retrieving relevant documents.\n",
    "\n",
    "The tutorial evaluates the translation accuracy and performance of each method, helping users choose the most suitable approach for their needs.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Translation using LLM](#translation-using-llm)\n",
    "- [Translation using RAG](#translation-using-rag)\n",
    "- [Evaluation of translation results](#evaluation-of-translation-resultsr)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [ `langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample text and output the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"load_dotenv\",\n",
    "        \"langchain_openai\",\n",
    "        \"transformers\",\n",
    "        \"faiss-cpu\",\n",
    "        \"sentence_transformers\",\n",
    "        \"sacrebleu\",\n",
    "        \"unbabel-comet\",\n",
    "        \"load_from_checkpoint\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Translation\",  # title\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration File for Managing API Keys as Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key Information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using LLM\n",
    "\n",
    "Translation using LLM refers to using a large language model (LLM), such as GPT-4, to translate text from one language to another. \n",
    "The model processes the input text and generates a direct translation based on its pre-trained knowledge. This approach is simple, fast, and effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43; trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=6a4fb624-6770-4ba0-b055-2bee90a8fcfc; trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=c26c4172-c7cc-40ab-9a24-c374bbb77ddc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese_text: äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œå„å›½éƒ½åœ¨åŠ ç´§ç ”ç©¶å¦‚ä½•åˆ©ç”¨è¿™ä¸€æŠ€æœ¯æé«˜ç”Ÿäº§åŠ›ã€‚\n",
      "Translation: ì¸ê³µì§€ëŠ¥ì´ ì„¸ê³„ë¥¼ ë³€í™”ì‹œí‚¤ê³  ìˆìœ¼ë©°, ê°êµ­ì€ ì´ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ìƒì‚°ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ë¥¼ ê°€ì†í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43; trace=7fa642e2-6d50-42dd-ab36-89c1a4ed6a43,id=c26c4172-c7cc-40ab-9a24-c374bbb77ddc\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=f209f675-10f6-4c8d-aa5c-9114535ae27d; trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=ea4aac3a-bbf1-40a4-94bb-238a77e5d351; trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=856eca3e-b98e-4394-9528-6c418a9451ff\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=f209f675-10f6-4c8d-aa5c-9114535ae27d; trace=f209f675-10f6-4c8d-aa5c-9114535ae27d,id=856eca3e-b98e-4394-9528-6c418a9451ff\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=75137c7f-b658-46b7-b2e6-0f212dcb7587; trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=1c4315ae-f9ce-48e4-9092-e85c11d8d4eb; trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=3041959b-0eb0-4ff5-a7c3-0919373ccaa8\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=75137c7f-b658-46b7-b2e6-0f212dcb7587; trace=75137c7f-b658-46b7-b2e6-0f212dcb7587,id=3041959b-0eb0-4ff5-a7c3-0919373ccaa8\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create PromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a professional translator.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Please translate the following Chinese document into natural and accurate Korean.\"\n",
    "            \"Consider the context and vocabulary to ensure smooth and fluent sentences.:.\\n\\n\"\n",
    "            \"**Chinese Original Text:** {chinese_text}\\n\\n**English Translation:**\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "translation_chain = RunnableSequence(prompt, llm)\n",
    "\n",
    "chinese_text = \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œå„å›½éƒ½åœ¨åŠ ç´§ç ”ç©¶å¦‚ä½•åˆ©ç”¨è¿™ä¸€æŠ€æœ¯æé«˜ç”Ÿäº§åŠ›ã€‚\"\n",
    "\n",
    "response = translation_chain.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "print(\"Chinese_text:\", chinese_text)\n",
    "print(\"Translation:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using RAG \n",
    "\n",
    "Translation using RAG (Retrieval-Augmented Generation) enhances translation accuracy by combining a pre-trained LLM with a retrieval mechanism. It first retrieves relevant documents or data related to the input text, then uses this additional context to generate a more precise and contextually accurate translation. This approach is particularly useful for technical terms, specialized content, or context-sensitive translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search Implementation Using FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It is widely used for approximate nearest neighbor (ANN) search in large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result\n",
      "1. å½“åœ°çƒå‘˜å¹¶éä¸“ä¸šäººå£«ï¼Œè€Œæ˜¯å†œæ°‘ã€å»ºç­‘å·¥äººã€æ•™å¸ˆå’Œå­¦ç”Ÿï¼Œå¯¹è¶³çƒçš„çƒ­çˆ±å°†ä»–ä»¬å‡èšåœ¨ä¸€èµ·\n",
      "2. â€å¡å¡è¯´é“\n",
      "3. â€œè¶³çƒè®©æˆ‘ä»¬ç»“è¯†æ–°æœ‹å‹ï¼Œè¿æ¥æ›´å¹¿é˜”çš„ä¸–ç•Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "file_path = \"data/news_cn.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"file not found!!: {file_path}\")\n",
    "\n",
    "loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Vectorizing Sentences Individually\n",
    "sentences = []\n",
    "for doc in docs:\n",
    "    text = doc.page_content\n",
    "    sentence_list = text.split(\"ã€‚\")  # Splitting Chinese sentences based on 'ã€‚'\n",
    "    sentences.extend(\n",
    "        [sentence.strip() for sentence in sentence_list if sentence.strip()]\n",
    "    )\n",
    "\n",
    "\n",
    "# Store sentences in the FAISS vector database\n",
    "vector_store = FAISS.from_texts(sentences, embedding=embeddings)\n",
    "\n",
    "# Search vectors using keywords \"äººå·¥æ™ºèƒ½\"\n",
    "search_results = vector_store.similarity_search(\"äººå·¥æ™ºèƒ½\", k=3)\n",
    "\n",
    "# check result\n",
    "print(\"Search result\")\n",
    "for idx, result in enumerate(search_results, start=1):\n",
    "    print(f\"{idx}. {result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare translation using LLM and translation using RAG.\n",
    "\n",
    "First, write the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# NLTKì˜ ë¬¸ì¥ í† í°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ ì‹¤í–‰ í•„ìš”)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Document Search Function (Used in RAG)\n",
    "def retrieve_relevant_docs(query, vector_store, k=3):\n",
    "\n",
    "    # Perform search and return relevant documents\n",
    "    search_results = vector_store.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in search_results]\n",
    "\n",
    "\n",
    "# Translation using only LLM\n",
    "def translate_with_llm(chinese_text):\n",
    "\n",
    "    prompt_template_llm = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Translate the following Chinese sentence into Korean:\",\n",
    "            ),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\"user\", \"Please provide an accurate translation.\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_llm = RunnableSequence(prompt_template_llm, llm)\n",
    "\n",
    "    return translation_chain_llm.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# RAG-based Translation\n",
    "def translate_with_rag(chinese_text, vector_store):\n",
    "\n",
    "    retrieved_docs = retrieve_relevant_docs(chinese_text, vector_store)\n",
    "\n",
    "    # Add retrieved documents as context\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Construct prompt template (Using RAG)\n",
    "\n",
    "    prompt_template_rag = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Below is the Chinese text that needs to be translated into Korean. Additionally, the following context has been provided from relevant documents that might help you in producing a more accurate and context-aware translation.\",\n",
    "            ),\n",
    "            (\"system\", f\"Context (Relevant Documents):\\n{context}\"),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Please provide a translation that is both accurate and reflects the context from the documents provided.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_rag = RunnableSequence(prompt_template_rag, llm)\n",
    "\n",
    "    # Request translation using RAG\n",
    "\n",
    "    return translation_chain_rag.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# Function to store document text as a list\n",
    "def chinese_text_from_file_loader(path):\n",
    "    \"\"\"\n",
    "    íŒŒì¼ì—ì„œ ì¤‘êµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•˜ê³  ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    return split_chinese_sentences_from_docs(docs)\n",
    "\n",
    "\n",
    "def split_chinese_sentences_from_docs(docs):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content  # ë¬¸ì„œ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        sentences.extend(split_chinese_sentences(text))  # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¶”ê°€\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_chinese_sentences(text):\n",
    "    \"\"\"\n",
    "    - ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ê³¼ ë¬¸ì¥ë¶€í˜¸ë¥¼ í•¨ê»˜ ë¶„ë¦¬.\n",
    "    - ë¬¸ì¥ê³¼ ë¬¸ì¥ë¶€í˜¸ë¥¼ ë‹¤ì‹œ ê²°í•©í•˜ì—¬ ë°˜í™˜.\n",
    "    \"\"\"\n",
    "    # ë¬¸ì¥ê³¼ ë¬¸ì¥ë¶€í˜¸ ë¶„ë¦¬\n",
    "    sentence_list = re.split(r\"([ã€‚ï¼ï¼Ÿ])\", text)\n",
    "\n",
    "    # ë¬¸ì¥ê³¼ ë¬¸ì¥ë¶€í˜¸ë¥¼ ê²°í•©í•˜ì—¬ ë³µì›\n",
    "    merged_sentences = [\n",
    "        \"\".join(x) for x in zip(sentence_list[0::2], sentence_list[1::2])\n",
    "    ]\n",
    "\n",
    "    # ë¹ˆ ë¬¸ì¥ ì œê±° í›„ ë°˜í™˜\n",
    "    return [sentence.strip() for sentence in merged_sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "def count_chinese_sentences(docs):\n",
    "    if isinstance(docs, str):\n",
    "        # `input_data`ê°€ ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° ë°”ë¡œ ì²˜ë¦¬\n",
    "        sentences = split_chinese_sentences(docs)\n",
    "\n",
    "    print(f\"ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}\")\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_english_sentences_from_docs(docs):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ë¬¸ì¥ì„ ë¶„ë¦¬í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content  # ë¬¸ì„œ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        sentences.extend(split_english_sentences(text))  # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¶”ê°€\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_english_sentences(text):\n",
    "    \"\"\"\n",
    "    - NLTKì˜ `sent_tokenize()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì •í™•í•˜ê²Œ ë¶„ë¦¬.\n",
    "    - ê¸°ë³¸ì ìœ¼ë¡œ ë§ˆì¹¨í‘œ(`.`), ë¬¼ìŒí‘œ(`?`), ëŠë‚Œí‘œ(`!`)ë¥¼ ì¸ì‹í•˜ì—¬ ë¬¸ì¥ì„ êµ¬ë¶„.\n",
    "    \"\"\"\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "\n",
    "def count_paragraphs_and_sentences(docs):\n",
    "    \"\"\"\n",
    "    ì£¼ì–´ì§„ íŒŒì¼ì—ì„œ ì˜ì–´ ë¬¸ì¥ì˜ ê°œìˆ˜ë¥¼ ì„¸ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if isinstance(docs, str):\n",
    "        # `input_data`ê°€ ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° ë°”ë¡œ ì²˜ë¦¬\n",
    "        paragraphs = paragraphs = re.split(r\"\\n\\s*\\n\", docs.strip())\n",
    "        paragraphs = [\n",
    "            para.strip() for para in paragraphs if para.strip()\n",
    "        ]  # ë¹ˆ ë¬¸ë‹¨ ì œê±°\n",
    "        sentences = [sent for para in paragraphs for sent in sent_tokenize(para)]\n",
    "\n",
    "        print(f\"ğŸ“Œ ì „ì²´ ë¬¸ë‹¨ ê°œìˆ˜: {len(paragraphs)}\")\n",
    "        print(f\"ğŸ“Œ ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: {len(sentences)}\")\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the written functions to perform the comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input chinese text\n",
      "ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: 15\n",
      "æ•°æ®é¢†åŸŸè¿æ¥å›½å®¶æ ‡å‡†ã€‚10æœˆ8æ—¥ï¼Œå›½å®¶å‘æ”¹å§”ç­‰éƒ¨é—¨å‘å¸ƒå…³äºå°å‘ã€Šå›½å®¶æ•°æ®æ ‡å‡†ä½“ç³»å»ºè®¾æŒ‡å—ã€‹(ä»¥ä¸‹ç®€ç§°ã€ŠæŒ‡å—ã€‹)çš„é€šçŸ¥ã€‚ä¸ºâ€œå……åˆ†å‘æŒ¥æ ‡å‡†åœ¨æ¿€æ´»æ•°æ®è¦ç´ æ½œèƒ½ã€åšå¼ºåšä¼˜åšå¤§æ•°å­—ç»æµç­‰æ–¹é¢çš„è§„èŒƒå’Œå¼•é¢†ä½œç”¨â€ï¼Œå›½å®¶å‘å±•æ”¹é©å§”ã€å›½å®¶æ•°æ®å±€ã€ä¸­å¤®ç½‘ä¿¡åŠã€å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨ã€è´¢æ”¿éƒ¨ã€å›½å®¶æ ‡å‡†å§”ç»„ç»‡ç¼–åˆ¶äº†ã€Šå›½å®¶æ•°æ®æ ‡å‡†ä½“ç³»å»ºè®¾æŒ‡å—ã€‹ã€‚ã€ŠæŒ‡å—ã€‹æå‡ºï¼Œåˆ°2026å¹´åº•ï¼ŒåŸºæœ¬å»ºæˆå›½å®¶æ•°æ®æ ‡å‡†ä½“ç³»ï¼Œå›´ç»•æ•°æ®æµé€šåˆ©ç”¨åŸºç¡€è®¾æ–½ã€æ•°æ®ç®¡ç†ã€æ•°æ®æœåŠ¡ã€è®­ç»ƒæ•°æ®é›†ã€å…¬å…±æ•°æ®æˆæƒè¿è¥ã€æ•°æ®ç¡®æƒã€æ•°æ®èµ„æºå®šä»·ã€ä¼ä¸šæ•°æ®èŒƒå¼äº¤æ˜“ç­‰æ–¹é¢åˆ¶ä¿®è®¢30é¡¹ä»¥ä¸Šæ•°æ®é¢†åŸŸåŸºç¡€é€šç”¨å›½å®¶æ ‡å‡†ï¼Œå½¢æˆä¸€æ‰¹æ ‡å‡†åº”ç”¨ç¤ºèŒƒæ¡ˆä¾‹ï¼Œå»ºæˆæ ‡å‡†éªŒè¯å’Œåº”ç”¨æœåŠ¡å¹³å°ï¼ŒåŸ¹è‚²ä¸€æ‰¹å…·å¤‡æ•°æ®ç®¡ç†èƒ½åŠ›è¯„ä¼°ã€æ•°æ®è¯„ä»·ã€æ•°æ®æœåŠ¡èƒ½åŠ›è¯„ä¼°ã€å…¬å…±æ•°æ®æˆæƒè¿è¥ç»©æ•ˆè¯„ä¼°ç­‰èƒ½åŠ›çš„ç¬¬ä¸‰æ–¹æ ‡å‡†åŒ–æœåŠ¡æœºæ„ã€‚ã€ŠæŒ‡å—ã€‹æ˜ç¡®ï¼Œæ•°æ®æ ‡å‡†ä½“ç³»æ¡†æ¶åŒ…å«åŸºç¡€é€šç”¨ã€æ•°æ®åŸºç¡€è®¾æ–½ã€æ•°æ®èµ„æºã€æ•°æ®æŠ€æœ¯ã€æ•°æ®æµé€šã€èåˆåº”ç”¨ã€å®‰å…¨ä¿éšœç­‰7ä¸ªéƒ¨åˆ†ã€‚æ•°æ®åŸºç¡€è®¾æ–½æ–¹é¢ï¼Œæ ‡å‡†æ¶‰åŠå­˜ç®—è®¾æ–½ä¸­çš„æ•°æ®ç®—åŠ›è®¾æ–½ã€æ•°æ®å­˜å‚¨è®¾æ–½ï¼Œç½‘ç»œè®¾æ–½ä¸­çš„5Gç½‘ç»œæ•°æ®ä¼ è¾“ã€å…‰çº¤æ•°æ®ä¼ è¾“ã€å«æ˜Ÿäº’è”ç½‘æ•°æ®ä¼ è¾“ï¼Œæ­¤å¤–è¿˜æœ‰æµé€šåˆ©ç”¨è®¾æ–½ã€‚æ•°æ®æµé€šæ–¹é¢ï¼Œæ ‡å‡†åŒ…æ‹¬æ•°æ®äº§å“ã€æ•°æ®ç¡®æƒã€æ•°æ®èµ„æºå®šä»·ã€æ•°æ®æµé€šäº¤æ˜“ã€‚èåˆåº”ç”¨æ–¹é¢ï¼Œæ ‡å‡†æ¶‰åŠå·¥ä¸šåˆ¶é€ ã€å†œä¸šå†œæ‘ã€å•†è´¸æµé€šã€äº¤é€šè¿è¾“ã€é‡‘èæœåŠ¡ã€ç§‘æŠ€åˆ›æ–°ã€æ–‡åŒ–æ—…æ¸¸(æ–‡ç‰©)ã€å«ç”Ÿå¥åº·ã€åº”æ€¥ç®¡ç†ã€æ°”è±¡æœåŠ¡ã€åŸå¸‚æ²»ç†ã€ç»¿è‰²ä½ç¢³ã€‚å®‰å…¨ä¿éšœæ–¹é¢ï¼Œæ ‡å‡†æ¶‰åŠæ•°æ®åŸºç¡€è®¾æ–½å®‰å…¨ï¼Œæ•°æ®è¦ç´ å¸‚åœºå®‰å…¨ï¼Œæ•°æ®æµé€šå®‰å…¨ã€‚æ•°æ®èµ„æºä¸­çš„æ•°æ®æ²»ç†æ ‡å‡†åŒ…æ‹¬æ•°æ®ä¸šåŠ¡è§„åˆ’ã€æ•°æ®è´¨é‡ç®¡ç†ã€æ•°æ®è°ƒæŸ¥ç›˜ç‚¹ã€æ•°æ®èµ„æºç™»è®°ï¼›è®­ç»ƒæ•°æ®é›†æ–¹é¢çš„æ ‡å‡†åŒ…æ‹¬è®­ç»ƒæ•°æ®é›†é‡‡é›†å¤„ç†ã€è®­ç»ƒæ•°æ®é›†æ ‡æ³¨ã€è®­ç»ƒæ•°æ®é›†åˆæˆã€‚åœ¨ç»„ç»‡ä¿éšœæ–¹é¢ï¼Œå°†æŒ‡å¯¼å»ºç«‹å…¨å›½æ•°æ®æ ‡å‡†åŒ–æŠ€æœ¯ç»„ç»‡ï¼ŒåŠ å¿«æ¨è¿›æ€¥ç”¨ã€æ€¥éœ€æ•°æ®æ ‡å‡†åˆ¶ä¿®è®¢å·¥ä½œï¼Œå¼ºåŒ–ä¸æœ‰å…³æ ‡å‡†åŒ–æŠ€æœ¯ç»„ç»‡ã€è¡Œä¸šã€åœ°æ–¹åŠç›¸å…³ç¤¾å›¢ç»„ç»‡ä¹‹é—´çš„æ²Ÿé€šåä½œã€åè°ƒè”åŠ¨ï¼Œä»¥æ ‡å‡†åŒ–ä¿ƒè¿›æ•°æ®äº§ä¸šç”Ÿæ€å»ºè®¾ã€‚åŒæ—¶è¿˜å°†å®Œå–„æ ‡å‡†è¯•ç‚¹æ”¿ç­–é…å¥—ï¼Œæ­å»ºæ•°æ®æ ‡å‡†åŒ–å…¬å…±æœåŠ¡å¹³å°ï¼Œå¼€å±•æ ‡å‡†å®£è´¯ï¼Œé€‰æ‹©é‡ç‚¹åœ°æ–¹ã€è¡Œä¸šå…ˆè¡Œå…ˆè¯•ï¼Œæ‰“é€ å…¸å‹ç¤ºèŒƒã€‚æ¢ç´¢æ¨åŠ¨æ•°æ®äº§å“ç¬¬ä¸‰æ–¹æ£€éªŒæ£€æµ‹ï¼Œæ·±åŒ–æ•°æ®æ ‡å‡†å®æ–½è¯„ä»·ç®¡ç†ã€‚åœ¨äººæ‰åŸ¹å…»æ–¹é¢ï¼Œå°†æ‰“é€ æ ‡å‡†é…å¥—çš„æ•°æ®äººæ‰åŸ¹è®­è¯¾ç¨‹ï¼Œå½¢æˆä¸€æ‰¹æ•°æ®æ ‡å‡†åŒ–ä¸“ä¸šäººæ‰ã€‚ä¼˜åŒ–æ•°æ®å›½é™…æ ‡å‡†åŒ–ä¸“å®¶é˜Ÿä¼ï¼Œæ”¯æŒå‚ä¸å›½é™…æ ‡å‡†åŒ–æ´»åŠ¨ï¼Œå¼ºåŒ–å›½é™…äº¤æµã€‚\n",
      "\n",
      "Translation using LLM\n",
      "ğŸ“Œ ì „ì²´ ë¬¸ë‹¨ ê°œìˆ˜: 1\n",
      "ğŸ“Œ ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: 15\n",
      "ë°ì´í„° ë¶„ì•¼ì— êµ­ê°€ í‘œì¤€ì´ ë„ì…ë©ë‹ˆë‹¤. 10ì›” 8ì¼, êµ­ê°€ë°œì „ê°œí˜ìœ„ì›íšŒ ë“± ê´€ë ¨ ë¶€ì²˜ëŠ” ã€Šêµ­ê°€ ë°ì´í„° í‘œì¤€ ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œë¼ì¸ã€‹(ì´í•˜ ã€Šê°€ì´ë“œë¼ì¸ã€‹)ì˜ ì¸ì‡„ì— ê´€í•œ í†µì§€ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. â€œë°ì´í„° ìš”ì†Œì˜ ì ì¬ë ¥ì„ í™œì„±í™”í•˜ê³ , ë””ì§€í„¸ ê²½ì œë¥¼ ê°•í•˜ê²Œ í•˜ê³  ìš°ìˆ˜í•˜ê²Œ í•˜ë©° í¬ê²Œ ë§Œë“œëŠ” ë° ìˆì–´ í‘œì¤€ì˜ ê·œë²”ì  ì—­í• ê³¼ ì„ ë„ì  ì—­í• ì„ ì¶©ë¶„íˆ ë°œíœ˜í•˜ê¸° ìœ„í•´â€, êµ­ê°€ë°œì „ê°œí˜ìœ„ì›íšŒ, êµ­ê°€ë°ì´í„°êµ­, ì¤‘ì•™ ì¸í„°ë„· ì •ë³´ ì‚¬ë¬´ì†Œ, ì‚°ì—…ì •ë³´í™”ë¶€, ì¬ì •ë¶€, êµ­ê°€í‘œì¤€ìœ„ì›íšŒê°€ ã€Šêµ­ê°€ ë°ì´í„° í‘œì¤€ ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œë¼ì¸ã€‹ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. ã€Šê°€ì´ë“œë¼ì¸ã€‹ì€ 2026ë…„ ë§ê¹Œì§€ êµ­ê°€ ë°ì´í„° í‘œì¤€ ì²´ê³„ì˜ ê¸°ë³¸ì ì¸ êµ¬ì¶•ì„ ì™„ë£Œí•˜ê³ , ë°ì´í„° ìœ í†µê³¼ í™œìš©ì„ ìœ„í•œ ì¸í”„ë¼, ë°ì´í„° ê´€ë¦¬, ë°ì´í„° ì„œë¹„ìŠ¤, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸, ê³µê³µ ë°ì´í„° ê¶Œí•œ ìš´ì˜, ë°ì´í„° ê¶Œë¦¬ í™•ë¦½, ë°ì´í„° ìì› ê°€ê²© ì±…ì •, ê¸°ì—… ë°ì´í„° íŒ¨ëŸ¬ë‹¤ì„ ê±°ë˜ ë“± 30ê°œ ì´ìƒì˜ ë°ì´í„° ë¶„ì•¼ì˜ ê¸°ë³¸ì ì¸ ì¼ë°˜ êµ­ê°€ í‘œì¤€ì„ ì œì • ë° ê°œì •í•˜ì—¬, ì¼ë ¨ì˜ í‘œì¤€ ì ìš© ì‹œë²” ì‚¬ë¡€ë¥¼ í˜•ì„±í•˜ê³ , í‘œì¤€ ê²€ì¦ ë° ì ìš© ì„œë¹„ìŠ¤ í”Œë«í¼ì„ êµ¬ì¶•í•˜ë©°, ë°ì´í„° ê´€ë¦¬ ëŠ¥ë ¥ í‰ê°€, ë°ì´í„° í‰ê°€, ë°ì´í„° ì„œë¹„ìŠ¤ ëŠ¥ë ¥ í‰ê°€, ê³µê³µ ë°ì´í„° ê¶Œí•œ ìš´ì˜ ì„±ê³¼ í‰ê°€ ë“±ì˜ ëŠ¥ë ¥ì„ ê°–ì¶˜ ì œ3ì í‘œì¤€í™” ì„œë¹„ìŠ¤ ê¸°ê´€ì„ ìœ¡ì„±í•  ê³„íšì´ë¼ê³  ëª…ì‹œí–ˆìŠµë‹ˆë‹¤. ã€Šê°€ì´ë“œë¼ì¸ã€‹ì€ ë°ì´í„° í‘œì¤€ ì²´ê³„ì˜ í”„ë ˆì„ì›Œí¬ê°€ ê¸°ë³¸ ì¼ë°˜, ë°ì´í„° ì¸í”„ë¼, ë°ì´í„° ìì›, ë°ì´í„° ê¸°ìˆ , ë°ì´í„° ìœ í†µ, ìœµí•© ì‘ìš©, ì•ˆì „ ë³´ì¥ ë“± 7ê°œ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤ê³  ëª…í™•íˆ í–ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì¸í”„ë¼ ë¶€ë¶„ì—ì„œëŠ” í‘œì¤€ì´ ë°ì´í„° ì—°ì‚° ì‹œì„¤ê³¼ ë°ì´í„° ì €ì¥ ì‹œì„¤, ë„¤íŠ¸ì›Œí¬ ì‹œì„¤ì˜ 5G ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì „ì†¡, ê´‘ì„¬ìœ  ë°ì´í„° ì „ì†¡, ìœ„ì„± ì¸í„°ë„· ë°ì´í„° ì „ì†¡, ê·¸ë¦¬ê³  ìœ í†µ í™œìš© ì‹œì„¤ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤. ë°ì´í„° ìœ í†µ ë¶€ë¶„ì—ì„œëŠ” ë°ì´í„° ì œí’ˆ, ë°ì´í„° ê¶Œë¦¬ í™•ë¦½, ë°ì´í„° ìì› ê°€ê²© ì±…ì •, ë°ì´í„° ìœ í†µ ê±°ë˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ìœµí•© ì‘ìš© ë¶€ë¶„ì—ì„œëŠ” ì‚°ì—… ì œì¡°, ë†ì—… ë° ë†ì´Œ, ìƒì—… ìœ í†µ, êµí†µ ìš´ì†¡, ê¸ˆìœµ ì„œë¹„ìŠ¤, ê³¼í•™ ê¸°ìˆ  í˜ì‹ , ë¬¸í™” ê´€ê´‘(ë¬¸í™”ì¬), ë³´ê±´ ê±´ê°•, ê¸´ê¸‰ ê´€ë¦¬, ê¸°ìƒ ì„œë¹„ìŠ¤, ë„ì‹œ ê´€ë¦¬, ë…¹ìƒ‰ ì €íƒ„ì†Œ ë“±ì„ í¬í•¨í•œ í‘œì¤€ì´ ë‹¤ë¤„ì§‘ë‹ˆë‹¤. ì•ˆì „ ë³´ì¥ ë¶€ë¶„ì—ì„œëŠ” ë°ì´í„° ì¸í”„ë¼ ì•ˆì „, ë°ì´í„° ìš”ì†Œ ì‹œì¥ ì•ˆì „, ë°ì´í„° ìœ í†µ ì•ˆì „ê³¼ ê´€ë ¨ëœ í‘œì¤€ì´ í¬í•¨ë©ë‹ˆë‹¤. ë°ì´í„° ìì› ì¤‘ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í‘œì¤€ì—ëŠ” ë°ì´í„° ì—…ë¬´ ê³„íš, ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬, ë°ì´í„° ì¡°ì‚¬ ë° ì ê²€, ë°ì´í„° ìì› ë“±ë¡ì´ í¬í•¨ë©ë‹ˆë‹¤; í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ ê´€ë ¨ í‘œì¤€ì—ëŠ” í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ ìˆ˜ì§‘ ì²˜ë¦¬, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ ì£¼ì„, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ í•©ì„±ì´ í¬í•¨ë©ë‹ˆë‹¤. ì¡°ì§ ë³´ì¥ ì¸¡ë©´ì—ì„œëŠ” ì „êµ­ ë°ì´í„° í‘œì¤€í™” ê¸°ìˆ  ì¡°ì§ì„ ì„¤ë¦½í•˜ë„ë¡ ì§€ë„í•˜ê³ , ê¸´ê¸‰ í•„ìš” ë°ì´í„° í‘œì¤€ì˜ ì œì • ë° ê°œì • ì‘ì—…ì„ ê°€ì†í™”í•˜ë©°, ê´€ë ¨ í‘œì¤€í™” ê¸°ìˆ  ì¡°ì§, ì‚°ì—…, ì§€ì—­ ë° ê´€ë ¨ ë‹¨ì²´ ê°„ì˜ ì†Œí†µ í˜‘ë ¥, ì¡°ì •ì„ ê°•í™”í•˜ì—¬ í‘œì¤€í™”ë¥¼ í†µí•´ ë°ì´í„° ì‚°ì—… ìƒíƒœê³„ êµ¬ì¶•ì„ ì´‰ì§„í•  ê²ƒì…ë‹ˆë‹¤. ë˜í•œ í‘œì¤€ ì‹œë²” ì •ì±…ì„ ë³´ì™„í•˜ê³ , ë°ì´í„° í‘œì¤€í™” ê³µê³µ ì„œë¹„ìŠ¤ í”Œë«í¼ì„ êµ¬ì¶•í•˜ë©°, í‘œì¤€ ì„ ì „ í™œë™ì„ ì§„í–‰í•˜ê³ , ì£¼ìš” ì§€ì—­ê³¼ ì‚°ì—…ì—ì„œ ì„ ë„ì ìœ¼ë¡œ ì‹œë²” ì‚¬ì—…ì„ ì§„í–‰í•˜ì—¬ ì „í˜•ì ì¸ ì‚¬ë¡€ë¥¼ ë§Œë“¤ì–´ ë‚¼ ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° ì œí’ˆì˜ ì œ3ì ê²€ì‚¬ ë° ê²€ì‚¬ë¥¼ ì´‰ì§„í•˜ê³  ë°ì´í„° í‘œì¤€ ì‹œí–‰ í‰ê°€ ê´€ë¦¬ì˜ ì‹¬í™”ë¥¼ íƒìƒ‰í•  ê²ƒì…ë‹ˆë‹¤. ì¸ì¬ ì–‘ì„± ì¸¡ë©´ì—ì„œëŠ” í‘œì¤€í™”ëœ ë°ì´í„° ì¸ì¬ êµìœ¡ ê³¼ì •ì„ ê°œë°œí•˜ê³ , ë°ì´í„° í‘œì¤€í™” ì „ë¬¸ ì¸ì¬ë¥¼ ì–‘ì„±í•  ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° êµ­ì œ í‘œì¤€í™” ì „ë¬¸ê°€ íŒ€ì„ ìµœì í™”í•˜ê³ , êµ­ì œ í‘œì¤€í™” í™œë™ì— ì°¸ì—¬ë¥¼ ì§€ì›í•˜ë©°, êµ­ì œ êµë¥˜ë¥¼ ê°•í™”í•  ê²ƒì…ë‹ˆë‹¤.\n",
      "\n",
      "Translation using RAG\n",
      "ğŸ“Œ ì „ì²´ ë¬¸ë‹¨ ê°œìˆ˜: 1\n",
      "ğŸ“Œ ì „ì²´ ë¬¸ì¥ ê°œìˆ˜: 15\n",
      "ë°ì´í„° ë¶„ì•¼ êµ­ê°€ í‘œì¤€ì´ ë„ì…ë˜ì—ˆìŠµë‹ˆë‹¤. 10ì›” 8ì¼, êµ­ê°€ ë°œì „ ê°œí˜ ìœ„ì›íšŒ ë“± ë¶€ì²˜ëŠ” ã€Šêµ­ê°€ ë°ì´í„° í‘œì¤€ ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œë¼ì¸ã€‹(ì´í•˜ ã€Šê°€ì´ë“œë¼ì¸ã€‹)ì˜ ë°°í¬ì— ê´€í•œ í†µì§€ë¥¼ ë°œí‘œí–ˆìŠµë‹ˆë‹¤. ì´ëŠ” \"ë°ì´í„° ìš”ì†Œì˜ ì ì¬ë ¥ì„ í™œì„±í™”í•˜ê³ , ë””ì§€í„¸ ê²½ì œë¥¼ ê°•í•˜ê²Œ í•˜ê³ , ìš°ìˆ˜í•˜ê²Œ í•˜ê³ , í¬ê²Œ ë§Œë“œëŠ” ë° ìˆì–´ í‘œì¤€ì˜ ê·œë²”ì  ë° ì„ ë„ì  ì—­í• ì„ ì¶©ë¶„íˆ ë°œíœ˜í•˜ê¸° ìœ„í•´\" êµ­ê°€ ë°œì „ ê°œí˜ ìœ„ì›íšŒ, êµ­ê°€ ë°ì´í„°êµ­, ì¤‘ì•™ ì¸í„°ë„· ì •ë³´ ì‚¬ë¬´ì†Œ, ì‚°ì—… ë° ì •ë³´í™” ë¶€, ì¬ë¬´ë¶€, êµ­ê°€ í‘œì¤€ ìœ„ì›íšŒê°€ ã€Šêµ­ê°€ ë°ì´í„° í‘œì¤€ ì²´ê³„ êµ¬ì¶• ê°€ì´ë“œë¼ì¸ã€‹ì„ í¸ì°¬í–ˆìŠµë‹ˆë‹¤. ã€Šê°€ì´ë“œë¼ì¸ã€‹ì€ 2026ë…„ ë§ê¹Œì§€ êµ­ê°€ ë°ì´í„° í‘œì¤€ ì²´ê³„ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ êµ¬ì¶•í•˜ê³ , ë°ì´í„° ìœ í†µ í™œìš© ì¸í”„ë¼, ë°ì´í„° ê´€ë¦¬, ë°ì´í„° ì„œë¹„ìŠ¤, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸, ê³µê³µ ë°ì´í„° í—ˆê°€ ìš´ì˜, ë°ì´í„° ê¶Œë¦¬ í™•ì¸, ë°ì´í„° ìì› ê°€ê²© ì±…ì •, ê¸°ì—… ë°ì´í„° íŒ¨ëŸ¬ë‹¤ì„ ê±°ë˜ ë“± 30ê°œ ì´ìƒì˜ ë°ì´í„° ë¶„ì•¼ ê¸°ë³¸ ê³µí†µ êµ­ê°€ í‘œì¤€ì„ ì œì • ë° ìˆ˜ì •í•˜ë©°, ì¼ë ¨ì˜ í‘œì¤€ ì ìš© ì‹œë²” ì‚¬ë¡€ë¥¼ í˜•ì„±í•˜ê³ , í‘œì¤€ ê²€ì¦ ë° ì‘ìš© ì„œë¹„ìŠ¤ í”Œë«í¼ì„ êµ¬ì¶•í•˜ë©°, ë°ì´í„° ê´€ë¦¬ ëŠ¥ë ¥ í‰ê°€, ë°ì´í„° í‰ê°€, ë°ì´í„° ì„œë¹„ìŠ¤ ëŠ¥ë ¥ í‰ê°€, ê³µê³µ ë°ì´í„° í—ˆê°€ ìš´ì˜ ì„±ê³¼ í‰ê°€ ë“±ì˜ ëŠ¥ë ¥ì„ ê°–ì¶˜ ì œ3ì í‘œì¤€í™” ì„œë¹„ìŠ¤ ê¸°ê´€ì„ ìœ¡ì„±í•  ê³„íšì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ã€Šê°€ì´ë“œë¼ì¸ã€‹ì€ ë°ì´í„° í‘œì¤€ ì²´ê³„ì˜ í”„ë ˆì„ì›Œí¬ê°€ ê¸°ë³¸ ê³µí†µ, ë°ì´í„° ê¸°ë°˜ ì‹œì„¤, ë°ì´í„° ìì›, ë°ì´í„° ê¸°ìˆ , ë°ì´í„° ìœ í†µ, ìœµí•© ì‘ìš©, ì•ˆì „ ë³´ì¥ ë“± 7ê°œ ë¶€ë¶„ìœ¼ë¡œ êµ¬ì„±ëœë‹¤ê³  ëª…ì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ ì‹œì„¤ ê´€ë ¨ í‘œì¤€ì€ ë°ì´í„° ê³„ì‚° ì‹œì„¤, ë°ì´í„° ì €ì¥ ì‹œì„¤ì„ í¬í•¨í•œ ì €ì¥ ë° ê³„ì‚° ì‹œì„¤, 5G ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ì „ì†¡, ê´‘ì„¬ìœ  ë°ì´í„° ì „ì†¡, ìœ„ì„± ì¸í„°ë„· ë°ì´í„° ì „ì†¡ì„ í¬í•¨í•œ ë„¤íŠ¸ì›Œí¬ ì‹œì„¤, ê·¸ë¦¬ê³  ìœ í†µ í™œìš© ì‹œì„¤ì„ í¬í•¨í•©ë‹ˆë‹¤. ë°ì´í„° ìœ í†µ ê´€ë ¨ í‘œì¤€ì€ ë°ì´í„° ì œí’ˆ, ë°ì´í„° ê¶Œë¦¬ í™•ì¸, ë°ì´í„° ìì› ê°€ê²© ì±…ì •, ë°ì´í„° ìœ í†µ ê±°ë˜ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ìœµí•© ì‘ìš© ê´€ë ¨ í‘œì¤€ì€ ì‚°ì—… ì œì¡°, ë†ì—… ë° ë†ì´Œ, ìƒì—… ìœ í†µ, êµí†µ ìš´ì†¡, ê¸ˆìœµ ì„œë¹„ìŠ¤, ê³¼í•™ ê¸°ìˆ  í˜ì‹ , ë¬¸í™” ê´€ê´‘(ë¬¸í™”ì¬), ë³´ê±´ ê±´ê°•, ë¹„ìƒ ê´€ë¦¬, ê¸°ìƒ ì„œë¹„ìŠ¤, ë„ì‹œæ²»ç†, ì¹œí™˜ê²½ ì €íƒ„ì†Œ ë¶„ì•¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì•ˆì „ ë³´ì¥ ê´€ë ¨ í‘œì¤€ì€ ë°ì´í„° ê¸°ë°˜ ì‹œì„¤ì˜ ì•ˆì „, ë°ì´í„° ìš”ì†Œ ì‹œì¥ì˜ ì•ˆì „, ë°ì´í„° ìœ í†µì˜ ì•ˆì „ì„ í¬í•¨í•©ë‹ˆë‹¤. ë°ì´í„° ìì› ë‚´ ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í‘œì¤€ì€ ë°ì´í„° ì‚¬ì—… ê³„íš, ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬, ë°ì´í„° ì¡°ì‚¬ ë° ì ê²€, ë°ì´í„° ìì› ë“±ë¡ì„ í¬í•¨í•˜ë©°; í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ ê´€ë ¨ í‘œì¤€ì€ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ ìˆ˜ì§‘ ë° ì²˜ë¦¬, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ ì£¼ì„, í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ í•©ì„±ì„ í¬í•¨í•©ë‹ˆë‹¤. ì¡°ì§ ë³´ì¥ ì¸¡ë©´ì—ì„œëŠ” ì „êµ­ ë°ì´í„° í‘œì¤€í™” ê¸°ìˆ  ì¡°ì§ì„ ì„¤ë¦½í•˜ë„ë¡ ì•ˆë‚´í•˜ê³ , ê¸´ê¸‰íˆ í•„ìš”í•œ ë°ì´í„° í‘œì¤€ì˜ ì œì • ë° ìˆ˜ì •ì„ ê°€ì†í™”í•˜ë©°, ê´€ë ¨ í‘œì¤€í™” ê¸°ìˆ  ì¡°ì§, ì‚°ì—…, ì§€ì—­ ë° ê´€ë ¨ ë‹¨ì²´ ê°„ì˜ ì†Œí†µ í˜‘ë ¥ ë° ì¡°ì • ì—°ê³„ë¥¼ ê°•í™”í•˜ì—¬ í‘œì¤€í™”ë¥¼ í†µí•´ ë°ì´í„° ì‚°ì—… ìƒíƒœê³„ êµ¬ì¶•ì„ ì´‰ì§„í•  ì˜ˆì •ì…ë‹ˆë‹¤. ë˜í•œ í‘œì¤€ ì‹œë²” ì •ì±…ì„ ë³´ì™„í•˜ê³ , ë°ì´í„° í‘œì¤€í™” ê³µê³µ ì„œë¹„ìŠ¤ í”Œë«í¼ì„ êµ¬ì¶•í•˜ë©°, í‘œì¤€ ë³´ê¸‰ì„ ì‹¤ì‹œí•˜ê³ , ì£¼ìš” ì§€ì—­ ë° ì‚°ì—…ì—ì„œ ì„ í–‰ ì‹œí—˜ì„ ì„ íƒí•˜ì—¬ ì „í˜•ì ì¸ ì‹œë²”ì„ êµ¬ì¶•í•  ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° ì œí’ˆì˜ ì œ3ì ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ë¥¼ ì´‰ì§„í•˜ê³ , ë°ì´í„° í‘œì¤€ ì‹œí–‰ í‰ê°€ ê´€ë¦¬ë¥¼ ì‹¬í™”í•  ê³„íšì…ë‹ˆë‹¤. ì¸ì¬ ì–‘ì„± ì¸¡ë©´ì—ì„œëŠ” í‘œì¤€ì— ë¶€í•©í•˜ëŠ” ë°ì´í„° ì¸ì¬ êµìœ¡ ê³¼ì •ì„ ê°œë°œí•˜ê³ , ë°ì´í„° í‘œì¤€í™” ì „ë¬¸ ì¸ë ¥ì„ ì–‘ì„±í•  ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° êµ­ì œ í‘œì¤€í™” ì „ë¬¸ê°€ íŒ€ì„ ìµœì í™”í•˜ê³ , êµ­ì œ í‘œì¤€í™” í™œë™ì— ì°¸ì—¬ë¥¼ ì§€ì›í•˜ë©°, êµ­ì œ êµë¥˜ë¥¼ ê°•í™”í•  ê²ƒì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# sentences = chinese_text_from_file_loader(\"data/comparison_cn.txt\")\n",
    "sentences = chinese_text_from_file_loader(\"data/comparison_cn copy.txt\")\n",
    "\n",
    "chinese_text = \"\"\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    chinese_text += sentence\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm_translation = translate_with_llm(chinese_text)\n",
    "\n",
    "\n",
    "# RAG\n",
    "rag_translation = translate_with_rag(chinese_text, vector_store)\n",
    "\n",
    "\n",
    "print(\"\\ninput chinese text\")\n",
    "count_chinese_sentences(chinese_text)\n",
    "print(chinese_text)\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using LLM\")\n",
    "count_paragraphs_and_sentences(llm_translation.content)\n",
    "print(llm_translation.content)\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using RAG\")\n",
    "count_paragraphs_and_sentences(rag_translation.content)\n",
    "print(rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ìƒì„± ë¬¸ì¥ì˜ ìˆ˜ê°€ ì¼ì¹˜í•´ì•¼í•¨í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make comparison_json\n",
    "def make_comparison_json(chinese_text, translation_1, translation_2):\n",
    "    # ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸° (ì›ë¬¸ê³¼ ë²ˆì—­ì„ ê° ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸°)\n",
    "    # chinese_sentences = chinese_text.split(\"ã€‚\")\n",
    "    chinese_sentences = split_chinese_sentences(chinese_text)\n",
    "    translation_1_sentences = translation_1.split(\".\")\n",
    "    translation_2_sentences = translation_2.split(\".\")\n",
    "\n",
    "    print(\"\\nchinese_sentences:\", len(chinese_sentences))\n",
    "    print(\"\\ntranslation_1_sentences:\", len(translation_1_sentences))\n",
    "    print(\"\\ntranslation_2_sentences:\", len(translation_2_sentences))\n",
    "\n",
    "    # ê° ë¬¸ì¥ì˜ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘í•˜ì—¬ ì €ì¥\n",
    "    data = []\n",
    "    for i in range(len(chinese_sentences)):\n",
    "        # ë¬¸ì¥ë³„ë¡œ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘\n",
    "        sentence_data = {\n",
    "            \"chinese_text\": chinese_sentences[i].strip(),\n",
    "            \"translation_1\": (\n",
    "                translation_1_sentences[i].strip()\n",
    "                if i < len(translation_1_sentences)\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"translation_2\": (\n",
    "                translation_2_sentences[i].strip()\n",
    "                if i < len(translation_2_sentences)\n",
    "                else \"\"\n",
    "            ),\n",
    "        }\n",
    "        data.append(sentence_data)\n",
    "\n",
    "    # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    output_file = \"data/translation_comparison2.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Translation comparison data saved to {output_file}\")\n",
    "\n",
    "    make_comparison_json(chinese_text, llm_translation.content, rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of translation results\n",
    "\n",
    "Evaluation of translation results using BLEU and TER scores.\n",
    "Considering the addition of COMET and GPT for further assessment.\n",
    "Aiming to improve accuracy and quality in translation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸° (ì›ë¬¸ê³¼ ë²ˆì—­ì„ ê° ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸°)\n",
    "chinese_sentences = chinese_text.split(\"ã€‚\")\n",
    "translation_1_sentences = translatllm_translation.content.split(\".\")\n",
    "translation_2_sentences = translation_2.split(\".\")\n",
    "\n",
    "# ê° ë¬¸ì¥ì˜ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘í•˜ì—¬ ì €ì¥\n",
    "data = []\n",
    "for i in range(len(chinese_sentences)):\n",
    "    # ë¬¸ì¥ë³„ë¡œ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘\n",
    "    sentence_data = {\n",
    "        \"chinese_text\": chinese_sentences[i].strip(),\n",
    "        \"translation_1\": (\n",
    "            translation_1_sentences[i].strip()\n",
    "            if i < len(translation_1_sentences)\n",
    "            else \"\"\n",
    "        ),\n",
    "        \"translation_2\": (\n",
    "            translation_2_sentences[i].strip()\n",
    "            if i < len(translation_2_sentences)\n",
    "            else \"\"\n",
    "        ),\n",
    "    }\n",
    "    data.append(sentence_data)\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "output_file = \"translation_comparison.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Translation comparison data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ **Translation Quality Evaluation (BLEU & TER Scores)**\n",
      "\n",
      "â•’â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â••\n",
      "â”‚    â”‚ Category      â”‚ Text                                           â”‚ BLEU   â”‚ TER   â”‚\n",
      "â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚  0 â”‚ Source Text   â”‚ è¿™ä¸ªäº§å“åœ¨å¸‚åœºä¸Šå¾ˆå—æ¬¢è¿ã€‚                     â”‚ -      â”‚ -     â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1 â”‚ Translation 1 â”‚ This product is very popular in the market.    â”‚ 0.0    â”‚ 800.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  2 â”‚ Translation 2 â”‚ This product is well received in the market.   â”‚ 0.0    â”‚ 800.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  3 â”‚ Source Text   â”‚ äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚                         â”‚ -      â”‚ -     â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  4 â”‚ Translation 1 â”‚ Artificial intelligence is changing the world. â”‚ 0.0    â”‚ 600.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  5 â”‚ Translation 2 â”‚ AI is transforming the world.                  â”‚ 0.0    â”‚ 500.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  6 â”‚ Source Text   â”‚ å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­å§ã€‚                       â”‚ -      â”‚ -     â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  7 â”‚ Translation 1 â”‚ The weather is great, let's go to the park.    â”‚ 0.0    â”‚ 900.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  8 â”‚ Translation 2 â”‚ It's nice outside, let's visit the park.       â”‚ 0.0    â”‚ 700.0 â”‚\n",
      "â•˜â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•›\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import sacrebleu\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "#  BLEU\n",
    "def calculate_bleu(reference, candidate):\n",
    "    return round(sacrebleu.sentence_bleu(candidate, [reference]).score, 3)\n",
    "\n",
    "\n",
    "# TER\n",
    "def calculate_ter(reference, candidate):\n",
    "    ter_metric = sacrebleu.metrics.TER()\n",
    "    return round(ter_metric.corpus_score([candidate], [[reference]]).score, 3)\n",
    "\n",
    "\n",
    "json_file_path = \"data/translations_comparison.json\"\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "translations_data = load_json_data(json_file_path)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(translations_data)\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    source_text = row[\"source_text\"]\n",
    "    translation_1 = row[\"translation_1\"]\n",
    "    translation_2 = row[\"translation_2\"]\n",
    "\n",
    "    # translation_1 evaluation\n",
    "    bleu_1 = calculate_bleu(source_text, translation_1)\n",
    "    ter_1 = calculate_ter(source_text, translation_1)\n",
    "\n",
    "    # translation_2 evaluation\n",
    "    bleu_2 = calculate_bleu(source_text, translation_2)\n",
    "    ter_2 = calculate_ter(source_text, translation_2)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Source Text\",\n",
    "            \"Text\": source_text,\n",
    "            \"BLEU\": \"-\",\n",
    "            \"TER\": \"-\",\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Translation 1\",\n",
    "            \"Text\": translation_1,\n",
    "            \"BLEU\": bleu_1,\n",
    "            \"TER\": ter_1,\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Translation 2\",\n",
    "            \"Text\": translation_2,\n",
    "            \"BLEU\": bleu_2,\n",
    "            \"TER\": ter_2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def display_results(dataframe):\n",
    "    print(\"\\nğŸ“Œ **Translation Quality Evaluation (BLEU & TER Scores)**\\n\")\n",
    "    print(tabulate(dataframe, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "display_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-9y5W8e20-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "- Author: [Wonyoung Lee](https://github.com/BaBetterB)\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/BaBetterB/LangChain-OpenTutorial/blob/main/15-Agent/05-Iteration-HumanInTheLoop.ipynb)\n",
    "[![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial compares two approaches to translating Chinese text into English using LangChain.\n",
    "\n",
    "The first approach utilizes a single LLM (e.g. GPT-4) to generate a straightforward translation. The second approach employs Retrieval-Augmented Generation (RAG), which enhances translation accuracy by retrieving relevant documents.\n",
    "\n",
    "The tutorial evaluates the translation accuracy and performance of each method, helping users choose the most suitable approach for their needs.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environement Setup](#environment-setup)\n",
    "- [Translation using LLM](#translation-using-llm)\n",
    "- [Translation using RAG](#translation-using-rag)\n",
    "- [Evaluation of translation results](#evaluation-of-translation-resultsr)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [ `langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load sample text and output the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_community\",\n",
    "        \"load_dotenv\",\n",
    "        \"langchain_openai\",\n",
    "        \"transformers\",\n",
    "        \"faiss-cpu\",\n",
    "        \"sentence_transformers\",\n",
    "        \"sacrebleu\",\n",
    "        \"unbabel-comet\",\n",
    "        \"load_from_checkpoint\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Translation\",  # title\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it.\n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration File for Managing API Keys as Environment Variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API Key Information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using LLM\n",
    "\n",
    "Translation using LLM refers to using a large language model (LLM), such as GPT-4, to translate text from one language to another. \n",
    "The model processes the input text and generates a direct translation based on its pre-trained knowledge. This approach is simple, fast, and effective.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\herme\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-opentutorial-9y5W8e20-py3.11\\Lib\\site-packages\\langsmith\\client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=da9f30b3-832e-4c54-9c26-ca08fba1dc57,id=da9f30b3-832e-4c54-9c26-ca08fba1dc57; trace=da9f30b3-832e-4c54-9c26-ca08fba1dc57,id=68e83e35-78d0-4d81-a77d-f42981cdc9dd; trace=da9f30b3-832e-4c54-9c26-ca08fba1dc57,id=063af90b-12b3-4ff2-a3fe-f3a0c70f653b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese_text: äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œå„å›½éƒ½åœ¨åŠ ç´§ç ”ç©¶å¦‚ä½•åˆ©ç”¨è¿™ä¸€æŠ€æœ¯æé«˜ç”Ÿäº§åŠ›ã€‚\n",
      "Translation: Artificial intelligence is transforming the world, and countries are intensifying their research on how to leverage this technology to enhance productivity.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Create PromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a professional translator.\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Please translate the following Chinese document into natural and accurate English.\"\n",
    "            \"Consider the context and vocabulary to ensure smooth and fluent sentences.:.\\n\\n\"\n",
    "            \"**Chinese Original Text:** {chinese_text}\\n\\n**English Translation:**\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "translation_chain = RunnableSequence(prompt, llm)\n",
    "\n",
    "chinese_text = \"äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œå„å›½éƒ½åœ¨åŠ ç´§ç ”ç©¶å¦‚ä½•åˆ©ç”¨è¿™ä¸€æŠ€æœ¯æé«˜ç”Ÿäº§åŠ›ã€‚\"\n",
    "\n",
    "response = translation_chain.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "print(\"Chinese_text:\", chinese_text)\n",
    "print(\"Translation:\", response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation using RAG \n",
    "\n",
    "Translation using RAG (Retrieval-Augmented Generation) enhances translation accuracy by combining a pre-trained LLM with a retrieval mechanism. It first retrieves relevant documents or data related to the input text, then uses this additional context to generate a more precise and contextually accurate translation. This approach is particularly useful for technical terms, specialized content, or context-sensitive translations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Search Implementation Using FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI for efficient similarity search and clustering of dense vectors. It is widely used for approximate nearest neighbor (ANN) search in large-scale datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=da9f30b3-832e-4c54-9c26-ca08fba1dc57,id=da9f30b3-832e-4c54-9c26-ca08fba1dc57; trace=da9f30b3-832e-4c54-9c26-ca08fba1dc57,id=063af90b-12b3-4ff2-a3fe-f3a0c70f653b\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c,id=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c; trace=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c,id=a4b810cc-37b4-4a1e-b333-fa35c6ddaeb3; trace=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c,id=59f8b6ff-1583-4494-937c-b3f846c777ff\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c,id=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c; trace=48902cf3-6f26-4cdd-a1de-b8b0e6fb787c,id=59f8b6ff-1583-4494-937c-b3f846c777ff\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=54a5e0ae-114c-4c55-8af0-48b57aa370b4,id=54a5e0ae-114c-4c55-8af0-48b57aa370b4; trace=54a5e0ae-114c-4c55-8af0-48b57aa370b4,id=c5055036-e3a9-4e00-8afa-f9d99046e7eb; trace=54a5e0ae-114c-4c55-8af0-48b57aa370b4,id=801868aa-f3a0-407f-9c26-af2464d7062b\n",
      "Failed to multipart ingest runs: langsmith.utils.LangSmithAuthError: Authentication failed for https://api.smith.langchain.com/runs/multipart. HTTPError('401 Client Error: Unauthorized for url: https://api.smith.langchain.com/runs/multipart', '{\"detail\":\"Invalid token\"}')trace=54a5e0ae-114c-4c55-8af0-48b57aa370b4,id=54a5e0ae-114c-4c55-8af0-48b57aa370b4; trace=54a5e0ae-114c-4c55-8af0-48b57aa370b4,id=801868aa-f3a0-407f-9c26-af2464d7062b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search result\n",
      "1. å½“åœ°çƒå‘˜å¹¶éä¸“ä¸šäººå£«ï¼Œè€Œæ˜¯å†œæ°‘ã€å»ºç­‘å·¥äººã€æ•™å¸ˆå’Œå­¦ç”Ÿï¼Œå¯¹è¶³çƒçš„çƒ­çˆ±å°†ä»–ä»¬å‡èšåœ¨ä¸€èµ·\n",
      "2. â€å¡å¡è¯´é“\n",
      "3. â€œè¶³çƒè®©æˆ‘ä»¬ç»“è¯†æ–°æœ‹å‹ï¼Œè¿æ¥æ›´å¹¿é˜”çš„ä¸–ç•Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "file_path = \"data/news_cn.txt\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"file not found!!: {file_path}\")\n",
    "\n",
    "loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "# Vectorizing Sentences Individually\n",
    "sentences = []\n",
    "for doc in docs:\n",
    "    text = doc.page_content\n",
    "    sentence_list = text.split(\"ã€‚\")  # Splitting Chinese sentences based on 'ã€‚'\n",
    "    sentences.extend(\n",
    "        [sentence.strip() for sentence in sentence_list if sentence.strip()]\n",
    "    )\n",
    "\n",
    "\n",
    "# Store sentences in the FAISS vector database\n",
    "vector_store = FAISS.from_texts(sentences, embedding=embeddings)\n",
    "\n",
    "# Search vectors using keywords \"äººå·¥æ™ºèƒ½\"\n",
    "search_results = vector_store.similarity_search(\"äººå·¥æ™ºèƒ½\", k=3)\n",
    "\n",
    "# check result\n",
    "print(\"Search result\")\n",
    "for idx, result in enumerate(search_results, start=1):\n",
    "    print(f\"{idx}. {result.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare translation using LLM and translation using RAG.\n",
    "\n",
    "First, write the necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "# Download the necessary data for sentence tokenization in NLTK (requires initial setup)\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "# Document Search Function (Used in RAG)\n",
    "def retrieve_relevant_docs(query, vector_store, k=3):\n",
    "\n",
    "    # Perform search and return relevant documents\n",
    "    search_results = vector_store.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in search_results]\n",
    "\n",
    "\n",
    "# Translation using only LLM\n",
    "def translate_with_llm(chinese_text):\n",
    "\n",
    "    prompt_template_llm = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Translate the following Chinese sentence into English:\",\n",
    "            ),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\"user\", \"Please provide an accurate translation.\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_llm = RunnableSequence(prompt_template_llm, llm)\n",
    "\n",
    "    return translation_chain_llm.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# RAG-based Translation\n",
    "def translate_with_rag(chinese_text, vector_store):\n",
    "\n",
    "    retrieved_docs = retrieve_relevant_docs(chinese_text, vector_store)\n",
    "\n",
    "    # Add retrieved documents as context\n",
    "\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "\n",
    "    # Construct prompt template (Using RAG)\n",
    "\n",
    "    prompt_template_rag = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a translation expert. Below is the Chinese text that needs to be translated into English. Additionally, the following context has been provided from relevant documents that might help you in producing a more accurate and context-aware translation.\",\n",
    "            ),\n",
    "            (\"system\", f\"Context (Relevant Documents):\\n{context}\"),\n",
    "            (\"user\", f'Chinese sentence: \"{chinese_text}\"'),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Please provide a translation that is both accurate and reflects the context from the documents provided.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    translation_chain_rag = RunnableSequence(prompt_template_rag, llm)\n",
    "\n",
    "    # Request translation using RAG\n",
    "\n",
    "    return translation_chain_rag.invoke({\"chinese_text\": chinese_text})\n",
    "\n",
    "\n",
    "# Load Chinese text from a file and split it into sentences, returning them as a list.\n",
    "def chinese_text_from_file_loader(path):\n",
    "\n",
    "    # Load data\n",
    "    loader = TextLoader(path, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "\n",
    "    return split_chinese_sentences_from_docs(docs)\n",
    "\n",
    "\n",
    "# Split sentences from a list of documents and return them as a list\n",
    "def split_chinese_sentences_from_docs(docs):\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        sentences.extend(split_chinese_sentences(text))\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Use regular expressions to split sentences and punctuation together.\n",
    "# Then, combine the sentences and punctuation back and return them\n",
    "def split_chinese_sentences(text):\n",
    "\n",
    "    # Separate sentences and punctuation,\n",
    "    sentence_list = re.split(r\"([ã€‚ï¼ï¼Ÿ])\", text)\n",
    "\n",
    "    # Combine the sentences and punctuation back to restore them.\n",
    "    merged_sentences = [\n",
    "        \"\".join(x) for x in zip(sentence_list[0::2], sentence_list[1::2])\n",
    "    ]\n",
    "\n",
    "    # Remove empty sentences and return the result.\n",
    "    return [sentence.strip() for sentence in merged_sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "def count_chinese_sentences(docs):\n",
    "    if isinstance(docs, str):\n",
    "        sentences = split_chinese_sentences(docs)\n",
    "\n",
    "    print(f\"Total number of sentences: {len(sentences)}\")\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def split_english_sentences_from_docs(docs):\n",
    "\n",
    "    sentences = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        sentences.extend(split_english_sentences(text))\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# Use NLTK's sent_tokenize() to split sentences accurately.\n",
    "# By default, it recognizes periods (.), question marks (?), and exclamation marks (!) to separate sentences.\n",
    "def split_english_sentences(text):\n",
    "\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "\n",
    "def count_paragraphs_and_sentences(docs):\n",
    "\n",
    "    if isinstance(docs, str):\n",
    "\n",
    "        paragraphs = paragraphs = re.split(r\"\\n\\s*\\n\", docs.strip())\n",
    "        paragraphs = [para.strip() for para in paragraphs if para.strip()]\n",
    "        sentences = [sent for para in paragraphs for sent in sent_tokenize(para)]\n",
    "\n",
    "        print(f\"Total number of paragraphs : {len(paragraphs)}\")\n",
    "        print(f\"Total number of sentences  : {len(sentences)}\")\n",
    "    return len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the written functions to perform the comparison.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input chinese text\n",
      "Total number of sentences: 15\n",
      "æ•°æ®é¢†åŸŸè¿æ¥å›½å®¶æ ‡å‡†ã€‚10æœˆ8æ—¥ï¼Œå›½å®¶å‘æ”¹å§”ç­‰éƒ¨é—¨å‘å¸ƒå…³äºå°å‘ã€Šå›½å®¶æ•°æ®æ ‡å‡†ä½“ç³»å»ºè®¾æŒ‡å—ã€‹(ä»¥ä¸‹ç®€ç§°ã€ŠæŒ‡å—ã€‹)çš„é€šçŸ¥ã€‚ä¸ºâ€œå……åˆ†å‘æŒ¥æ ‡å‡†åœ¨æ¿€æ´»æ•°æ®è¦ç´ æ½œèƒ½ã€åšå¼ºåšä¼˜åšå¤§æ•°å­—ç»æµç­‰æ–¹é¢çš„è§„èŒƒå’Œå¼•é¢†ä½œç”¨â€ï¼Œå›½å®¶å‘å±•æ”¹é©å§”ã€å›½å®¶æ•°æ®å±€ã€ä¸­å¤®ç½‘ä¿¡åŠã€å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨ã€è´¢æ”¿éƒ¨ã€å›½å®¶æ ‡å‡†å§”ç»„ç»‡ç¼–åˆ¶äº†ã€Šå›½å®¶æ•°æ®æ ‡å‡†ä½“ç³»å»ºè®¾æŒ‡å—ã€‹ã€‚ã€ŠæŒ‡å—ã€‹æå‡ºï¼Œåˆ°2026å¹´åº•ï¼ŒåŸºæœ¬å»ºæˆå›½å®¶æ•°æ®æ ‡å‡†ä½“ç³»ï¼Œå›´ç»•æ•°æ®æµé€šåˆ©ç”¨åŸºç¡€è®¾æ–½ã€æ•°æ®ç®¡ç†ã€æ•°æ®æœåŠ¡ã€è®­ç»ƒæ•°æ®é›†ã€å…¬å…±æ•°æ®æˆæƒè¿è¥ã€æ•°æ®ç¡®æƒã€æ•°æ®èµ„æºå®šä»·ã€ä¼ä¸šæ•°æ®èŒƒå¼äº¤æ˜“ç­‰æ–¹é¢åˆ¶ä¿®è®¢30é¡¹ä»¥ä¸Šæ•°æ®é¢†åŸŸåŸºç¡€é€šç”¨å›½å®¶æ ‡å‡†ï¼Œå½¢æˆä¸€æ‰¹æ ‡å‡†åº”ç”¨ç¤ºèŒƒæ¡ˆä¾‹ï¼Œå»ºæˆæ ‡å‡†éªŒè¯å’Œåº”ç”¨æœåŠ¡å¹³å°ï¼ŒåŸ¹è‚²ä¸€æ‰¹å…·å¤‡æ•°æ®ç®¡ç†èƒ½åŠ›è¯„ä¼°ã€æ•°æ®è¯„ä»·ã€æ•°æ®æœåŠ¡èƒ½åŠ›è¯„ä¼°ã€å…¬å…±æ•°æ®æˆæƒè¿è¥ç»©æ•ˆè¯„ä¼°ç­‰èƒ½åŠ›çš„ç¬¬ä¸‰æ–¹æ ‡å‡†åŒ–æœåŠ¡æœºæ„ã€‚ã€ŠæŒ‡å—ã€‹æ˜ç¡®ï¼Œæ•°æ®æ ‡å‡†ä½“ç³»æ¡†æ¶åŒ…å«åŸºç¡€é€šç”¨ã€æ•°æ®åŸºç¡€è®¾æ–½ã€æ•°æ®èµ„æºã€æ•°æ®æŠ€æœ¯ã€æ•°æ®æµé€šã€èåˆåº”ç”¨ã€å®‰å…¨ä¿éšœç­‰7ä¸ªéƒ¨åˆ†ã€‚æ•°æ®åŸºç¡€è®¾æ–½æ–¹é¢ï¼Œæ ‡å‡†æ¶‰åŠå­˜ç®—è®¾æ–½ä¸­çš„æ•°æ®ç®—åŠ›è®¾æ–½ã€æ•°æ®å­˜å‚¨è®¾æ–½ï¼Œç½‘ç»œè®¾æ–½ä¸­çš„5Gç½‘ç»œæ•°æ®ä¼ è¾“ã€å…‰çº¤æ•°æ®ä¼ è¾“ã€å«æ˜Ÿäº’è”ç½‘æ•°æ®ä¼ è¾“ï¼Œæ­¤å¤–è¿˜æœ‰æµé€šåˆ©ç”¨è®¾æ–½ã€‚æ•°æ®æµé€šæ–¹é¢ï¼Œæ ‡å‡†åŒ…æ‹¬æ•°æ®äº§å“ã€æ•°æ®ç¡®æƒã€æ•°æ®èµ„æºå®šä»·ã€æ•°æ®æµé€šäº¤æ˜“ã€‚èåˆåº”ç”¨æ–¹é¢ï¼Œæ ‡å‡†æ¶‰åŠå·¥ä¸šåˆ¶é€ ã€å†œä¸šå†œæ‘ã€å•†è´¸æµé€šã€äº¤é€šè¿è¾“ã€é‡‘èæœåŠ¡ã€ç§‘æŠ€åˆ›æ–°ã€æ–‡åŒ–æ—…æ¸¸(æ–‡ç‰©)ã€å«ç”Ÿå¥åº·ã€åº”æ€¥ç®¡ç†ã€æ°”è±¡æœåŠ¡ã€åŸå¸‚æ²»ç†ã€ç»¿è‰²ä½ç¢³ã€‚å®‰å…¨ä¿éšœæ–¹é¢ï¼Œæ ‡å‡†æ¶‰åŠæ•°æ®åŸºç¡€è®¾æ–½å®‰å…¨ï¼Œæ•°æ®è¦ç´ å¸‚åœºå®‰å…¨ï¼Œæ•°æ®æµé€šå®‰å…¨ã€‚æ•°æ®èµ„æºä¸­çš„æ•°æ®æ²»ç†æ ‡å‡†åŒ…æ‹¬æ•°æ®ä¸šåŠ¡è§„åˆ’ã€æ•°æ®è´¨é‡ç®¡ç†ã€æ•°æ®è°ƒæŸ¥ç›˜ç‚¹ã€æ•°æ®èµ„æºç™»è®°ï¼›è®­ç»ƒæ•°æ®é›†æ–¹é¢çš„æ ‡å‡†åŒ…æ‹¬è®­ç»ƒæ•°æ®é›†é‡‡é›†å¤„ç†ã€è®­ç»ƒæ•°æ®é›†æ ‡æ³¨ã€è®­ç»ƒæ•°æ®é›†åˆæˆã€‚åœ¨ç»„ç»‡ä¿éšœæ–¹é¢ï¼Œå°†æŒ‡å¯¼å»ºç«‹å…¨å›½æ•°æ®æ ‡å‡†åŒ–æŠ€æœ¯ç»„ç»‡ï¼ŒåŠ å¿«æ¨è¿›æ€¥ç”¨ã€æ€¥éœ€æ•°æ®æ ‡å‡†åˆ¶ä¿®è®¢å·¥ä½œï¼Œå¼ºåŒ–ä¸æœ‰å…³æ ‡å‡†åŒ–æŠ€æœ¯ç»„ç»‡ã€è¡Œä¸šã€åœ°æ–¹åŠç›¸å…³ç¤¾å›¢ç»„ç»‡ä¹‹é—´çš„æ²Ÿé€šåä½œã€åè°ƒè”åŠ¨ï¼Œä»¥æ ‡å‡†åŒ–ä¿ƒè¿›æ•°æ®äº§ä¸šç”Ÿæ€å»ºè®¾ã€‚åŒæ—¶è¿˜å°†å®Œå–„æ ‡å‡†è¯•ç‚¹æ”¿ç­–é…å¥—ï¼Œæ­å»ºæ•°æ®æ ‡å‡†åŒ–å…¬å…±æœåŠ¡å¹³å°ï¼Œå¼€å±•æ ‡å‡†å®£è´¯ï¼Œé€‰æ‹©é‡ç‚¹åœ°æ–¹ã€è¡Œä¸šå…ˆè¡Œå…ˆè¯•ï¼Œæ‰“é€ å…¸å‹ç¤ºèŒƒã€‚æ¢ç´¢æ¨åŠ¨æ•°æ®äº§å“ç¬¬ä¸‰æ–¹æ£€éªŒæ£€æµ‹ï¼Œæ·±åŒ–æ•°æ®æ ‡å‡†å®æ–½è¯„ä»·ç®¡ç†ã€‚åœ¨äººæ‰åŸ¹å…»æ–¹é¢ï¼Œå°†æ‰“é€ æ ‡å‡†é…å¥—çš„æ•°æ®äººæ‰åŸ¹è®­è¯¾ç¨‹ï¼Œå½¢æˆä¸€æ‰¹æ•°æ®æ ‡å‡†åŒ–ä¸“ä¸šäººæ‰ã€‚ä¼˜åŒ–æ•°æ®å›½é™…æ ‡å‡†åŒ–ä¸“å®¶é˜Ÿä¼ï¼Œæ”¯æŒå‚ä¸å›½é™…æ ‡å‡†åŒ–æ´»åŠ¨ï¼Œå¼ºåŒ–å›½é™…äº¤æµã€‚\n",
      "\n",
      "Translation using LLM\n",
      "Total number of paragraphs : 1\n",
      "Total number of sentences  : 16\n",
      "The data sector welcomes national standards. On October 8, the National Development and Reform Commission (NDRC) and other departments issued a notice regarding the publication of the \"Guidelines for the Construction of National Data Standard System\" (hereinafter referred to as the \"Guidelines\"). To \"fully leverage the role of standards in activating the potential of data elements, strengthening, optimizing, and enlarging the digital economy,\" the NDRC, the National Data Bureau, the Central Cyberspace Affairs Commission, the Ministry of Industry and Information Technology, the Ministry of Finance, and the National Standardization Administration organized the preparation of the \"Guidelines.\" The \"Guidelines\" propose that by the end of 2026, a national data standard system will be basically established, with more than 30 foundational general national standards in the field of data being developed or revised, focusing on aspects such as data circulation and utilization infrastructure, data management, data services, training data sets, public data authorization and operation, data ownership, data resource pricing, and enterprise data paradigm transactions. A batch of standard application demonstration cases will be formed, a standard verification and application service platform will be built, and a number of third-party standardization service organizations with capabilities in data management capability assessment, data evaluation, data service capability assessment, and public data authorization operation performance assessment will be cultivated. The \"Guidelines\" clarify that the framework of the data standard system includes seven parts: foundational general standards, data infrastructure, data resources, data technology, data circulation, integrated applications, and security assurance. In terms of data infrastructure, the standards involve data computing facilities and data storage facilities within computing and storage infrastructure, 5G network data transmission, optical fiber data transmission, and satellite internet data transmission within network infrastructure, as well as circulation and utilization facilities. Regarding data circulation, the standards include data products, data ownership, data resource pricing, and data circulation transactions. In terms of integrated applications, the standards cover industrial manufacturing, agriculture and rural areas, commercial circulation, transportation, financial services, technological innovation, cultural tourism (cultural relics), health and wellness, emergency management, meteorological services, urban governance, and green low-carbon initiatives. For security assurance, the standards address data infrastructure security, data element market security, and data circulation security. Data governance standards within data resources include data business planning, data quality management, data inventory assessment, and data resource registration; standards related to training data sets include collection and processing of training data sets, labeling of training data sets, and synthesis of training data sets. In terms of organizational support, guidance will be provided to establish national data standardization technical organizations, accelerate the revision and development of urgently needed data standards, and strengthen communication and coordination with relevant standardization technical organizations, industries, localities, and related associations, promoting data industry ecosystem construction through standardization. Additionally, supporting policies for standard pilot projects will be improved, a public service platform for data standardization will be established, standard promotion will be conducted, and key locations and industries will be selected for pilot testing to create typical demonstrations. Efforts will be made to explore promoting third-party inspection and testing of data products and to deepen the evaluation and management of data standard implementation. In terms of talent cultivation, training courses for data talents that complement standards will be developed, resulting in a batch of professionals in data standardization. The team of experts in international data standardization will be optimized, support for participation in international standardization activities will be provided, and international exchanges will be strengthened.\n",
      "\n",
      "Translation using RAG\n",
      "Total number of paragraphs : 3\n",
      "Total number of sentences  : 16\n",
      "The data sector is ushering in national standards. On October 8, the National Development and Reform Commission and other departments released a notice on the issuance of the \"Guidelines for the Construction of the National Data Standard System\" (hereinafter referred to as the \"Guidelines\"). To \"fully leverage the regulatory and guiding role of standards in activating the potential of data factors and strengthening, optimizing, and expanding the digital economy,\" the National Development and Reform Commission, the National Data Bureau, the Cyberspace Administration of China, the Ministry of Industry and Information Technology, the Ministry of Finance, and the National Standardization Administration organized the compilation of the \"Guidelines.\" The \"Guidelines\" propose that by the end of 2026, a basic national data standard system should be established, with over 30 foundational general national standards being formulated and revised in areas such as data circulation and utilization infrastructure, data management, data services, training data sets, public data authorized operations, data rights confirmation, data resource pricing, and enterprise data paradigm transactions. A number of standard application demonstration cases should be formed, a standard verification and application service platform established, and a batch of third-party standardization service institutions with capabilities in data management capability assessment, data evaluation, data service capability assessment, and public data authorized operation performance evaluation should be nurtured.\n",
      "\n",
      "The \"Guidelines\" clearly outline that the framework of the data standard system includes seven components: foundational general standards, data infrastructure, data resources, data technology, data circulation, integrated applications, and security assurance. In terms of data infrastructure, the standards cover data computing facilities and data storage facilities within computing and storage infrastructure, as well as data transmission over 5G networks, fiber-optic networks, and satellite internet in network infrastructure, along with circulation and utilization facilities. Regarding data circulation, the standards include data products, data rights confirmation, data resource pricing, and data circulation transactions. For integrated applications, the standards address areas such as industrial manufacturing, agriculture and rural development, trade circulation, transportation, financial services, technological innovation, cultural tourism (cultural relics), health care, emergency management, meteorological services, urban governance, and green low-carbon initiatives. In the area of security assurance, the standards relate to data infrastructure security, data factor market security, and data circulation security. The data governance standards within data resources include data business planning, data quality management, data survey and inventory, and data resource registration; standards concerning training data sets cover training data set collection and processing, training data set labeling, and training data set synthesis.\n",
      "\n",
      "In terms of organizational support, there will be guidance for establishing national data standardization technical organizations, accelerating the revision and formulation of urgently needed data standards, and strengthening communication and collaboration with relevant standardization technical organizations, industries, localities, and related associations to promote data industry ecosystem construction through standardization. Additionally, policies for standard pilot projects will be improved, a public service platform for data standardization will be established, standard promotion activities will be conducted, and key localities and industries will be selected for pilot tests to create typical demonstrations. Efforts will be made to explore the promotion of third-party inspection and testing of data products and deepen the management and evaluation of data standard implementation. In terms of talent cultivation, training courses for data talents that align with standards will be developed to produce a group of professionals in data standardization. The international standardization expert team will be optimized, supporting participation in international standardization activities and strengthening international exchanges.\n"
     ]
    }
   ],
   "source": [
    "# sentences = chinese_text_from_file_loader(\"data/comparison_cn.txt\")\n",
    "sentences = chinese_text_from_file_loader(\"data/comparison_cn copy.txt\")\n",
    "\n",
    "chinese_text = \"\"\n",
    "\n",
    "\n",
    "for sentence in sentences:\n",
    "    chinese_text += sentence\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm_translation = translate_with_llm(chinese_text)\n",
    "\n",
    "\n",
    "# RAG\n",
    "rag_translation = translate_with_rag(chinese_text, vector_store)\n",
    "\n",
    "\n",
    "print(\"\\ninput chinese text\")\n",
    "count_chinese_sentences(chinese_text)\n",
    "print(chinese_text)\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using LLM\")\n",
    "count_paragraphs_and_sentences(llm_translation.content)\n",
    "print(llm_translation.content)\n",
    "\n",
    "\n",
    "print(\"\\nTranslation using RAG\")\n",
    "count_paragraphs_and_sentences(rag_translation.content)\n",
    "print(rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸ ë°ì´í„° ì…‹ ìƒì„± ë¬¸ì¥ì˜ ìˆ˜ê°€ ì¼ì¹˜í•´ì•¼í•¨í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make comparison_json\n",
    "def make_comparison_json(chinese_text, translation_1, translation_2):\n",
    "    # ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸° (ì›ë¬¸ê³¼ ë²ˆì—­ì„ ê° ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸°)\n",
    "    # chinese_sentences = chinese_text.split(\"ã€‚\")\n",
    "    chinese_sentences = split_chinese_sentences(chinese_text)\n",
    "    translation_1_sentences = translation_1.split(\".\")\n",
    "    translation_2_sentences = translation_2.split(\".\")\n",
    "\n",
    "    print(\"\\nchinese_sentences:\", len(chinese_sentences))\n",
    "    print(\"\\ntranslation_1_sentences:\", len(translation_1_sentences))\n",
    "    print(\"\\ntranslation_2_sentences:\", len(translation_2_sentences))\n",
    "\n",
    "    # ê° ë¬¸ì¥ì˜ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘í•˜ì—¬ ì €ì¥\n",
    "    data = []\n",
    "    for i in range(len(chinese_sentences)):\n",
    "        # ë¬¸ì¥ë³„ë¡œ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘\n",
    "        sentence_data = {\n",
    "            \"chinese_text\": chinese_sentences[i].strip(),\n",
    "            \"translation_1\": (\n",
    "                translation_1_sentences[i].strip()\n",
    "                if i < len(translation_1_sentences)\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"translation_2\": (\n",
    "                translation_2_sentences[i].strip()\n",
    "                if i < len(translation_2_sentences)\n",
    "                else \"\"\n",
    "            ),\n",
    "        }\n",
    "        data.append(sentence_data)\n",
    "\n",
    "    # JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    output_file = \"data/translation_comparison2.json\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Translation comparison data saved to {output_file}\")\n",
    "\n",
    "    make_comparison_json(chinese_text, llm_translation.content, rag_translation.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of translation results\n",
    "\n",
    "Evaluation of translation results using BLEU and TER scores.\n",
    "Considering the addition of COMET and GPT for further assessment.\n",
    "Aiming to improve accuracy and quality in translation evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸° (ì›ë¬¸ê³¼ ë²ˆì—­ì„ ê° ë¬¸ì¥ë³„ë¡œ ë‚˜ëˆ„ê¸°)\n",
    "chinese_sentences = chinese_text.split(\"ã€‚\")\n",
    "translation_1_sentences = translatllm_translation.content.split(\".\")\n",
    "translation_2_sentences = translation_2.split(\".\")\n",
    "\n",
    "# ê° ë¬¸ì¥ì˜ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘í•˜ì—¬ ì €ì¥\n",
    "data = []\n",
    "for i in range(len(chinese_sentences)):\n",
    "    # ë¬¸ì¥ë³„ë¡œ ì›ë¬¸ê³¼ ë²ˆì—­ì„ ë§¤í•‘\n",
    "    sentence_data = {\n",
    "        \"chinese_text\": chinese_sentences[i].strip(),\n",
    "        \"translation_1\": (\n",
    "            translation_1_sentences[i].strip()\n",
    "            if i < len(translation_1_sentences)\n",
    "            else \"\"\n",
    "        ),\n",
    "        \"translation_2\": (\n",
    "            translation_2_sentences[i].strip()\n",
    "            if i < len(translation_2_sentences)\n",
    "            else \"\"\n",
    "        ),\n",
    "    }\n",
    "    data.append(sentence_data)\n",
    "\n",
    "# JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "output_file = \"translation_comparison.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Translation comparison data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ **Translation Quality Evaluation (BLEU & TER Scores)**\n",
      "\n",
      "â•’â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â••\n",
      "â”‚    â”‚ Category      â”‚ Text                                           â”‚ BLEU   â”‚ TER   â”‚\n",
      "â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
      "â”‚  0 â”‚ Source Text   â”‚ è¿™ä¸ªäº§å“åœ¨å¸‚åœºä¸Šå¾ˆå—æ¬¢è¿ã€‚                     â”‚ -      â”‚ -     â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  1 â”‚ Translation 1 â”‚ This product is very popular in the market.    â”‚ 0.0    â”‚ 800.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  2 â”‚ Translation 2 â”‚ This product is well received in the market.   â”‚ 0.0    â”‚ 800.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  3 â”‚ Source Text   â”‚ äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œã€‚                         â”‚ -      â”‚ -     â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  4 â”‚ Translation 1 â”‚ Artificial intelligence is changing the world. â”‚ 0.0    â”‚ 600.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  5 â”‚ Translation 2 â”‚ AI is transforming the world.                  â”‚ 0.0    â”‚ 500.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  6 â”‚ Source Text   â”‚ å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘ä»¬å»å…¬å›­å§ã€‚                       â”‚ -      â”‚ -     â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  7 â”‚ Translation 1 â”‚ The weather is great, let's go to the park.    â”‚ 0.0    â”‚ 900.0 â”‚\n",
      "â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  8 â”‚ Translation 2 â”‚ It's nice outside, let's visit the park.       â”‚ 0.0    â”‚ 700.0 â”‚\n",
      "â•˜â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•â•§â•â•â•â•â•â•â•â•›\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import sacrebleu\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "#  BLEU\n",
    "def calculate_bleu(reference, candidate):\n",
    "    return round(sacrebleu.sentence_bleu(candidate, [reference]).score, 3)\n",
    "\n",
    "\n",
    "# TER\n",
    "def calculate_ter(reference, candidate):\n",
    "    ter_metric = sacrebleu.metrics.TER()\n",
    "    return round(ter_metric.corpus_score([candidate], [[reference]]).score, 3)\n",
    "\n",
    "\n",
    "json_file_path = \"data/translations_comparison.json\"\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "translations_data = load_json_data(json_file_path)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(translations_data)\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    source_text = row[\"source_text\"]\n",
    "    translation_1 = row[\"translation_1\"]\n",
    "    translation_2 = row[\"translation_2\"]\n",
    "\n",
    "    # translation_1 evaluation\n",
    "    bleu_1 = calculate_bleu(source_text, translation_1)\n",
    "    ter_1 = calculate_ter(source_text, translation_1)\n",
    "\n",
    "    # translation_2 evaluation\n",
    "    bleu_2 = calculate_bleu(source_text, translation_2)\n",
    "    ter_2 = calculate_ter(source_text, translation_2)\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Source Text\",\n",
    "            \"Text\": source_text,\n",
    "            \"BLEU\": \"-\",\n",
    "            \"TER\": \"-\",\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Translation 1\",\n",
    "            \"Text\": translation_1,\n",
    "            \"BLEU\": bleu_1,\n",
    "            \"TER\": ter_1,\n",
    "        }\n",
    "    )\n",
    "    results.append(\n",
    "        {\n",
    "            \"Category\": \"Translation 2\",\n",
    "            \"Text\": translation_2,\n",
    "            \"BLEU\": bleu_2,\n",
    "            \"TER\": ter_2,\n",
    "        }\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def display_results(dataframe):\n",
    "    print(\"\\nğŸ“Œ **Translation Quality Evaluation (BLEU & TER Scores)**\\n\")\n",
    "    print(tabulate(dataframe, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "\n",
    "display_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-9y5W8e20-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

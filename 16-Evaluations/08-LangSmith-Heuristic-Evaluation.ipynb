{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635d8ebb",
   "metadata": {},
   "source": [
    "# Heuristic Evaluation\n",
    "\n",
    "- Author: [Sunworl Kim](https://github.com/sunworl)\n",
    "- Design:\n",
    "- Peer Review:\n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "## Overview\n",
    "\n",
    "Heuristic evaluation is a quick and simple method of inference when insufficient time or information makes it impossible to make a perfectly reasonable judgment.\n",
    "\n",
    "This tutorial describes a heuristic evaluation for text generation and search augmentation generation (RAG) systems, and helps to understand through examples.\n",
    "\n",
    "The main components covered include:\n",
    "\n",
    "1. For work tokenization, tokenization is performed using NLTK's main token function.\n",
    "\n",
    "2. Perform a heuristic evaluation based on Rouge, BLEU, METOR, and SemScore.\n",
    "\n",
    "    - ROUGE : Used to evaluate the quality of automatic summaries and machine translations.\n",
    "    - BLEU : Mainly used for machine translation evaluation. Measures how similar the generated text is to the reference text.\n",
    "    - METEOR : An evaluation index developed to evaluate the quality of machine translation.\n",
    "    - SemScore : Compares model outputs with gold standard responses using Semantic Textual Similarity (STS).\n",
    "\n",
    "This guide is designed to help developers and researchers implement and understand these evaluation metrics for assessing the quality of text generation systems, particularly in the context of RAG applications.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Heuristic Evaluation Based on Rouge, BLEU, METOR, SemScore](#heuristic-evaluation-based-on-rouge-bleu-metor-semscore)\n",
    "- [Function Definition for RAG Performance Testing](#function-definition-for-rag-performance-testing)\n",
    "- [Word Tokenization Using NLTK](#word-tokenization-using-nltk)\n",
    "- [ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score](#rouge-recall-oriented-understudy-for-gisting-evaluation-score)\n",
    "- [BLEU (Bilingual Evaluation Understudy) Score](#bleu-bilingual-evaluation-understudy-score)\n",
    "- [METEOR (Metric for Evaluation of Translation with Explicit Ordering) Score](#meteormetric-for-evaluation-of-translation-with-explicit-ordering-score)\n",
    "- [SemScore](#semscore)\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langchain docs : Scoring Evaluator](https://python.langchain.com/v0.1/docs/guides/productionization/evaluation/string/scoring_eval_chain/)\n",
    "- [LangSmith docs : Evaluate a RAG application](https://docs.smith.langchain.com/evaluation/tutorials/rag)\n",
    "- [LangSmith docs : Evaluation concepts](https://docs.smith.langchain.com/evaluation/concepts)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7aba4",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "21943adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f25ec196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_openai\",\n",
    "        \"nltk\",\n",
    "        \"sentence_transformers\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f9065ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    { \n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Heuristic-Evaluation\"  \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a9ae0",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f99b5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API keys from .env file\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00c3f4",
   "metadata": {},
   "source": [
    "## Heuristic Evaluation Based on Rouge, BLEU, METOR, SemScore\n",
    "\n",
    "Heuristic evaluation is a reasoning method that can be used quickly and easily when perfect rational judgment is not possible due to insufficient time or information.\n",
    "\n",
    "(This also has the advantage of saving time and costs when using LLM as Judge.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fc536",
   "metadata": {},
   "source": [
    "## Function Definition for RAG Performance Testing\n",
    "\n",
    "Let's create a RAG system for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b78d33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The authors are Julia Wiesinger, Patrick Marlow, and Vladimir Vuskovic.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from myrag import PDFRAG\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create PDFRAG object\n",
    "\n",
    "rag = PDFRAG(\n",
    "    \"data/Newwhitepaper_Agents2.pdf\",\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# Create chain\n",
    "chain = rag.create_chain(retriever)\n",
    "\n",
    "# Generate answer to question\n",
    "chain.invoke(\"List up the name of the authors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4d831",
   "metadata": {},
   "source": [
    "Create a function named `ask_question`. It takes a dictionary named `inputs` as input and returns a dictionary named `answer`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0874c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to answer questions\n",
    "def ask_question(inputs: dict):\n",
    "    return {\"answer\": chain.invoke(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c108df",
   "metadata": {},
   "source": [
    "## Word Tokenization Using NLTK \n",
    "\n",
    "Word tokenization is the process of splitting text into individual words or tokens. NLTK (Natural Language Toolkit) provides a robust word tokenization functionality through its word_tokenize function.\n",
    "Main functions of morphological analyzer:\n",
    "\n",
    "- `word_tokenize` : NLTK's main tokenization function\n",
    "- `nltk.download('punkt')` : Downloads required tokenization models\n",
    "- `split()` : Python's basic string splitting\n",
    "- `word_tokenize()` : NLTK's advanced tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2d22b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello.', 'Nice', 'meet', 'you!', 'My', 'name', 'is', 'Chloe~~.']\n",
      "['Hello.', 'My', 'name', 'is', 'Chloe~^^.', 'Nice', 'meet', 'you!!']\n",
      "============================================================\n",
      "['Hello', '.', 'Nice', 'meet', 'you', '!', 'My', 'name', 'is', 'Chloe~~', '.']\n",
      "['Hello', '.', 'My', 'name', 'is', 'Chloe~^^', '.', 'Nice', 'meet', 'you', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "sent1 = \"Hello. Nice meet you! My name is Chloe~~.\"\n",
    "sent2 = \"Hello. My name is Chloe~^^. Nice meet you!!\"\n",
    "\n",
    "# Basic string split\n",
    "print(sent1.split())\n",
    "print(sent2.split())\n",
    "\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# NLTK word tokenization\n",
    "print(word_tokenize(sent1))\n",
    "print(word_tokenize(sent2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0901c",
   "metadata": {},
   "source": [
    "## ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score\n",
    "\n",
    "- It is an evaluation metric used to assess the quality of automatic summarization and machine translation.\n",
    "- Measures how many important keywords from the reference text are included in the generated text.\n",
    "- Calculated based on n-gram overlap.\n",
    "\n",
    "\n",
    "\n",
    "  > Note: What is N-gram?\n",
    "\n",
    "  ![image.png](assets/08-langsmith-heuristic-evaluation-01.png)\n",
    "\n",
    "\n",
    "\n",
    "**Rouge-1**\n",
    "- Measures the similarity at the word level.\n",
    "- Evaluates the individual word matches between two sentences.\n",
    "\n",
    "**Rouge-2**\n",
    "- Measures the similarity based on overlapping consecutive word pairs (bigrams).\n",
    "- Evaluates the matches of consecutive two-word pairs between two sentences.\n",
    "  \n",
    "**Rouge-L**\n",
    "- Measures the similarity based on the Longest Common Subsequence (LCS).\n",
    "- Evaluates the word order at the sentence level without requiring continuous matches.\n",
    "- More flexible and ROUGE-N as it can capture longer distance word relationships.\n",
    "- Naturally reflects sentence structure similarity by finding the longest sequence that preserves word order but allows gaps.\n",
    "\n",
    "\n",
    "**Prectical Example**\n",
    "\n",
    "Example sentences\n",
    "- Original Sentence : \"I met a cute dog while jogging in the park this morning.\"\n",
    "- Generated Sentence : \"I saw a little cute cat while taking a walk in the park this morning.\"\n",
    "\n",
    "1. ROUGE-1 Analysis\n",
    "   - Each word is compared individually.\n",
    "   - Matching words : \"I\", \"a\", \"cute\", \"while\", \"in\", \"the\", \"park\", \"this\", \"morning\"\n",
    "   - These words appear in both sentences, so they are reflected in the score.\n",
    "\n",
    "\n",
    "2. ROUGE-2 Analysis\n",
    "   - Compares sequences of consecutive word pairs.\n",
    "   - Matching phrases : \"in the\", \"the park\", \"park this\", \"this morning\"\n",
    "   - These two-word combinations appear in both sentences, so they are reflected in the score.\n",
    "\n",
    "\n",
    "3. ROUGE-L Analysis\n",
    "   - Finds the longest common subsequence while maintaining word order.\n",
    "   - Longest common subsequence : \"I a cute while in the park this morning\"\n",
    "   - These words appear in the same order in both sentences, so this sequence is reflected in the ROUGE-L score.\n",
    "\n",
    "\n",
    "This example demonstrates how each ROUGE metric captures different aspects of similarity:\n",
    "\n",
    "- **ROUGE-1** captures basic content overlap through individual word matches.\n",
    "- **ROUGE-2** identifies common phrases and local word order.\n",
    "- **ROUGE-L** evaluates overall sentence structure while allowing for gaps between matched words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a974ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little cute cat while taking a walk in the park this morning.\n",
      "[rouge1] 0.68966\n",
      "[rouge2] 0.37037\n",
      "[rougeL] 0.68966\n",
      "============================================================\n",
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little and cute dog on the park bench this morning.\n",
      "[rouge1] 0.66667\n",
      "[rouge2] 0.37037\n",
      "[rougeL] 0.66667\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "sent1 = \"I met a cute dog while jogging in the park this morning.\"\n",
    "sent2 = \"I saw a little cute cat while taking a walk in the park this morning.\"\n",
    "sent3 = \"I saw a little and cute dog on the park bench this morning.\"\n",
    "\n",
    "\n",
    "# Define custom tokenizer class\n",
    "class NLTKTokenizer:\n",
    "    def tokenize(self, text):\n",
    "        return word_tokenize(text.lower())\n",
    "\n",
    "\n",
    "# Initialize RougeScorer with the NLTK tokenizer class\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    [\"rouge1\", \"rouge2\", \"rougeL\"], \n",
    "    use_stemmer=False, \n",
    "    tokenizer=NLTKTokenizer()\n",
    ")\n",
    "\n",
    "# Compare first pair of sentences\n",
    "print(\n",
    "    f\"[1] {sent1}\\n[2] {sent2}\\n[rouge1] {scorer.score(sent1, sent2)['rouge1'].fmeasure:.5f}\\n[rouge2] {scorer.score(sent1, sent2)['rouge2'].fmeasure:.5f}\\n[rougeL] {scorer.score(sent1, sent2)['rougeL'].fmeasure:.5f}\"\n",
    ")\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# Compare second pair of sentences\n",
    "print(\n",
    "    f\"[1] {sent1}\\n[2] {sent3}\\n[rouge1] {scorer.score(sent1, sent3)['rouge1'].fmeasure:.5f}\\n[rouge2] {scorer.score(sent1, sent2)['rouge2'].fmeasure:.5f}\\n[rougeL] {scorer.score(sent1, sent3)['rougeL'].fmeasure:.5f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e76a09",
   "metadata": {},
   "source": [
    "## BLEU (Bilingual Evaluation Understudy) Score\n",
    "\n",
    "BLEU is a metric used to evaluate text generation quality, particularly in machine translation. It measures the similarity between generated text and reference text by comparing the overlap of word sequences (n-grams).\n",
    "\n",
    "### Key Features of BLEU\n",
    "\n",
    "1. N-gram Precision\n",
    "    - BLEU calculates precision from 1-gram (individual words) to 4-gram (sequences of 4 words)\n",
    "    - The precision measures how many n-grams in the generated text match those in the reference text\n",
    "    - Higher n-gram matches indicate better phrase-level similarity\n",
    "\n",
    "2. Brevity Penalty\n",
    "    - Imposes a penalty if the generated text is shorter than the reference text.\n",
    "    - This prevents the system from achieving high precision by only generating short sentences.\n",
    "\n",
    "3. Geometric Mean\n",
    "    - The final BLEU score is the geometric mean of the n-gram precisions multiplied by the brevity penalty.\n",
    "    - Results in a score between 0 and 1\n",
    "\n",
    "### Example Anaysis\n",
    "\n",
    "- Original Sentence : \"I met a cute dog while jogging in the park this morning.\"\n",
    "- Generated Sentence : \"I saw a little cute cat while taking a walk in the park this morning.\"\n",
    "\n",
    "\n",
    "1. 1-gram(Unigram) Analysis\n",
    "\n",
    "    - Matching words: \"I\", \"a\", \"cute\", \"in\", \"the\", \"park\", \"this\", \"morning\"\n",
    "    - Precision : 8 / 15 ≈ 0.5333\n",
    "\n",
    "2. 2-gram(Bigram) Analysis\n",
    "    - Matching pairs: \"in the\", \"the park\", \"this morning\"\n",
    "    - Precision : 3 / 14 ≈ 0.2143\n",
    "\n",
    "3. 3-gram(Trigram) Analysis\n",
    "    - Matching sequences: \"in the park\", \"the park this\", \"park this morning\"\n",
    "    - Precision : 3 / 13 ≈ 0.2308\n",
    "\n",
    "4. 4-gram Analysis\n",
    "    - Matching sequences: \"in the park this\", \"the park this morning\"\n",
    "    - Precision : 2 / 12 ≈ 0.1667\n",
    "\n",
    "4. Brevity Penalty\n",
    "    - Since the lengths of the two sentences are similar, there is no penalty (1.0).\n",
    "\n",
    "5. Final BLEU Score\n",
    "    - Geometric mean (0.5333, 0.2143, 0.2308, 0.1667) * 1.0\n",
    "    - The final BLEU score is 0.2531 or about 25.31%.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- Only checks for simple string matches without considering meaning.\n",
    "- Does not distinguish the importance of words.\n",
    "\n",
    "BLEU scores range from 0 to 1, with scores closer to 1 indicating higher quality. However, achieving a perfect score of 1 is very difficult in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c8e5a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 tokens: ['I', 'met', 'a', 'cute', 'dog', 'while', 'jogging', 'in', 'the', 'park', 'this', 'morning', '.']\n",
      "Sentence 2 tokens: ['I', 'saw', 'a', 'little', 'cute', 'cat', 'while', 'taking', 'a', 'walk', 'in', 'the', 'park', 'this', 'morning', '.']\n",
      "Sentence 3 tokens: ['I', 'saw', 'a', 'little', 'and', 'cute', 'dog', 'on', 'the', 'park', 'bench', 'this', 'morning', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "sent1 = \"I met a cute dog while jogging in the park this morning.\"\n",
    "sent2 = \"I saw a little cute cat while taking a walk in the park this morning.\"\n",
    "sent3 = \"I saw a little and cute dog on the park bench this morning.\"\n",
    "\n",
    "# tokenization\n",
    "reference = word_tokenize(sent1)\n",
    "candidate1 = word_tokenize(sent2)\n",
    "candidate2 = word_tokenize(sent3)\n",
    "\n",
    "print(\"Sentence 1 tokens:\", reference)\n",
    "print(\"Sentence 2 tokens:\", candidate1)\n",
    "print(\"Sentence 3 tokens:\", candidate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e2c5b430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little cute cat while taking a walk in the park this morning.\n",
      "[score] 0.34235\n",
      "============================================================\n",
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little and cute dog on the park bench this morning.\n",
      "[score] 0.11064\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "\n",
    "# Initialize smoothing function\n",
    "smoothie = SmoothingFunction().method1\n",
    "\n",
    "# Calculate and print BLEU score for first pair\n",
    "bleu_score = sentence_bleu(\n",
    "    [word_tokenize(sent1)],\n",
    "    word_tokenize(sent2),\n",
    "    smoothing_function=smoothie\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {bleu_score:.5f}\")\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# Calculate and print BLEU score for second pair\n",
    "bleu_score = sentence_bleu(\n",
    "    [word_tokenize(sent1)],\n",
    "    word_tokenize(sent3),\n",
    "    smoothing_function=smoothie\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent3}\\n[score] {bleu_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07e356",
   "metadata": {},
   "source": [
    "## METEOR(Metric for Evaluation of Translation with Explicit Ordering) Score\n",
    "\n",
    "A metric developed to evaluate the quality of machine translation and text generation.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. Word Matching\n",
    "   - Exact Matching : Identical words\n",
    "   - Stem Matching : Words with the same root (e.g., \"run\" and \"running\")\n",
    "   - Synonym Matching : Words with the same meaning (e.g., \"quick\" and \"fast\")\n",
    "   - Paraphrase Matching : Phrase-level synonyms (commonly used in machine translation)\n",
    "\n",
    "2. Precision and Recall Analysis\n",
    "   - Precision : Proportion of words in the generated text that match the reference text\n",
    "   - Recall : Proportion of words in the reference text that match the generated text\n",
    "   - F-mean : Harmonic mean of precision and recall\n",
    "\n",
    "3. Order Penalty\n",
    "   - Evaluates word order similarity between texts.\n",
    "   - Applies penalties for non-consecutive matches.\n",
    "   - Ensures fluency and natural word ordering.\n",
    "\n",
    "4. Weighted Evaluation\n",
    "   - Assigns different weights to match types (exact, stem, synonym, paraphrase).\n",
    "   - Allows customization based on evaluation needs.\n",
    "\n",
    "### METEOR Score Calculation Process\n",
    "\n",
    "1. Word Matching : Find all possible matches between texts.\n",
    "2. Precision and Recall : Calculate based on matched words.\n",
    "3. F-mean : Compute harmonic mean of precision and recall.\n",
    "4. Order Penalty : Assess word order differences.\n",
    "5. Final Score : F-mean × (1 - Order Penalty).\n",
    "\n",
    "### Example\n",
    "- Reference : \"The cat is on the mat\"\n",
    "- Generated : \"On the mat is a cat\"\n",
    "\n",
    "1. Word Matching : All content words match(\"the\", \"cat\", \"is\", \"on\", \"mat\")\n",
    "2. Precision & Recall = 1.0 (all words match)\n",
    "3. F-mean = 1.0\n",
    "4. Order Penalty : 0.1 (due to different word ordering)\n",
    "5. Final METEOR Score = 1 * (1 - 0.1) = 0.9\n",
    "\n",
    "### Advantages of METEOR\n",
    "\n",
    "1. Recognizes synonyms and word variations.\n",
    "2. Balances precision and recall.\n",
    "3. Considers word order importance.\n",
    "4. Effective with single reference text.\n",
    "5. Correlates well with human judgment.\n",
    "\n",
    "### METEOR vs BLEU vs ROUGE\n",
    "\n",
    "- METEOR allows for more flexible evaluation by considering semantic similarity of words.\n",
    "- It tends to match human judgment better than BLEU.\n",
    "- Unlike ROUGE, we explicitly consider word order.\n",
    "- METEOR can be more complicated and time consuming to calculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "004c1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Suppress NLTK download messages\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Import and ensure WordNet is loaded\n",
    "wn.ensure_loaded()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec89414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little cute cat while taking a walk in the park this morning.\n",
      "[score] 0.70489\n",
      "============================================================\n",
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little and cute dog on the park bench this morning.\n",
      "[score] 0.62812\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import warnings\n",
    "from nltk.translate import meteor_score\n",
    "\n",
    "# Suppress NLTK download messages\n",
    "with warnings.catch_warnings():\n",
    "   warnings.filterwarnings(\"ignore\")\n",
    "   nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "sent1 = \"I met a cute dog while jogging in the park this morning.\"\n",
    "sent2 = \"I saw a little cute cat while taking a walk in the park this morning.\"\n",
    "sent3 = \"I saw a little and cute dog on the park bench this morning.\"\n",
    "\n",
    "# Calculate METEOR score for first pair\n",
    "meteor = meteor_score.meteor_score(\n",
    "   [word_tokenize(sent1)],\n",
    "   word_tokenize(sent2),\n",
    ")\n",
    "\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {meteor:.5f}\")\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# Calculate METEOR score for second pair\n",
    "meteor = meteor_score.meteor_score(\n",
    "   [word_tokenize(sent1)],\n",
    "   word_tokenize(sent3),\n",
    ")\n",
    "print(f\"[1] {sent1}\\n[2] {sent3}\\n[score] {meteor:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc43b99",
   "metadata": {},
   "source": [
    "## SemScore\n",
    "\n",
    "- [SEMSCORE: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity](https://arxiv.org/pdf/2401.17072)\n",
    "\n",
    "This research introduces SemScore, an efficient evaluation metric that uses semantic textual similarity (STS) to compare model outputs with reference responses. \n",
    "\n",
    "After evaluating 12 major instruction-tuned LLMs using 8 common text generation metrics, SemScore demonstrated the highest correlation with human evaluation.\n",
    "\n",
    "\n",
    "### Key Features of SemScore\n",
    "\n",
    "1. Semantic Textual Similarity (STS)\n",
    "   - Measures the semantic similarity between the generated text and the reference text.\n",
    "   - Considers the overall meaning of the sentences beyond simple word matching.\n",
    "\n",
    "2. Utilization of Pre-trained Language Models\n",
    "   - Uses pre-trained language models such as BERT or RoBERTa to generate sentence embeddings.\n",
    "   - This allows for better capture of context and meaning.\n",
    "\n",
    "3. Multiple Reference Handling\n",
    "   - Can consider multiple reference answers.\n",
    "   - This is particularly useful for open-ended questions or creative tasks.\n",
    "\n",
    "4. Granular Evaluation\n",
    "   - Evaluates not only the entire response but also parts of the response (e.g., sentence-level).\n",
    "\n",
    "5. High Correlation with Human Evaluation\n",
    "   - SemScore shows a high correlation with human evaluators' judgments.\n",
    "\n",
    "\n",
    "### Calculation Process\n",
    "\n",
    "1. Text Embedding Generation\n",
    "   - Converts the generated text and reference text into vectors using pre-trained language models.\n",
    "\n",
    "2. Similarity Computation\n",
    "   - Calculates the cosine similarity between the embeddings of the generated text and the reference text.\n",
    "\n",
    "3. Selection of Maximum Similarity\n",
    "   - If there are multiple references, selects the highest similarity score.\n",
    "\n",
    "4. Normalization\n",
    "   - Normalizes the final score to a value between 0 and 1.\n",
    "\n",
    "### Advantages of SemScore\n",
    "\n",
    "1. Semantic Understanding\n",
    "   - Considers the meaning of sentences beyond surface-level word matches.\n",
    "\n",
    "2. Flexibility\n",
    "   - Allows for various forms of answers, making it suitable for creative tasks or open-ended questions.\n",
    "\n",
    "3. Context Consideration\n",
    "   - Uses pre-trained language models to better understand the context of words and sentences.\n",
    "\n",
    "4. Multilingual Support\n",
    "   - Can evaluate multiple languages using multilingual models.\n",
    "\n",
    "### SemScore vs Other Evaluation Metrics\n",
    "\n",
    "- Unlike BLEU and ROUGE, it does not rely solely on simple n-gram matching.\n",
    "- Measures more advanced semantic similarity compared to METEOR.\n",
    "- Similar to BERTScore but specialized for instruction-based tasks.\n",
    "\n",
    "Uses the `SentenceTransformer` model to generate sentence embeddings and calculates the cosine similarity between two sentences.\n",
    "- The model used in the paper is `all-mpnet-base-v2`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1221d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little cute cat while taking a walk in the park this morning.\n",
      "[score] 0.69124\n",
      "============================================================\n",
      "[1] I met a cute dog while jogging in the park this morning.\n",
      "[2] I saw a little and cute dog on the park bench this morning.\n",
      "[score] 0.76015\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress all warnings and logging messages\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)  # Suppress all warning types\n",
    "\n",
    "# Disable tqdm warnings\n",
    "import tqdm.autonotebook\n",
    "tqdm.autonotebook.tqdm = tqdm.std.tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.getLogger(\"sentence_transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tqdm\").setLevel(logging.ERROR)\n",
    "\n",
    "sent1 = \"I met a cute dog while jogging in the park this morning.\"\n",
    "sent2 = \"I saw a little cute cat while taking a walk in the park this morning.\"\n",
    "sent3 = \"I saw a little and cute dog on the park bench this morning.\"\n",
    "\n",
    "# Load SentenceTransformer model silently\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\", use_auth_token=False)\n",
    "\n",
    "# Encoding sentences\n",
    "sent1_encoded = model.encode(sent1, convert_to_tensor=True, show_progress_bar=False)\n",
    "sent2_encoded = model.encode(sent2, convert_to_tensor=True, show_progress_bar=False)\n",
    "sent3_encoded = model.encode(sent3, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "# Calculate and print cosine similarity for first pair\n",
    "cosine_similarity = util.pytorch_cos_sim(sent1_encoded, sent2_encoded).item()\n",
    "print(f\"[1] {sent1}\\n[2] {sent2}\\n[score] {cosine_similarity:.5f}\")\n",
    "\n",
    "print(\"===\" * 20)\n",
    "\n",
    "# Calculate and print cosine similarity for second pair\n",
    "cosine_similarity = util.pytorch_cos_sim(sent1_encoded, sent3_encoded).item()\n",
    "print(f\"[1] {sent1}\\n[2] {sent3}\\n[score] {cosine_similarity:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cac10",
   "metadata": {},
   "source": [
    "The evaluator summarized above is as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2fa5e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.schemas import Run, Example\n",
    "import os\n",
    "\n",
    "# Set tokenizer parallelization for HuggingFace models\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "class NLTKWrapper:\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return word_tokenize(text)\n",
    "   \n",
    "\n",
    "def rouge_evaluator(metric: str = \"rouge1\") -> dict:\n",
    "    # Define wrapper function\n",
    "    def _rouge_evaluator(run: Run, example: Example) -> dict:\n",
    "\n",
    "        # Get generated output and reference answer\n",
    "        student_answer = run.outputs.get(\"answer\", \"\")\n",
    "        reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "        # Calculate ROUGE score\n",
    "        scorer = rouge_scorer.RougeScorer(\n",
    "            [\"rouge1\", \"rouge2\", \"rougeL\"], \n",
    "            use_stemmer=True, \n",
    "            tokenizer=NLTKWrapper()\n",
    "        )\n",
    "        scores = scorer.score(reference_answer, student_answer)\n",
    "\n",
    "        # Return ROUGE score\n",
    "        rouge = scores[metric].fmeasure\n",
    "        return {\"key\": \"ROUGE\", \"score\": rouge}\n",
    "    return _rouge_evaluator\n",
    "\n",
    "\n",
    "def bleu_evaluator(run: Run, example: Example) -> dict:\n",
    "    # Get generated output and reference answer\n",
    "    student_answer = run.outputs.get(\"answer\", \"\")\n",
    "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # Tokenization\n",
    "    reference_tokens = word_tokenize(reference_answer)\n",
    "    student_tokens = word_tokenize(student_answer)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = sentence_bleu([reference_tokens], student_tokens)\n",
    "\n",
    "    return {\"key\": \"BLEU\", \"score\": bleu_score}\n",
    "\n",
    "\n",
    "def meteor_evaluator(run: Run, example: Example) -> dict:\n",
    "    # Get generated output and reference answer\n",
    "    student_answer = run.outputs.get(\"answer\", \"\")\n",
    "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # Tokenization\n",
    "    reference_tokens = word_tokenize(reference_answer)\n",
    "    student_tokens = word_tokenize(student_answer)\n",
    "\n",
    "    # Calculate METEOR score\n",
    "    meteor = meteor_score.meteor_score([reference_tokens], student_tokens)\n",
    "    return {\"key\": \"METEOR\", \"score\": meteor}\n",
    "\n",
    "\n",
    "def semscore_evaluator(run: Run, example: Example) -> dict:\n",
    "    # Get generated output and reference answer\n",
    "    student_answer = run.outputs.get(\"answer\", \"\")\n",
    "    reference_answer = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # Load SentenceTransformer model\n",
    "    model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "    # Generate sentence embeddings\n",
    "    student_embedding = model.encode(student_answer, convert_to_tensor=True)\n",
    "    reference_embedding = model.encode(reference_answer, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = util.pytorch_cos_sim(\n",
    "        student_embedding, reference_embedding\n",
    "    ).item()\n",
    "\n",
    "    return {\"key\": \"sem_score\", \"score\": cosine_similarity}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2753835",
   "metadata": {},
   "source": [
    "The evaluation is conducted using the Heuristic Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ef397b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Heuristic-EVAL-201c2ddf' at:\n",
      "https://smith.langchain.com/o/97d4ef95-7b86-4c82-9f4c-3f18e315c9b2/datasets/920886b5-0aa2-4f47-b23f-b3dfc33135ef/compare?selectedSessions=048bce87-ae35-4fd2-af16-6c738fc93762\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d6bdbf06ce4423bf01666333cb4a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.WARNING)\n",
    "\n",
    "# Define Evaluator\n",
    "heuristic_evalulators = [\n",
    "    rouge_evaluator(metric=\"rougeL\"),\n",
    "    bleu_evaluator,\n",
    "    meteor_evaluator,\n",
    "    semscore_evaluator,\n",
    "]\n",
    "\n",
    "# Set Dataset Name\n",
    "dataset_name = \"RAG_EVAL_DATASET\"\n",
    "\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    ask_question,\n",
    "    data=dataset_name,\n",
    "    evaluators=heuristic_evalulators,\n",
    "    experiment_prefix=\"Heuristic-EVAL\",\n",
    "    # Define Experimental Metadata\n",
    "    metadata={\n",
    "        \"variant\": \"Evaluater Using Heuristic-EVAL (Rouge, BLEU, METEOR, SemScore)\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7d078",
   "metadata": {},
   "source": [
    "Check the results.\n",
    "\n",
    "\n",
    "  ![image.png](assets/08-langsmith-heuristic-evaluation-02.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-Y7QSWb-L-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

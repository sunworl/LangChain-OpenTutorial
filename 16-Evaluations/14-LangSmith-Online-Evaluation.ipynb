{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith Online Evaluation\n",
    "\n",
    "- Author: [JeongGi Park](https://github.com/jeongkpa)\n",
    "- Design: []()\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/07-LCEL-Interface.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/07-LCEL-Interface.ipynb)\n",
    "\n",
    "## Overview\n",
    "This module provides tools to evaluate and track the performance of language models using **LangSmith's** online evaluation capabilities.\n",
    "\n",
    "\n",
    "By setting up chains and using custom configurations, users can assess model outputs, including **hallucination** detection and context recall, ensuring robust performance in various scenarios.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Build a Pipeline for Online Evaluations](#Build-a-Pipeline-for-Online-Evaluations)\n",
    "- [Set Up the RAG System with PDFRAG](#Set-Up-the-RAG-System-with-PDFRAG)\n",
    "- [Create a Parallel Evaluation Runnable](#Create-a-Parallel-Evaluation-Runnable)\n",
    "- [Make Online LLM-as-judge](#Make-Online-LLM-as-judge)\n",
    "- [Run Evaluations](#Run-Evaluations)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langsmith DOC](https://docs.smith.langchain.com/)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-openai\",\n",
    "        \"langchain_community\",\n",
    "        \"pymupdf\",\n",
    "        \"faiss-cpu\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"LangSmith-Online-Evaluation\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note] If you are using a `.env` file, proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Pipeline for Online Evaluations\n",
    "\n",
    "The provided Python script defines a class `PDFRAG` and related functionality to set up a retriever-augmented generation (RAG) pipeline for online evaluation of language models.\n",
    "\n",
    "### Explain for 'PDFRAG'\n",
    "\n",
    "The `PDFRAG` class is a modular framework for:\n",
    "\n",
    "1. Document Loading: Ingesting a PDF document.\n",
    "2. Document Splitting: Dividing the content into manageable chunks for processing.\n",
    "3. Vectorstore Creation: Converting chunks into vector representations using embeddings.\n",
    "4. Retriever Setup: Enabling retrieval of the most relevant chunks for a given query.\n",
    "5. Chain Construction: Creating a question-answering (QA) chain with prompt templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "class PDFRAG:\n",
    "    def __init__(self, file_path: str, llm):\n",
    "        self.file_path = file_path\n",
    "        self.llm = llm\n",
    "\n",
    "    def load_documents(self):\n",
    "        # Load Documents\n",
    "        loader = PyMuPDFLoader(self.file_path)\n",
    "        docs = loader.load()\n",
    "        return docs\n",
    "\n",
    "    def split_documents(self, docs):\n",
    "        # Split Documents\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "        split_documents = text_splitter.split_documents(docs)\n",
    "        return split_documents\n",
    "\n",
    "    def create_vectorstore(self, split_documents):\n",
    "        # Embedding\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "        # Create DB\n",
    "        vectorstore = FAISS.from_documents(\n",
    "            documents=split_documents, embedding=embeddings\n",
    "        )\n",
    "        return vectorstore\n",
    "\n",
    "    def create_retriever(self):\n",
    "        vectorstore = self.create_vectorstore(\n",
    "            self.split_documents(self.load_documents())\n",
    "        )\n",
    "        # Retriever\n",
    "        retriever = vectorstore.as_retriever()\n",
    "        return retriever\n",
    "\n",
    "    def create_chain(self, retriever):\n",
    "        # Create Prompt\n",
    "        prompt = PromptTemplate.from_template(\n",
    "            \"\"\"You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "\n",
    "        #Context: \n",
    "        {context}\n",
    "\n",
    "        #Question:\n",
    "        {question}\n",
    "\n",
    "        #Answer:\"\"\"\n",
    "        )\n",
    "\n",
    "        # Chain\n",
    "        chain = (\n",
    "            {\n",
    "                \"context\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up the RAG System with PDFRAG\n",
    "\n",
    "The following code demonstrates how to instantiate and use the `PDFRAG` class to set up a retriever-augmented generation (RAG) pipeline using a specific PDF document and a GPT-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create a PDFRAG object\n",
    "rag = PDFRAG(\n",
    "    \"data/Newwhitepaper_Agents2.pdf\",\n",
    "    ChatOpenAI(model=\"gpt-4o-mini\", temperature=0),\n",
    ")\n",
    "\n",
    "# Create a retriever\n",
    "retriever = rag.create_retriever()\n",
    "\n",
    "# Create a chain\n",
    "rag_chain = rag.create_chain(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Parallel Evaluation Runnable\n",
    "\n",
    "The following code demonstrates how to create a `RunnableParallel` object to evaluate multiple aspects of the retriever-augmented generation (RAG) pipeline concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Create a RunnableParallel object.\n",
    "evaluation_runnable = RunnableParallel(\n",
    "    {\n",
    "        \"context\": retriever,\n",
    "        \"answer\": rag_chain,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = evaluation_runnable.invoke(\"How do agents differ from standalone language models?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Online LLM-as-judge\n",
    "\n",
    "### 1. click Add Rule\n",
    "\n",
    "![make-online-LLM-as-judge](./assets/14-LangSmith-Online-Evaluation-01.png)\n",
    "\n",
    "### 2. Create Evaluator\n",
    "![create-evaluator](./assets/14-LangSmith-Online-Evaluation-02.png)\n",
    "\n",
    "\n",
    "### 3. Set Secrets & API Keys\n",
    "![set-secrets-API-keys](./assets/14-LangSmith-Online-Evaluation-03.png)\n",
    "![set-API-Keys](./assets/14-LangSmith-Online-Evaluation-04.png)\n",
    "\n",
    "\n",
    "### 4. Set Provider, Model, Prompt\n",
    "![set-provider-model-prompt](./assets/14-LangSmith-Online-Evaluation-05.png)\n",
    "\n",
    "\n",
    "### 5. Select Halluciantion\n",
    "![select-hallucination](./assets/14-LangSmith-Online-Evaluation-06.png)\n",
    "\n",
    "\n",
    "\n",
    "### 6. Set facts for output.context\n",
    "![set-facts](./assets/14-LangSmith-Online-Evaluation-07.png)\n",
    "\n",
    "### 7. Set answer for output.answer\n",
    "![set-answer](./assets/14-LangSmith-Online-Evaluation-08.png)\n",
    "\n",
    "\n",
    "### 8. Check Preview for Data\n",
    "![check-preview](./assets/14-LangSmith-Online-Evaluation-09.png)\n",
    "\n",
    "\n",
    "**Caution**\n",
    "\n",
    "You must view the preview and then turn off preview mode again before proceeding to the next step. And you have to fill \"Name\" to continue.\n",
    "\n",
    "![continue](./assets/14-LangSmith-Online-Evaluation-10.png)\n",
    "\n",
    "![fill-evaluator](./assets/14-LangSmith-Online-Evaluation-13.png)\n",
    "\n",
    "### 9. Save and Continue\n",
    "![save](./assets/14-LangSmith-Online-Evaluation-11.png)\\\n",
    "![check-for-save](./assets/14-LangSmith-Online-Evaluation-14.png)\n",
    "\n",
    "### 10. Make \"Tag\"\n",
    "\n",
    "![make-tag](./assets/14-LangSmith-Online-Evaluation-15.png)\n",
    "\n",
    "Instead of evaluating all steps, you can set \"Tag\" to evaluate only specific tags.\n",
    "\n",
    "\n",
    "### 11. Set \"Tag\" that you want\n",
    "![set-tag](./assets/14-LangSmith-Online-Evaluation-16.png)\n",
    "\n",
    "### 12. Run evaluations only for specific tags (hallucination)\n",
    "![run-eval](./assets/14-LangSmith-Online-Evaluation-17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "The following code demonstrates how to perform evaluations on the retriever-augmented generation (RAG) pipeline, including **hallucination** detection, context recall assessment, and combined evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# set a tag\n",
    "hallucination_config = RunnableConfig(tags=[\"hallucination_eval\"])\n",
    "context_recall_config = RunnableConfig(tags=[\"context_recall_eval\"])\n",
    "all_eval_config = RunnableConfig(tags=[\"hallucination_eval\", \"context_recall_eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run chain\n",
    "_ = evaluation_runnable.invoke(\"How do agents differ from standalone language models?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Hallucination evaluation\n",
    "_ = evaluation_runnable.invoke(\n",
    "    \"How do agents differ from standalone language models?\", config=hallucination_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request a Context Recall assessment\n",
    "_ = evaluation_runnable.invoke(\n",
    "    \"How do agents differ from standalone language models?\",\n",
    "    config=context_recall_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All evaluation requests\n",
    "_ = evaluation_runnable.invoke(\n",
    "    \"How do agents differ from standalone language models?\", config=all_eval_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-cyV7o7ra-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

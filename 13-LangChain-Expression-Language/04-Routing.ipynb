{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "- Author: [Jinu Cho](https://github.com/jinucho), [Lee Jungbin](https://github.com/leebeanbin)\n",
    "- Peer Review: [Teddy Lee](https://github.com/teddylee777), [김무상](https://github.com/musangk), [전창원](https://github.com/changwonjeon)\n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/05-Memory/06-ConversationSummaryMemory.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial introduces three key tools in LangChain: `RunnableSequence`, `RunnableBranch`, and `RunnableLambda`, essential for building efficient and powerful AI applications.\n",
    "\n",
    "`RunnableSequence` is a fundamental component that enables sequential processing pipelines, allowing structured and efficient handling of AI-related tasks. It provides automatic data flow management, error handling, and seamless integration with other LangChain components.\n",
    "\n",
    "`RunnableBranch` enables structured decision-making by routing input through predefined conditions, simplifying complex branching scenarios.\n",
    "\n",
    "`RunnableLambda` offers a flexible, function-based approach, ideal for lightweight transformations and inline processing.\n",
    "\n",
    "**Key Features of these components:**\n",
    "\n",
    "- **`RunnableSequence`:**\n",
    "  - Sequential processing pipeline creation\n",
    "  - Automatic data flow management\n",
    "  - Error handling and monitoring\n",
    "  - Support for async operations  \n",
    "\n",
    "- **`RunnableBranch`:**\n",
    "  - Dynamic routing based on conditions\n",
    "  - Structured decision trees\n",
    "  - Complex branching logic\n",
    "\n",
    "- **`RunnableLambda`:**\n",
    "  - Lightweight transformations\n",
    "  - Function-based processing\n",
    "  - Inline data manipulation\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [What is the RunnableSequence](#what-is-the-runnablesequence)\n",
    "- [What is the RunnableBranch](#what-is-the-runnablebranch)\n",
    "- [RunnableLambda](#runnablelambda)\n",
    "- [RunnableBranch](#runnablebranch)\n",
    "- [Comparison of RunnableBranch and RunnableLambda](#comparison-of-runnablesequence-runnablebranch-and-runnablelambda)\n",
    "\n",
    "### References\n",
    "- [RunnableSequence API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableSequence.html)\n",
    "- [LangChain Expression Language (LCEL)](https://python.langchain.com/docs/expression_language/interface)\n",
    "- [RunnableBranch API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html)  \n",
    "- [RunnableLambda API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "[Note]\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can check out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:59:52.208331Z",
     "start_time": "2025-01-15T14:59:50.390366Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:59:54.454657Z",
     "start_time": "2025-01-15T14:59:52.308364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_openai\",\n",
    "        \"pydantic\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set `OPENAI_API_KEY` in `.env` file and load it. \n",
    "\n",
    "[Note] This is not necessary if you've already set `OPENAI_API_KEY` in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-08T14:32:37.994065Z",
     "start_time": "2025-01-08T14:32:37.976718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"04-Routing\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T14:59:57.049102Z",
     "start_time": "2025-01-15T14:59:57.034060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "# Reload any variables that need to be overwritten from the previous cell\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the RunnableSequence\n",
    "\n",
    "`RunnableSequence` is a fundamental component in LangChain that enables the creation of sequential processing pipelines. It allows developers to chain multiple operations together where the output of one step becomes the input of the next step.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Sequential Processing**\n",
    "   - Ordered execution of operations\n",
    "   - Automatic data flow between steps\n",
    "   - Clear pipeline structure\n",
    "\n",
    "2. **Data Transformation**\n",
    "   - Input preprocessing\n",
    "   - State management\n",
    "   - Output formatting\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Pipeline-level error management\n",
    "   - Step-specific error recovery\n",
    "   - Fallback mechanisms\n",
    "\n",
    "Let's explore these concepts with practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "First, we will create a Chain that classifies incoming questions into one of three categories: math, science, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:00:00.704728Z",
     "start_time": "2025-01-15T14:59:59.545480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is a sample for processing purposes. It is likely being used as an example for a specific task or function. The content of the text is not specified beyond being a sample.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Basic Example: Text Processing Pipeline\n",
    "basic_chain = (\n",
    "    # Step 1: Input handling and prompt creation\n",
    "    PromptTemplate.from_template(\"Summarize this text in three sentences: {text}\")\n",
    "    # Step 2: LLM processing\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    # Step 3: Output parsing\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "result = basic_chain.invoke({\"text\": \"This is a sample text to process.\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Pipeline Creation\n",
    "\n",
    "In this section, we'll explore how to create fundamental pipelines using RunnableSequence. We'll start with a simple text generation pipeline and gradually build more complex functionality.\n",
    "\n",
    "**Understanding Basic Pipeline Structure**  \n",
    "- Sequential Processing: How data flows through the pipeline\n",
    "- Component Integration: Combining different LangChain components\n",
    "- Data Transformation: Managing input/output between steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:00:08.359680Z",
     "start_time": "2025-01-15T15:00:03.039049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Content: This text is a sample for processing purposes. It is likely being used as an example for a specific task or function. The content of the text is not specified beyond being a sample.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\"\"\"\n",
    "Basic Text Generation Pipeline\n",
    "This demonstrates the fundamental way to chain components in RunnableSequence.\n",
    "\n",
    "Flow:\n",
    "1. PromptTemplate -> Creates the prompt with specific instructions\n",
    "2. ChatOpenAI -> Processes the prompt and generates content\n",
    "3. StrOutputParser -> Cleans and formats the output\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Define the basic text generation chain\n",
    "basic_generation_chain = (\n",
    "    # Create prompt template for AI content generation\n",
    "        PromptTemplate.from_template(\n",
    "            \"\"\"Generate a detailed technical explanation about {topic} in AI/ML field.\n",
    "            Include:\n",
    "            - Core technical concepts\n",
    "            - Implementation details\n",
    "            - Real-world applications\n",
    "            - Technical challenges\n",
    "            \"\"\"\n",
    "        )\n",
    "        # Process with LLM\n",
    "        | ChatOpenAI(temperature=0.7)\n",
    "        # Convert output to clean string\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "basic_result = basic_generation_chain.invoke({\"topic\": \"Transformer architecture in LLMs\"})\n",
    "print(\"Generated Content:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Analysis Pipeline\n",
    "\n",
    "\n",
    "Building upon our basic pipeline, we'll now create a more sophisticated analysis system that processes and evaluates the generated content.\n",
    "\n",
    "**Key Features**\n",
    "- State Management: Maintaining context throughout the pipeline\n",
    "- Structured Analysis: Organizing output in a clear format\n",
    "- Error Handling: Basic error management implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:10.134043Z",
     "start_time": "2025-01-15T15:01:10.086294Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import time\n",
    "\n",
    "# Step 1: Define the analysis prompt template\n",
    "analysis_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Analyze this technical content and extract the most crucial insights:\n",
    "    \n",
    "    {generated_basic_content}\n",
    "    \n",
    "    Provide a concise analysis focusing only on the most important aspects:\n",
    "    (Importance : You should use Notion Syntax and try highliting with underlines, bold, emoji for title or something you describe context)\n",
    "    \n",
    "    Output format markdown outlet:\n",
    "    # Key Technical Analysis\n",
    "    \n",
    "    ## Core Concept Summary\n",
    "    [Extract and explain the 2-3 most fundamental concepts]\n",
    "    \n",
    "    ## Critical Implementation Insights\n",
    "    [Focus on crucial implementation details that make this technology work]\n",
    "    \n",
    "    ## Key Challenges & Solutions\n",
    "    [Identify the most significant challenges and their potential solutions]\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Step 2: Define the critical analysis chain\n",
    "analysis_chain = RunnableSequence(\n",
    "    first=analysis_prompt,\n",
    "    middle=[ChatOpenAI(temperature=0)],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# Step 3: Define the basic generation chain\n",
    "generation_prompt = RunnableLambda(lambda x: f\"\"\"Generate technical content about: {x['topic']}\"\"\")\n",
    "\n",
    "basic_generation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[generation_prompt],\n",
    "    last=ChatOpenAI(temperature=0.7)\n",
    ")\n",
    "\n",
    "# Step 4: Define the state initialization function\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"start_time\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "\n",
    "init_step = RunnableLambda(init_state)\n",
    "\n",
    "# Step 5: Define the content generation function\n",
    "def generated_basic_content(x):\n",
    "    content = basic_generation_chain.invoke({\"topic\": x[\"topic\"]})\n",
    "    return {\n",
    "        **x,\n",
    "        # \"generated_basic_content\": content.content\n",
    "        # To create a comprehensive wrap-up, you can combine the previous basic result with new annotated analysis.\n",
    "        \"generated_basic_content\": basic_result\n",
    "    }\n",
    "\n",
    "generate_step = RunnableLambda(generated_basic_content)\n",
    "\n",
    "# Step 6: Define the analysis function\n",
    "def perform_analysis(x):\n",
    "    analysis = analysis_chain.invoke({\"generated_basic_content\": x[\"generated_basic_content\"]})\n",
    "    return {\n",
    "        **x,\n",
    "        \"key_insights\": analysis\n",
    "    }\n",
    "\n",
    "analysis_step = RunnableLambda(perform_analysis)\n",
    "\n",
    "# Step 7: Define the output formatting function\n",
    "def format_output(x):\n",
    "    return {\n",
    "        \"timestamp\": x[\"start_time\"],\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"content\": x[\"generated_basic_content\"],\n",
    "        \"analysis\": x[\"key_insights\"],\n",
    "        \"formatted_output\": f\"\"\"\n",
    "# Technical Analysis Summary\n",
    "Generated: {x['start_time']}\n",
    "\n",
    "## Original Technical Content\n",
    "{x['generated_basic_content']}\n",
    "\n",
    "---\n",
    "\n",
    "{x['key_insights']}\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "format_step = RunnableLambda(format_output)\n",
    "\n",
    "# Step 8: Create the complete analysis pipeline\n",
    "analysis_pipeline = RunnableSequence(\n",
    "    first=init_step,\n",
    "    middle=[\n",
    "        generate_step,\n",
    "        analysis_step\n",
    "    ],\n",
    "    last=format_step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"left\">\n",
    " <img src = \"./assets/04-routing-runnable-pipeline.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:24.187435Z",
     "start_time": "2025-01-15T15:01:15.875860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Timestamp: 2025-01-16 00:01:15\n",
      "\n",
      "Topic: Transformer attention mechanisms\n",
      "\n",
      "Formatted Output: \n",
      "# Technical Analysis Summary\n",
      "Generated: 2025-01-16 00:01:15\n",
      "\n",
      "## Original Technical Content\n",
      "Transformer architecture in Language Model (LLM) is a type of neural network architecture that has gained popularity in the field of artificial intelligence and machine learning for its ability to handle sequential data efficiently. The core technical concept behind the Transformer architecture is the use of self-attention mechanisms to capture long-range dependencies in the input data.\n",
      "\n",
      "In a Transformer network, the input sequence is divided into tokens, which are then passed through multiple layers of self-attention and feedforward neural networks. The self-attention mechanism allows each token to attend to all other tokens in the input sequence, capturing the contextual information necessary for understanding the relationship between different parts of the input data. This enables the model to learn complex patterns in the data and generate more accurate predictions.\n",
      "\n",
      "The implementation of the Transformer architecture involves designing the network with multiple layers of self-attention and feedforward neural networks. Each layer consists of a multi-head self-attention mechanism, which allows the model to attend to different parts of the input data simultaneously. The output of the self-attention mechanism is then passed through a feedforward neural network with activation functions such as ReLU or GELU to introduce non-linearity into the model.\n",
      "\n",
      "Real-world applications of Transformer architecture in LLMs include natural language processing tasks such as language translation, text generation, and sentiment analysis. Transformers have shown state-of-the-art performance in these tasks, outperforming traditional recurrent neural networks and convolutional neural networks in terms of accuracy and efficiency. Companies like Google, OpenAI, and Facebook have used Transformer-based models in their products and services to improve language understanding and generation capabilities.\n",
      "\n",
      "However, there are also technical challenges associated with the Transformer architecture, such as the high computational cost of training and inference. Transformers require a large amount of memory and computational resources to process input sequences efficiently, making them computationally expensive to train and deploy. Researchers are actively working on developing more efficient versions of the Transformer architecture, such as the Transformer-XL and the Reformer, to address these challenges and make LLMs more accessible to a wider range of applications.\n",
      "\n",
      "---\n",
      "\n",
      "# Key Technical Analysis\n",
      "\n",
      "## Core Concept Summary\n",
      "- **Transformer Architecture**: Utilizes self-attention mechanisms to capture long-range dependencies in input data efficiently.\n",
      "- **Self-Attention Mechanism**: Allows each token to attend to all other tokens in the input sequence, enabling the model to understand relationships and learn complex patterns.\n",
      "\n",
      "## Critical Implementation Insights\n",
      "- **Multi-Layer Design**: Transformer network consists of multiple layers of self-attention and feedforward neural networks.\n",
      "- **Multi-Head Self-Attention**: Enables the model to attend to different parts of the input data simultaneously, enhancing contextual understanding.\n",
      "- **Activation Functions**: Utilized in feedforward neural networks to introduce non-linearity into the model for better predictions.\n",
      "\n",
      "## Key Challenges & Solutions\n",
      "- **High Computational Cost**: Training and inference in Transformers require significant memory and computational resources.\n",
      "- **Solutions**: Ongoing research focuses on developing more efficient versions like Transformer-XL and Reformer to address computational challenges and broaden application possibilities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def run_analysis(topic: str):\n",
    "    result = analysis_pipeline.invoke({\"topic\": topic})\n",
    "\n",
    "    print(\"Analysis Timestamp:\", result[\"timestamp\"])\n",
    "    print(\"\\nTopic:\", result[\"topic\"])\n",
    "    print(\"\\nFormatted Output:\", result[\"formatted_output\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis(\"Transformer attention mechanisms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Evaluation Pipeline\n",
    "\n",
    "In this section, we'll add structured evaluation capabilities to our pipeline, including proper error handling and validation.\n",
    "\n",
    "**Features**\n",
    "- Structured Output: Using schema-based parsing\n",
    "- Validation: Input and output validation\n",
    "- Error Management: Comprehensive error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:33.952957Z",
     "start_time": "2025-01-15T15:01:24.189868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Status: success\n",
      "\n",
      "Evaluation Results: {\n",
      "  \"technical_evaluation\": {\n",
      "    \"core_technical_concepts\": [\n",
      "      \"Transformer model\",\n",
      "      \"Attention mechanisms\",\n",
      "      \"Input sequence processing\",\n",
      "      \"Long-range dependencies\",\n",
      "      \"Context-specific attention patterns\"\n",
      "    ],\n",
      "    \"implementation_details\": \"The content provides a clear explanation of how the attention mechanism works in the Transformer model, including how attention scores are computed and used to generate the final output. It also highlights the advantages of the Transformer model over traditional RNNs and CNNs in capturing long-range dependencies and learning context-specific patterns.\",\n",
      "    \"quality_metrics\": {\n",
      "      \"accuracy\": 9,\n",
      "      \"clarity\": 8,\n",
      "      \"relevance\": 10,\n",
      "      \"depth\": 8\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Processing Time: 9.55 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Structured Evaluation Pipeline\n",
    "\n",
    "This demonstrates:\n",
    "1. Custom output parsing with schema validation\n",
    "2. Error handling at each pipeline stage\n",
    "3. Comprehensive validation system\n",
    "\"\"\"\n",
    "from langchain_core.runnables import RunnableSequence, RunnablePassthrough, RunnableLambda\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Step 1: Define structured output schema\n",
    "response_schemas = [\n",
    "    ResponseSchema(\n",
    "        name=\"technical_evaluation\",\n",
    "        description=\"Technical evaluation of the content\",\n",
    "        type=\"object\",\n",
    "        properties={\n",
    "            \"core_concepts\": {\n",
    "                \"type\": \"array\",\n",
    "                \"description\": \"Key technical concepts identified\"\n",
    "            },\n",
    "            \"implementation_details\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"complexity\": {\"type\": \"string\"},\n",
    "                    \"requirements\": {\"type\": \"array\"},\n",
    "                    \"challenges\": {\"type\": \"array\"}\n",
    "                }\n",
    "            },\n",
    "            \"quality_metrics\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"technical_accuracy\": {\"type\": \"number\"},\n",
    "                    \"completeness\": {\"type\": \"number\"},\n",
    "                    \"clarity\": {\"type\": \"number\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "evaluation_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Step 2: Create basic generation chain\n",
    "generation_prompt = RunnableLambda(lambda x: f\"\"\"Generate technical content about: {x['topic']}\"\"\")\n",
    "basic_generation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[generation_prompt],\n",
    "    last=ChatOpenAI(temperature=0.7)\n",
    ")\n",
    "\n",
    "# Step 3: Create analysis chain\n",
    "analysis_prompt = RunnableLambda(lambda x: f\"\"\"Analyze the following content: {x['generated_content']}\"\"\")\n",
    "analysis_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[analysis_prompt],\n",
    "    last=ChatOpenAI(temperature=0)\n",
    ")\n",
    "\n",
    "# Step 4: Create evaluation chain\n",
    "evaluation_prompt = RunnableLambda(\n",
    "    lambda x: f\"\"\"\n",
    "    Evaluate the following AI technical content:\n",
    "    {x['generated_content']}\n",
    "    \n",
    "    Provide a structured evaluation following these criteria:\n",
    "    1. Identify and list core technical concepts\n",
    "    2. Assess implementation details\n",
    "    3. Rate quality metrics (1-10)\n",
    "    \n",
    "    {evaluation_parser.get_format_instructions()}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "evaluation_chain = RunnableSequence(\n",
    "    first=RunnablePassthrough(),\n",
    "    middle=[evaluation_prompt, ChatOpenAI(temperature=0)],\n",
    "    last=evaluation_parser\n",
    ")\n",
    "\n",
    "# Helper function for error handling\n",
    "def try_or_error(func, error_list):\n",
    "    try:\n",
    "        return func()\n",
    "    except Exception as e:\n",
    "        error_list.append(str(e))\n",
    "        return None\n",
    "\n",
    "# Step 5: Create pipeline components\n",
    "def init_state(x):\n",
    "    return {\n",
    "        \"topic\": x[\"topic\"],\n",
    "        \"errors\": [],\n",
    "        \"start_time\": time.time()\n",
    "    }\n",
    "\n",
    "def generate_content(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"generated_content\": try_or_error(\n",
    "            lambda: basic_generation_chain.invoke({\"topic\": x[\"topic\"]}).content,\n",
    "            x[\"errors\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def perform_analysis(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"analysis\": try_or_error(\n",
    "            lambda: analysis_chain.invoke({\"generated_content\": x[\"generated_content\"]}).content,\n",
    "            x[\"errors\"]\n",
    "        )\n",
    "    }\n",
    "\n",
    "def perform_evaluation(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"evaluation\": try_or_error(\n",
    "            lambda: evaluation_chain.invoke(x),\n",
    "            x[\"errors\"]\n",
    "        ) if not x[\"errors\"] else None\n",
    "    }\n",
    "\n",
    "def finalize_output(x):\n",
    "    return {\n",
    "        **x,\n",
    "        \"completion_time\": time.time() - x[\"start_time\"],\n",
    "        \"status\": \"success\" if not x[\"errors\"] else \"error\"\n",
    "    }\n",
    "\n",
    "# Step 6: Create integrated pipeline\n",
    "def create_evaluation_pipeline():\n",
    "    return RunnableSequence(\n",
    "        first=RunnableLambda(init_state),\n",
    "        middle=[\n",
    "            RunnableLambda(generate_content),\n",
    "            RunnableLambda(perform_analysis),\n",
    "            RunnableLambda(perform_evaluation)\n",
    "        ],\n",
    "        last=RunnableLambda(finalize_output)\n",
    "    )\n",
    "\n",
    "# Example usage\n",
    "def demonstrate_evaluation():\n",
    "    pipeline = create_evaluation_pipeline()\n",
    "    result = pipeline.invoke({\"topic\": \"Transformer attention mechanisms\"})\n",
    "\n",
    "    print(\"Pipeline Status:\", result[\"status\"])\n",
    "    if result[\"status\"] == \"success\":\n",
    "        print(\"\\nEvaluation Results:\", json.dumps(result[\"evaluation\"], indent=2))\n",
    "    else:\n",
    "        print(\"\\nErrors Encountered:\", result[\"errors\"])\n",
    "\n",
    "    print(f\"\\nProcessing Time: {result['completion_time']:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the RunnableBranch\n",
    "\n",
    "`RunnableBranch` is a powerful tool that allows dynamic routing of logic based on input. It enables developers to flexibly define different processing paths depending on the characteristics of the input data.  \n",
    "\n",
    "`RunnableBranch` helps implement complex decision trees in a simple and intuitive way. This greatly improves code readability and maintainability while promoting logic modularization and reusability.  \n",
    "\n",
    "Additionally, `RunnableBranch` can dynamically evaluate branching conditions at runtime and select the appropriate processing routine, enhancing the system's adaptability and scalability.  \n",
    "\n",
    "Due to these features, `RunnableBranch` can be applied across various domains and is particularly useful for developing applications with high input data variability and volatility.\n",
    "\n",
    "By effectively utilizing `RunnableBranch`, developers can reduce code complexity and improve system flexibility and performance.\n",
    "\n",
    "### Dynamic Logic Routing Based on Input\n",
    "\n",
    "This section covers how to perform routing in LangChain Expression Language.\n",
    "\n",
    "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. This helps bring structure and consistency to interactions with LLMs.\n",
    "\n",
    "There are two primary methods for performing routing:\n",
    "\n",
    "1. Returning a Conditionally Executable Object from `RunnableLambda` (*Recommended*)  \n",
    "2. Using `RunnableBranch`\n",
    "\n",
    "Both methods can be explained using a two-step sequence, where the first step classifies the input question as related to math, science, or other, and the second step routes it to the corresponding prompt chain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Example\n",
    "\n",
    "First, we will create a Chain that classifies incoming questions into one of three categories: math, science, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:34.033831Z",
     "start_time": "2025-01-15T15:01:33.996442Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Classify the given user question into one of `math`, `science`, or `other`. Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain.\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    | StrOutputParser()  # Use a string output parser.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the created chain to classify the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:34.729957Z",
     "start_time": "2025-01-15T15:01:34.037030Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'math'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a question.\n",
    "chain.invoke({\"question\": \"What is 2+2?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:35.318878Z",
     "start_time": "2025-01-15T15:01:34.784702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'science'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a question.\n",
    "chain.invoke({\"question\": \"What is the law of action and reaction?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:36.600092Z",
     "start_time": "2025-01-15T15:01:35.321139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'other'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a question.\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableLambda  \n",
    "\n",
    "`RunnableLambda` is a type of `Runnable` designed to simplify the execution of a single transformation or operation using a lambda (anonymous) function. \n",
    "\n",
    "It is primarily used for lightweight, stateless operations where defining an entire custom `Runnable` class would be overkill.  \n",
    "\n",
    "Unlike `RunnableBranch`, which focuses on conditional branching logic, `RunnableLambda` excels in straightforward data transformations or function applications.\n",
    "\n",
    "Syntax  \n",
    "- `RunnableLambda` is initialized with a single lambda function or callable object.  \n",
    "- When invoked, the input value is passed directly to the lambda function.  \n",
    "- The lambda function processes the input and returns the result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create three sub-chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:43.392870Z",
     "start_time": "2025-01-15T15:01:43.301123Z"
    }
   },
   "outputs": [],
   "source": [
    "math_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in math. \\\n",
    "Always answer questions starting with \"Pythagoras once said...\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "science_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert in science. \\\n",
    "Always answer questions starting with \"Isaac Newton once said...\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")\n",
    "\n",
    "general_chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Respond to the following question concisely:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model=\"gpt-4o-mini\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Custom Functions  \n",
    "\n",
    "This is the recommended approach in the official LangChain documentation. You can wrap custom functions with `RunnableLambda` to handle routing between different outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:48.694721Z",
     "start_time": "2025-01-15T15:01:48.656956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Return each chain based on the contents included in the topic.\n",
    "def route(info):\n",
    "    if \"math\" in info[\"topic\"].lower():\n",
    "        return math_chain\n",
    "    elif \"science\" in info[\"topic\"].lower():\n",
    "        return science_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:01:54.060700Z",
     "start_time": "2025-01-15T15:01:54.051653Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = (\n",
    "    {\"topic\": chain, \"question\": itemgetter(\"question\")}\n",
    "    | RunnableLambda(\n",
    "        # Pass the routing function as an argument.\n",
    "        route\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:01.969592Z",
     "start_time": "2025-01-15T15:01:59.057745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pythagoras once said that understanding the relationships between different quantities is essential for grasping the universe's complexities. Calculus is the branch of mathematics that investigates how things change and helps us understand the concept of motion and rates of change. It consists of two main branches: differential calculus, which focuses on the concept of the derivative, measuring how a function changes as its input changes, and integral calculus, which deals with accumulation, essentially summing up small parts to find whole quantities, like areas under curves. Together, these tools allow us to analyze complex systems, model real-world phenomena, and solve problems involving continuous change.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a math-related question.\n",
    "full_chain.invoke({\"question\": \"Please explain the concept of calculus.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:07.166390Z",
     "start_time": "2025-01-15T15:02:01.974991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isaac Newton once said, \"What goes up must come down,\" reflecting his profound understanding of gravity. To calculate gravitational acceleration, we typically use the formula derived from Newton\\'s law of universal gravitation. The gravitational acceleration \\\\( g \\\\) at the surface of a celestial body, such as Earth, can be calculated using the equation:\\n\\n\\\\[\\ng = \\\\frac{G \\\\cdot M}{r^2}\\n\\\\]\\n\\nwhere \\\\( G \\\\) is the universal gravitational constant (\\\\(6.674 \\\\times 10^{-11} \\\\, \\\\text{m}^3 \\\\text{kg}^{-1} \\\\text{s}^{-2}\\\\)), \\\\( M \\\\) is the mass of the celestial body, and \\\\( r \\\\) is the radius from the center of the mass to the point where gravitational acceleration is being calculated. For Earth, this results in an approximate value of \\\\( 9.81 \\\\, \\\\text{m/s}^2 \\\\). Thus, gravitational acceleration can be understood as the force of gravity acting on a unit mass near the surface of a large body.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a science-related question.\n",
    "full_chain.invoke({\"question\": \"How is gravitational acceleration calculated?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:10.192047Z",
     "start_time": "2025-01-15T15:02:07.179066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval Augmented Generation) is a model framework that combines information retrieval and natural language generation. It retrieves relevant documents or information from a large database and uses that information to generate more accurate and contextually relevant text responses. This approach enhances the generation process by grounding it in concrete data, improving both the quality and relevance of the output.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke the chain with a general question.\n",
    "full_chain.invoke({\"question\": \"What is RAG (Retrieval Augmented Generation)?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RunnableBranch\n",
    "\n",
    "`RunnableBranch` is a special type of `Runnable` that allows you to define conditions and corresponding Runnable objects based on input values.\n",
    "\n",
    "However, it does not provide functionality that cannot be achieved with custom functions, so using custom functions is generally recommended.\n",
    "\n",
    "Syntax\n",
    "\n",
    "- `RunnableBranch` is initialized with a list of (condition, Runnable) pairs and a default Runnable.\n",
    "- When invoked, the input value is passed to each condition sequentially.\n",
    "- The first condition that evaluates to True is selected, and the corresponding Runnable is executed with the input value.\n",
    "- If no condition matches, the `default Runnable` is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:10.277611Z",
     "start_time": "2025-01-15T15:02:10.239781Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    # Check if the topic contains \"math\" and execute math_chain if true.\n",
    "    (lambda x: \"math\" in x[\"topic\"].lower(), math_chain),\n",
    "    # Check if the topic contains \"science\" and execute science_chain if true.\n",
    "    (lambda x: \"science\" in x[\"topic\"].lower(), science_chain),\n",
    "    # If none of the above conditions match, execute general_chain.\n",
    "    general_chain,\n",
    ")\n",
    "\n",
    "# Define the full chain that takes a topic and question, routes it, and parses the output.\n",
    "full_chain = (\n",
    "    {\"topic\": chain, \"question\": itemgetter(\"question\")} | branch | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the full chain with each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:16.064646Z",
     "start_time": "2025-01-15T15:02:10.283049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pythagoras once said that understanding the world around us often requires us to look deeper into the relationships between various elements. Calculus, much like the geometric principles he championed, is a branch of mathematics that studies how things change. It is fundamentally divided into two main areas: differentiation and integration.\\n\\nDifferentiation focuses on the concept of the derivative, which represents the rate of change of a quantity. For instance, if you think of a car’s velocity as the rate of change of its position over time, calculus allows us to analyze and predict this kind of change in different contexts.\\n\\nIntegration, on the other hand, deals with the accumulation of quantities, which can be thought of as the total size or area under a curve. It answers questions like how much distance is traveled over time, given a particular speed.\\n\\nTogether, these two concepts allow us to model and understand a vast array of phenomena—from physics to economics—enabling us to explain how systems evolve and interact over time. Just as Pythagoras sought to uncover the hidden relationships within numbers and shapes, calculus seeks to reveal the intricate patterns of change in our world.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"Please explain the concept of calculus.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:19.885867Z",
     "start_time": "2025-01-15T15:02:16.066650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Isaac Newton once said, \"What goes up must come down,\" which reflects the fundamental principle of gravitational attraction. Gravitational acceleration, often denoted as \\\\( g \\\\), can be calculated using the formula:\\n\\n\\\\[\\ng = \\\\frac{G \\\\cdot M}{r^2}\\n\\\\]\\n\\nwhere \\\\( G \\\\) is the universal gravitational constant (approximately \\\\( 6.674 \\\\times 10^{-11} \\\\, \\\\text{N m}^2/\\\\text{kg}^2 \\\\)), \\\\( M \\\\) is the mass of the object creating the gravitational field (like the Earth), and \\\\( r \\\\) is the distance from the center of the mass to the point where the acceleration is being measured (which is the radius of the Earth when calculating gravitational acceleration at its surface). For Earth, this results in a standard gravitational acceleration of approximately \\\\( 9.81 \\\\, \\\\text{m/s}^2 \\\\).'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"How is gravitational acceleration calculated?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:02:22.762591Z",
     "start_time": "2025-01-15T15:02:19.891167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval-Augmented Generation) is a framework that combines retrieval and generative models to improve the quality and relevance of generated text. It first retrieves relevant documents or information from a knowledge base and then uses this data to enhance the generation of responses, making the output more informative and contextually accurate.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"What is RAG (Retrieval Augmented Generation)?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an AI Learning Assistant\n",
    "\n",
    "Let's apply what we've learned about Runnable components to build a practical AI Learning Assistant. This system will help students by providing tailored responses based on their questions.\n",
    "\n",
    "First, let's set up our core components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:03:28.193058Z",
     "start_time": "2025-01-15T15:03:27.934065Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnableBranch, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from datetime import datetime\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "# Question Classification Component\n",
    "question_classifier = RunnableSequence(\n",
    "    first=PromptTemplate.from_template(\n",
    "        \"\"\"Classify this question into one of: beginner, intermediate, advanced\n",
    "        Consider:\n",
    "        - Complexity of concepts\n",
    "        - Prior knowledge required\n",
    "        - Technical depth needed\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Return only the classification word in lowercase.\"\"\"\n",
    "    ),\n",
    "    middle=[ChatOpenAI(temperature=0)],\n",
    "    last=StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example Generator Component\n",
    "example_generator = RunnableSequence(\n",
    "    first=PromptTemplate.from_template(\n",
    "        \"\"\"Generate a practical example for this concept.\n",
    "        Level: {level}\n",
    "        Question: {question}\n",
    "        \n",
    "        If code is needed, provide it in appropriate markdown format.\"\"\"\n",
    "    ),\n",
    "    middle=[ChatOpenAI(temperature=0.7)],\n",
    "    last=StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create our response generation strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:03:30.933238Z",
     "start_time": "2025-01-15T15:03:30.800719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Response Generation Strategy\n",
    "response_strategy = RunnableBranch(\n",
    "    (\n",
    "        lambda x: x[\"level\"] == \"beginner\",\n",
    "        RunnableSequence(\n",
    "            first=PromptTemplate.from_template(\n",
    "                \"\"\"Explain in simple terms for a beginner:\n",
    "                Question: {question}\n",
    "                \n",
    "                Use simple analogies and avoid technical jargon.\"\"\"\n",
    "            ),\n",
    "            middle=[ChatOpenAI(temperature=0.3)],\n",
    "            last=StrOutputParser()\n",
    "        )\n",
    "    ),\n",
    "    (\n",
    "        lambda x: x[\"level\"] == \"intermediate\",\n",
    "        RunnableSequence(\n",
    "            first=PromptTemplate.from_template(\n",
    "                \"\"\"Provide a detailed explanation with practical examples:\n",
    "                Question: {question}\n",
    "                \n",
    "                Include relevant technical concepts and use cases.\"\"\"\n",
    "            ),\n",
    "            middle=[ChatOpenAI(temperature=0.3)],\n",
    "            last=StrOutputParser()\n",
    "        )\n",
    "    ),\n",
    "    # Default case (advanced)\n",
    "    RunnableSequence(\n",
    "        first=PromptTemplate.from_template(\n",
    "            \"\"\"Give an in-depth technical explanation:\n",
    "            Question: {question}\n",
    "            \n",
    "            Include advanced concepts and detailed technical information.\"\"\"\n",
    "        ),\n",
    "        middle=[ChatOpenAI(temperature=0.3)],\n",
    "        last=StrOutputParser()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create our main pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:03:34.002787Z",
     "start_time": "2025-01-15T15:03:33.983471Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_response(x):\n",
    "    return {\n",
    "        \"question\": x[\"question\"],\n",
    "        \"level\": x[\"level\"],\n",
    "        \"explanation\": x[\"response\"],\n",
    "        \"example\": x[\"example\"],\n",
    "        \"metadata\": {\n",
    "            \"difficulty\": x[\"level\"],\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Main Learning Assistant Pipeline\n",
    "learning_assistant = RunnableSequence(\n",
    "    first=RunnableLambda(lambda x: {\"question\": x[\"question\"]}),\n",
    "    middle=[\n",
    "        RunnableLambda(lambda x: {\n",
    "            **x,\n",
    "            \"level\": question_classifier.invoke({\"question\": x[\"question\"]})\n",
    "        }),\n",
    "        RunnableLambda(lambda x: {\n",
    "            **x,\n",
    "            \"response\": response_strategy.invoke(x),\n",
    "            \"example\": example_generator.invoke(x)\n",
    "        })\n",
    "    ],\n",
    "    last=RunnableLambda(format_response)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T15:03:58.664265Z",
     "start_time": "2025-01-15T15:03:36.308469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is a variable in Python?\n",
      "Difficulty Level: beginner\n",
      "\n",
      "Explanation: In Python, a variable is like a container that holds information. Just like a box can hold toys, a variable can hold different types of data like numbers, text, or lists. You can give a variable a name, like \"age\" or \"name\", and then store information in it to use later in your program.Variables are used to store and manipulate data in a program.\n",
      "\n",
      "Example: A variable in Python is a placeholder for storing data values. It can be assigned a value which can be changed or accessed throughout the program.\n",
      "\n",
      "Example:\n",
      "```python\n",
      "# Assigning a value to a variable\n",
      "x = 5\n",
      "\n",
      "# Accessing the value of the variable\n",
      "print(x)  # Output: 5\n",
      "\n",
      "# Changing the value of the variable\n",
      "x = 10\n",
      "\n",
      "# Accessing the updated value of the variable\n",
      "print(x)  # Output: 10\n",
      "```\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: How does dependency injection work?\n",
      "Difficulty Level: intermediate\n",
      "\n",
      "Explanation: Dependency injection is a design pattern commonly used in object-oriented programming to achieve loose coupling between classes. It is a technique where one object supplies the dependencies of another object. This helps in making the code more modular, maintainable, and testable.\n",
      "\n",
      "There are three main types of dependency injection: constructor injection, setter injection, and interface injection.\n",
      "\n",
      "1. Constructor Injection: In constructor injection, the dependencies are provided through the class constructor. This is the most common type of dependency injection. Here is an example in Java:\n",
      "\n",
      "```java\n",
      "public class UserService {\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    public UserService(UserRepository userRepository) {\n",
      "        this.userRepository = userRepository;\n",
      "    }\n",
      "\n",
      "    // Other methods of UserService that use userRepository\n",
      "}\n",
      "```\n",
      "\n",
      "2. Setter Injection: In setter injection, the dependencies are provided through setter methods. Here is an example in Java:\n",
      "\n",
      "```java\n",
      "public class UserService {\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    public void setUserRepository(UserRepository userRepository) {\n",
      "        this.userRepository = userRepository;\n",
      "    }\n",
      "\n",
      "    // Other methods of UserService that use userRepository\n",
      "}\n",
      "```\n",
      "\n",
      "3. Interface Injection: In interface injection, the dependent object implements an interface that defines the method(s) to inject the dependency. Here is an example in Java:\n",
      "\n",
      "```java\n",
      "public interface UserRepositoryInjector {\n",
      "    void injectUserRepository(UserRepository userRepository);\n",
      "}\n",
      "\n",
      "public class UserService implements UserRepositoryInjector {\n",
      "    private UserRepository userRepository;\n",
      "\n",
      "    @Override\n",
      "    public void injectUserRepository(UserRepository userRepository) {\n",
      "        this.userRepository = userRepository;\n",
      "    }\n",
      "\n",
      "    // Other methods of UserService that use userRepository\n",
      "}\n",
      "```\n",
      "\n",
      "Dependency injection is commonly used in frameworks like Spring, where dependencies are managed by the framework and injected into the classes at runtime. This allows for easier configuration and management of dependencies.\n",
      "\n",
      "Overall, dependency injection helps in promoting code reusability, testability, and maintainability by decoupling the classes and their dependencies. It also makes it easier to switch out dependencies or mock them for testing purposes.\n",
      "\n",
      "Example: Dependency injection is a design pattern in which the dependencies of a class are provided externally. This helps in making the code more modular, testable and maintainable.\n",
      "\n",
      "Here is a practical example of how dependency injection works in Java:\n",
      "\n",
      "```java\n",
      "// Interface for the dependency\n",
      "interface Logger {\n",
      "    void log(String message);\n",
      "}\n",
      "\n",
      "// Class that depends on the Logger interface\n",
      "class UserService {\n",
      "    private Logger logger;\n",
      "\n",
      "    // Constructor injection\n",
      "    public UserService(Logger logger) {\n",
      "        this.logger = logger;\n",
      "    }\n",
      "\n",
      "    public void doSomething() {\n",
      "        logger.log(\"Doing something...\");\n",
      "    }\n",
      "}\n",
      "\n",
      "// Implementation of the Logger interface\n",
      "class ConsoleLogger implements Logger {\n",
      "    @Override\n",
      "    public void log(String message) {\n",
      "        System.out.println(message);\n",
      "    }\n",
      "}\n",
      "\n",
      "public class Main {\n",
      "    public static void main(String[] args) {\n",
      "        // Creating an instance of the Logger implementation\n",
      "        Logger logger = new ConsoleLogger();\n",
      "\n",
      "        // Passing the Logger implementation to the UserService class through constructor injection\n",
      "        UserService userService = new UserService(logger);\n",
      "\n",
      "        // Calling a method on the UserService class\n",
      "        userService.doSomething();\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "In this example, the `UserService` class depends on the `Logger` interface. Instead of creating an instance of the `Logger` implementation (`ConsoleLogger`) inside the `UserService` class, we provide the `Logger` implementation externally through constructor injection. This allows us to easily swap out different implementations of the `Logger` interface without modifying the `UserService` class.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: Explain quantum computing qubits\n",
      "Difficulty Level: intermediate\n",
      "\n",
      "Explanation: Quantum computing qubits are the fundamental building blocks of quantum computers. Unlike classical computers, which use bits to represent information as either a 0 or a 1, quantum computers use qubits to represent information as a combination of 0 and 1 simultaneously. This property, known as superposition, allows quantum computers to perform complex calculations much faster than classical computers.\n",
      "\n",
      "One of the key concepts in quantum computing is entanglement, which allows qubits to be correlated with each other in such a way that the state of one qubit can instantly affect the state of another qubit, regardless of the distance between them. This property enables quantum computers to perform parallel computations and solve certain problems exponentially faster than classical computers.\n",
      "\n",
      "There are several types of qubits that can be used in quantum computing, including superconducting qubits, trapped ions, and topological qubits. Each type of qubit has its own advantages and disadvantages, and researchers are actively working to develop new qubit technologies that can overcome existing limitations and improve the performance of quantum computers.\n",
      "\n",
      "One practical example of quantum computing qubits is in the field of cryptography. Quantum computers have the potential to break many of the encryption algorithms that are currently used to secure sensitive information, such as credit card numbers and government communications. By leveraging the power of qubits and quantum algorithms, researchers are developing new encryption techniques that are resistant to attacks from quantum computers.\n",
      "\n",
      "Another use case for quantum computing qubits is in the field of drug discovery. Quantum computers have the ability to simulate the behavior of molecules at the quantum level, which can help researchers design new drugs more efficiently and accurately. By using qubits to model the interactions between atoms and molecules, scientists can identify potential drug candidates and optimize their properties before conducting costly and time-consuming experiments in the lab.\n",
      "\n",
      "In conclusion, quantum computing qubits are a revolutionary technology that has the potential to transform many industries and solve complex problems that are currently beyond the reach of classical computers. By harnessing the power of superposition and entanglement, quantum computers can perform calculations at speeds that were previously thought impossible, opening up new possibilities for innovation and discovery.\n",
      "\n",
      "Example: Practical example:\n",
      "\n",
      "Imagine you have a classical computer with a bit that can be in one of two states: 0 or 1. This bit can represent a single piece of information. Now, imagine you have a quantum computer with a qubit. A qubit can be in a superposition of both 0 and 1 states at the same time. This means that a qubit can represent multiple pieces of information simultaneously.\n",
      "\n",
      "For example, if you have 3 qubits, they can be in a superposition of 8 different states (2^3 = 8). This allows quantum computers to perform complex calculations much faster than classical computers.\n",
      "\n",
      "```markdown\n",
      "// Example code in Qiskit for creating a quantum circuit with qubits\n",
      "\n",
      "from qiskit import QuantumCircuit\n",
      "\n",
      "# Create a quantum circuit with 3 qubits\n",
      "qc = QuantumCircuit(3)\n",
      "\n",
      "# Apply operations to the qubits\n",
      "qc.h(0)  # Apply a Hadamard gate to qubit 0\n",
      "qc.cx(0, 1)  # Apply a CNOT gate between qubit 0 and qubit 1\n",
      "qc.measure_all()  # Measure all qubits in the circuit\n",
      "\n",
      "print(qc)\n",
      "```\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "async def run_assistant():\n",
    "    # Example questions for different levels\n",
    "    questions = [\n",
    "        \"What is a variable in Python?\",\n",
    "        \"How does dependency injection work?\",\n",
    "        \"Explain quantum computing qubits\"\n",
    "    ]\n",
    "    \n",
    "    for question in questions:\n",
    "        result = await learning_assistant.ainvoke({\"question\": question})\n",
    "        print(f\"\\nQuestion: {result['question']}\")\n",
    "        print(f\"Difficulty Level: {result['level']}\")\n",
    "        print(f\"\\nExplanation: {result['explanation']}\")\n",
    "        print(f\"\\nExample: {result['example']}\")\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# For Jupyter environments\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the assistant\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_assistant())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of RunnableSequence, RunnableBranch, and RunnableLambda\n",
    "\n",
    "| Criteria | RunnableSequence | RunnableBranch | RunnableLambda |\n",
    "|----------|------------------|----------------|----------------|\n",
    "| Primary Purpose | Sequential pipeline processing | Conditional routing and branching | Simple transformations and functions |\n",
    "| Condition Definition | No conditions, sequential flow | Each condition defined as `(condition, runnable)` pair | All conditions within single function (`route`) |\n",
    "| Structure | Linear chain of operations | Tree-like branching structure | Function-based transformation |\n",
    "| Readability | Very clear for sequential processes | Becomes clearer as conditions increase | Very clear for simple logic |\n",
    "| Maintainability | Easy to maintain step-by-step flow | Clear separation between conditions and runnables | Can become complex if function grows large |\n",
    "| Flexibility | Flexible for linear processes | Must follow `(condition, runnable)` pattern | Allows flexible condition writing |\n",
    "| Scalability | Add or modify pipeline steps | Requires adding new conditions and runnables | Expandable by modifying function |\n",
    "| Error Handling | Pipeline-level error management | Branch-specific error handling | Basic error handling |\n",
    "| State Management | Maintains state throughout pipeline | State managed per branch | Typically stateless |\n",
    "| Recommended Use Case | When you need ordered processing steps | When there are many conditions or maintainability is priority | When conditions are simple or function-based |\n",
    "| Complexity Level | Medium to High | Medium | Low |\n",
    "| Async Support | Full async support | Limited async support | Basic async support |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
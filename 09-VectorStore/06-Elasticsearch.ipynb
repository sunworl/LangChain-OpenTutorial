{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch\n",
    "\n",
    "- Author: [liniar](https://github.com/namyoungkim)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/06-Elasticsearch.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/06-Elasticsearch.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "- This tutorial is designed for beginners to get started with Elasticsearch and its integration with LangChain.\n",
    "- You‚Äôll learn how to set up the environment, prepare data, and explore advanced search features like hybrid and semantic search.\n",
    "- By the end, you‚Äôll be equipped to use Elasticsearch for powerful and intuitive search applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Elasticsearch Setup](#elasticsearch-setup)\n",
    "- [Introduction to Elasticsearch](#introduction-to-elasticsearch)\n",
    "- [Data Preparation for Tutorial](#data-preparation-for-tutorial)\n",
    "- [Managing Elasticsearch Connections and Documents](#managing-elasticsearch-connections-and-documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [LangChain VectorStore Documentation](https://python.langchain.com/docs/how_to/vectorstores/)\n",
    "- [LangChain Elasticsearch Integration](https://python.langchain.com/docs/integrations/vectorstores/elasticsearch/)\n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup  \n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.  \n",
    "\n",
    "**[Note]**  \n",
    "- `langchain-opentutorial` is a package that provides a set of **easy-to-use environment setup,** **useful functions,** and **utilities for tutorials.**  \n",
    "- You can check out the [`langchain-opentutorial` ](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.  \n",
    "\n",
    "\n",
    "### üõ†Ô∏è **The following configurations will be set up**  \n",
    "\n",
    "- **Jupyter Notebook Output Settings**\n",
    "    - Display standard error ( `stderr` ) messages directly instead of capturing them.  \n",
    "- **Install Required Packages** \n",
    "    - Ensure all necessary dependencies are installed.  \n",
    "- **API Key Setup** \n",
    "    - Configure the API key for authentication.  \n",
    "- **PyTorch Device Selection Setup** \n",
    "    - Automatically select the optimal computing device (CPU, CUDA, or MPS).\n",
    "        - `{\"device\": \"mps\"}` : Perform embedding calculations using **MPS** instead of GPU. (For Mac users)\n",
    "        - `{\"device\": \"cuda\"}` : Perform embedding calculations using **GPU.** (For Linux and Windows users, requires CUDA installation)\n",
    "        - `{\"device\": \"cpu\"}` : Perform embedding calculations using **CPU.** (Available for all users)\n",
    "- **Embedding Model Local Storage Path** \n",
    "    - Define a local path for storing embedding models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Setup\n",
    "- In order to use the Elasticsearch vector search you must install the langchain-elasticsearch package.\n",
    "\n",
    "### üöÄ Setting Up Elasticsearch with Elastic Cloud (Colab Compatible)\n",
    "- Elastic Cloud allows you to manage Elasticsearch seamlessly in the cloud, eliminating the need for local installations.\n",
    "- It integrates well with Google Colab, enabling efficient experimentation and prototyping.\n",
    "\n",
    "\n",
    "### üìö What is Elastic Cloud?  \n",
    "- **Elastic Cloud** is a managed Elasticsearch service provided by Elastic.  \n",
    "- Supports **custom cluster configurations** and **auto-scaling.** \n",
    "- Deployable on **AWS**, **GCP**, and **Azure.**  \n",
    "- Compatible with **Google Colab,** allowing simplified cloud-based workflows.  \n",
    "\n",
    "### üìå Getting Started with Elastic Cloud  \n",
    "1. **Sign up for Elastic Cloud‚Äôs Free Trial.**  \n",
    "    - [Free Trial](https://cloud.elastic.co/registration?utm_source=langchain&utm_content=documentation)\n",
    "2. **Create an Elasticsearch Cluster.**  \n",
    "3. **Retrieve your Elasticsearch URL** and **Elasticsearch API Key** from the Elastic Cloud Console.  \n",
    "4. Add the following to your `.env` file\n",
    "    > ```\n",
    "    > ES_URL=https://my-elasticsearch-project-abd...:123\n",
    "    > ES_API_KEY=bk9X...\n",
    "    > ```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n9NVKk-Zf9Nq"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IMx2hZNXf9QL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain-core\",\n",
    "        \"langchain_huggingface\",\n",
    "        \"langchain_elasticsearch\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"elasticsearch\",\n",
    "        \"python-dotenv\",\n",
    "        \"uuid\",\n",
    "        \"torch\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from dotenv import load_dotenv\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
    "if not load_dotenv():\n",
    "    set_env(\n",
    "        {\n",
    "            \"OPENAI_API_KEY\": \"\",\n",
    "            \"LANGCHAIN_API_KEY\": \"\",\n",
    "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "            \"LANGCHAIN_PROJECT\": \"Elasticsearch\",\n",
    "            \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
    "            \"ES_URL\": \"\",\n",
    "            \"ES_API_KEY\": \"\",\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using MPS (Metal Performance Shaders) on macOS\n",
      "üñ•Ô∏è Current device in use: mps\n"
     ]
    }
   ],
   "source": [
    "# Automatically select the appropriate device\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if platform.system() == \"Darwin\":  # macOS specific\n",
    "        if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "            print(\"‚úÖ Using MPS (Metal Performance Shaders) on macOS\")\n",
    "            return \"mps\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"‚úÖ Using CUDA (NVIDIA GPU)\")\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        print(\"‚úÖ Using CPU\")\n",
    "        return \"cpu\"\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = get_device()\n",
    "print(\"üñ•Ô∏è Current device in use:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "N8C6pLTZf9Sb"
   },
   "outputs": [],
   "source": [
    "# Embedding Model Local Storage Path\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set the download path to ./cache/\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Elasticsearch\n",
    "- Elasticsearch is an open-source, distributed search and analytics engine designed to store, search, and analyze both structured and unstructured data in real-time.\n",
    "\n",
    "### üìå Key Features  \n",
    "- **Real-Time Search:** Instantly searchable data upon ingestion  \n",
    "- **Large-Scale Data Processing:** Efficient handling of vast datasets  \n",
    "- **Scalability:** Flexible scaling through clustering and distributed architecture  \n",
    "- **Versatile Search Support:** Keyword search, semantic search, and multimodal search  \n",
    "\n",
    "### üìå Use Cases  \n",
    "- **Log Analytics:** Real-time monitoring of system and application logs  \n",
    "- **Monitoring:** Server and network health tracking  \n",
    "- **Product Recommendations:** Behavior-based recommendation systems  \n",
    "- **Natural Language Processing (NLP):** Semantic text searches  \n",
    "- **Multimodal Search:** Text-to-image and image-to-image searches  \n",
    "\n",
    "### üß† Vector Database Functionality in Elasticsearch  \n",
    "- Elasticsearch supports vector data storage and similarity search via **Dense Vector Fields.** As a vector database, it excels in applications like NLP, image search, and recommendation systems.\n",
    "\n",
    "### üìå Core Vector Database Features  \n",
    "- **Dense Vector Field:** Store and query high-dimensional vectors  \n",
    "- **KNN (k-Nearest Neighbors) Search:** Find vectors most similar to the input  \n",
    "- **Semantic Search:** Perform meaning-based searches beyond keyword matching  \n",
    "- **Multimodal Search:** Combine text and image data for advanced search capabilities  \n",
    "\n",
    "### üìå Vector Search Use Cases  \n",
    "- **Semantic Search:** Understand user intent and deliver precise results  \n",
    "- **Text-to-Image Search:** Retrieve relevant images from textual descriptions  \n",
    "- **Image-to-Image Search:** Find visually similar images in a dataset  \n",
    "\n",
    "### üîó Official Documentation Links  \n",
    "- [Elasticsearch Official Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/index.html)  \n",
    "- [Elasticsearch Vector Search Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/dense-vector.html)  \n",
    "\n",
    "Elasticsearch goes beyond traditional text search engines, offering robust vector database capabilities essential for NLP and multimodal search applications. üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Tutorial\n",
    "- Let‚Äôs process **The Little Prince** using the `RecursiveCharacterTextSplitter` to create document chunks.\n",
    "- Then, we‚Äôll generate embeddings for each text chunk and store the resulting data in a vector database to proceed with a vector database tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Little Prince\\nWritten By Antoine de Saiot-Exupery (1900„Äú1944)', '[ Antoine de Saiot-Exupery ]']\n",
      "Total number of chunks: 1359\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "# Function to read text from a file (Cross-Platform)\n",
    "def read_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            # Normalize line endings (compatible with Windows, macOS, Linux)\n",
    "            raw_text = f.read().replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "        return raw_text\n",
    "    except UnicodeDecodeError as e:\n",
    "        raise ValueError(f\"Failed to decode the file with UTF-8 encoding: {e}\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"The specified file was not found: {file_path}\")\n",
    "\n",
    "\n",
    "# Function to split the text into chunks\n",
    "def split_text(raw_text, chunk_size=100, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,  # Default string length function\n",
    "        is_separator_regex=False,  # Default separator setting\n",
    "    )\n",
    "    split_docs = text_splitter.create_documents([raw_text])\n",
    "    return [doc.page_content for doc in split_docs]\n",
    "\n",
    "\n",
    "# Set file path and execute\n",
    "file_path = \"./data/the_little_prince.txt\"\n",
    "try:\n",
    "    # Read the file\n",
    "    raw_text = read_text_file(file_path)\n",
    "    # Split the text\n",
    "    docs = split_text(raw_text)\n",
    "\n",
    "    # Verify output\n",
    "    print(docs[:2])  # Print the first 5 chunks\n",
    "    print(f\"Total number of chunks: {len(docs)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1359\n",
      "1024\n",
      "CPU times: user 7.25 s, sys: 3.48 s, total: 10.7 s\n",
      "Wall time: 18.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## text embedding\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "\n",
    "hf_embeddings_e5_instruct = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": device},  # mps, cuda, cpu\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")\n",
    "\n",
    "embedded_documents = hf_embeddings_e5_instruct.embed_documents(docs)\n",
    "\n",
    "print(len(embedded_documents))\n",
    "print(len(embedded_documents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Elasticsearch Connections and Documents\n",
    "### ElasticsearchConnectionManager\n",
    "- The `ElasticsearchConnectionManager` is a class designed to manage connections to an Elasticsearch instance.\n",
    "- It facilitates connecting to the Elasticsearch server and provides functionalities for creating and deleting indices.\n",
    "\n",
    "### Initialization\n",
    "**Setting Up the Elasticsearch Client**\n",
    "- Begin by creating an Elasticsearch client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "ES_URL = os.environ[\"ES_URL\"]  # Elasticsearch host URL\n",
    "ES_API_KEY = os.environ[\"ES_API_KEY\"]  # Elasticsearch API key\n",
    "\n",
    "# Ensure required environment variables are set\n",
    "if not ES_URL or not ES_API_KEY:\n",
    "    raise ValueError(\"Both ES_URL and ES_API_KEY must be set in environment variables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elasticsearch import ElasticsearchConnectionManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchain_tutorial_es\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector dimension\n",
    "dims = len(embedded_documents[0])\n",
    "\n",
    "\n",
    "# üõ†Ô∏è Define the mapping for the new index\n",
    "# This structure specifies the schema for documents stored in Elasticsearch\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"metadata\": {\"properties\": {\"doc_id\": {\"type\": \"keyword\"}}},\n",
    "        \"text\": {\"type\": \"text\"},  # Field for storing textual content\n",
    "        \"vector\": {  # Field for storing vector embeddings\n",
    "            \"type\": \"dense_vector\",  # Specifies dense vector type\n",
    "            \"dims\": dims,  # Number of dimensions in the vector\n",
    "            \"index\": True,  # Enable indexing for vector search\n",
    "            \"similarity\": \"cosine\",  # Use cosine similarity for vector comparisons\n",
    "        },\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you'll learn how to generate text embeddings for documents using a Hugging Face model.\n",
    "- First, we'll set up a multilingual model with the `HuggingFaceEmbeddings` class and choose the optimal device (mps, cuda, or cpu) for computation.\n",
    "- Then, we'll generate embeddings for a list of documents and print the results to ensure everything is working correctly.\n",
    "\n",
    "The `ElasticsearchConnectionManager` class manages the connection to an Elasticsearch server.\n",
    "- This instance uses the server URL, API key, embedding model, and index name to connect to Elasticsearch and initialize the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/ [status:200 duration:0.701s]\n",
      "INFO:utils.elasticsearch:‚úÖ Successfully connected to Elasticsearch!\n",
      "INFO:elastic_transport.transport:GET https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/ [status:200 duration:0.555s]\n",
      "INFO:utils.elasticsearch:‚úÖ Vector store initialized for index 'langchain_tutorial_es'.\n"
     ]
    }
   ],
   "source": [
    "es_connection_manager = ElasticsearchConnectionManager(\n",
    "    es_url=ES_URL,\n",
    "    api_key=ES_API_KEY,\n",
    "    embedding_model=hf_embeddings_e5_instruct,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:404 duration:0.183s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.259s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' created successfully.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create index\n",
    "es_connection_manager.create_index(index_name, mapping=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.180s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.209s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete index\n",
    "es_connection_manager.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticsearchDocumentManager\n",
    "- The `ElasticsearchDocumentManager` leverages the `ElasticsearchConnectionManager` to handle document management tasks.\n",
    "- This class performs operations such as inserting, deleting, and searching documents, with the capability to enhance performance through parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.elasticsearch import ElasticsearchDocumentManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_document_manager = ElasticsearchDocumentManager(\n",
    "    connection_manager=es_connection_manager,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert\n",
    "- The `upsert` method of the `es_document_manager` is used to insert or update documents in the specified Elasticsearch index.\n",
    "- It takes the original texts, their corresponding embedded documents, and the index name to efficiently manage the document storage and retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:5.399s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:5.555s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:3.942s]\n",
      "INFO:utils.elasticsearch:‚úÖ Bulk upsert completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 591 ms, sys: 63 ms, total: 654 ms\n",
      "Wall time: 15.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "es_document_manager.upsert(\n",
    "    texts=docs,\n",
    "    embedded_documents=embedded_documents,\n",
    "    index_name=index_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_delete_by_query?conflicts=proceed [status:200 duration:0.354s]\n"
     ]
    }
   ],
   "source": [
    "es_document_manager.delete(index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert_parallel\n",
    "- The `upsert_parallel` method of the `es_document_manager` facilitates the parallel insertion or updating of documents in the specified Elasticsearch index.\n",
    "- It processes the documents in batches of 100, utilizing up to 8 workers to enhance performance and efficiency in managing large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.347s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.582s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.753s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.850s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.600s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.479s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.462s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.869s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:2.609s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.347s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.676s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:0.888s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.851s]\n",
      "INFO:elastic_transport.transport:PUT https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/_bulk [status:200 duration:1.626s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 656 ms, sys: 45.4 ms, total: 702 ms\n",
      "Wall time: 7.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "es_document_manager.upsert_parallel(\n",
    "    index_name=index_name,\n",
    "    texts=docs,\n",
    "    embedded_documents=embedded_documents,\n",
    "    batch_size=100,\n",
    "    max_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is evident that parallel_upsert is **faster.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search\n",
    "- The code performs a search query, \"Who are the Little Prince‚Äôs friends?\", using the `es_document_manager` to retrieve relevant documents from the specified Elasticsearch index.\n",
    "- By default ( `use_similarity=False` ), it uses the **BM25** algorithm, which is a bag-of-words retrieval function that ranks documents based on the query terms' appearances, regardless of their semantic meaning.\n",
    "- It fetches the top 10 results, then prints the query and each result in a formatted manner for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.735s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "================================================\n",
      "0  : \"Who are you?\" said the little prince.\n",
      "1  : \"Who are you--Who are you--Who are you?\" answered the echo.\n",
      "2  : people. For some, who are travelers, the stars are guides. For others they are no more than little\n",
      "3  : people. For some, who are travelers, the stars are guides. For others they are no more than little\n",
      "4  : (picture)\n",
      "\"Who are you?\" asked the little prince, and added, \"You are very pretty to look at.\"\n",
      "5  : no more than little lights in the sky. For others, who are scholars, they are problems . For my\n",
      "6  : no more than little lights in the sky. For others, who are scholars, they are problems . For my\n",
      "7  : \"Who are you?\" he demanded, thunderstruck. \n",
      "\"We are roses,\" the roses said.\n",
      "8  : \"No,\" said the little prince. \"I am looking for friends. What does that mean-- ‚Äòtame‚Äò?\"\n",
      "9  : \"Just that,\" said the fox. \"To me, you are still nothing more than a little boy who is just like a\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "\n",
    "results = es_document_manager.search(index_name=index_name, query=search_query, k=10)\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"================================================\")\n",
    "for idx_, result in enumerate(results):\n",
    "    print(idx_, \" :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves the top 10 relevant documents using similarity-based matching(cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search?_source_includes=metadata,text [status:200 duration:0.377s]\n",
      "INFO:utils.elasticsearch:‚úÖ Found 10 similar documents.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "================================================\n",
      "0  : \"Who are you?\" said the little prince.\n",
      "1  : \"Then what?\" asked the little prince.\n",
      "2  : And the little prince asked himself:\n",
      "3  : \"Why is that?\" asked the little prince.\n",
      "4  : \"What do you do here?\" the little prince asked.\n",
      "5  : [ Chapter 13 ]\n",
      "- the little prince visits the businessman\n",
      "6  : But the little prince was wondering... The planet was tiny. Over what could this king really rule?\n",
      "7  : \"Where are the men?\" the little prince asked, politely.\n",
      "8  : \"No,\" said the little prince. \"I am looking for friends. What does that mean-- ‚Äòtame‚Äò?\"\n",
      "9  : But the little prince added:\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "results = es_document_manager.search(query=search_query, k=10, use_similarity=True)\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"================================================\")\n",
    "for idx_, result in enumerate(results):\n",
    "    print(idx_, \" :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs a search for the query \"Who are the Little Prince‚Äôs friends?\" while also filtering results based on the **keyword \"friend,\"** retrieving the top 10 relevant documents and printing their content alongside additional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search?_source_includes=metadata,text [status:200 duration:0.248s]\n",
      "INFO:utils.elasticsearch:‚úÖ Hybrid search completed. Found 10 results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================\n",
      "üîç Question:  Who are the Little Prince‚Äôs friends?\n",
      "================================================\n",
      "0  : \"My friend the fox--\" the little prince said to me. 0.9277072\n",
      "1  : any more. If you want a friend, tame me...\" 0.91347504\n",
      "2  : a grown-up. I have a serious reason: he is the best friend I have in the world. I have another 0.905076\n",
      "3  : My friend broke into another peal of laughter: \"But where do you think he would go?\" 0.90468454\n",
      "4  : He was only a fox like a hundred thousand other foxes. But I have made him my friend, and now he is 0.9021255\n",
      "5  : that you have known me. You will always be my friend. You will want to laugh with me. And you will 0.89545083\n",
      "6  : a friend. And if I forget him, I may become like the grown-ups who are no longer interested in 0.8951793\n",
      "7  : that you have known me. You will always be my friend. You will want to laugh with me. And you will 0.8949666\n",
      "8  : \"That man is the only one of them all whom I could have made my friend. But his planet is indeed 0.8948114\n",
      "9  : to seek, in other days, merely by pulling up his chair; and he wanted to help his friend. 0.8929472\n"
     ]
    }
   ],
   "source": [
    "search_query = \"Who are the Little Prince‚Äôs friends?\"\n",
    "keyword = \"friend\"\n",
    "results = es_document_manager.search(\n",
    "    query=search_query, k=10, use_similarity=True, keyword=keyword\n",
    ")\n",
    "\n",
    "print(\"================================================\")\n",
    "print(\"üîç Question: \", search_query)\n",
    "print(\"================================================\")\n",
    "for idx_, contents in enumerate(results):\n",
    "    print(idx_, \" :\", contents[0].page_content, contents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This approach ensures that the search results are both contextually meaningful and aligned with the specified keyword constraint, making it especially useful in scenarios where both precision and context matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read\n",
    "- This code retrieves the IDs of all documents stored in the specified Elasticsearch index using the `get_documents_ids` method of the `es_document_manager`, and then prints the list of these document IDs for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.468s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mfqx9ZQBuaU-CwHIDaXY', 'mvqx9ZQBuaU-CwHIDaXY', 'm_qx9ZQBuaU-CwHIDaXY', 'nPqx9ZQBuaU-CwHIDaXY', 'nfqx9ZQBuaU-CwHIDaXY', 'nvqx9ZQBuaU-CwHIDaXY', 'n_qx9ZQBuaU-CwHIDaXY', 'oPqx9ZQBuaU-CwHIDaXY', 'ofqx9ZQBuaU-CwHIDaXY', 'ovqx9ZQBuaU-CwHIDaXY']\n"
     ]
    }
   ],
   "source": [
    "ids = es_document_manager.get_documents_ids(index_name)\n",
    "print(ids[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fetches documents from the specified Elasticsearch index using a list of document IDs, specifically retrieving the first 10 IDs.\n",
    "\n",
    "It then prints each document's ID along with its corresponding text for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_search [status:200 duration:0.377s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fb6a7033-465e-4a39-8577-5797fcc67c20 :  \"What does this mean?\" I demanded. \"Why are you talking with snakes?\"\n",
      "e549da15-6a9c-4589-9645-5263d9aa2615 :  I had loosened the golden muffler that he always wore. I had moistened his temples, and had given\n",
      "4c6a0aa2-a626-4a59-838d-989f07cff105 :  and had given him some water to drink. And now I did not dare ask him any more questions. He looked\n",
      "101c6f61-3bc8-4036-b0b8-e9a36119970f :  He looked at me very gravely, and put his arms around my neck. I felt his heart beating like the\n",
      "075b3e39-80c1-434e-b632-96b586c32f6b :  beating like the heart of a dying bird, shot with someone‚Äòs rifle...\n",
      "d451662f-52dc-41cf-b8e9-2cc81a6f7138 :  \"I am glad that you have found what was the matter with your engine,\" he said. \"Now you can go back\n",
      "9ee4c5fa-68f1-4292-9d95-362834edc807 :  you can go back home--\"\n",
      "dcdd7bc6-214e-454a-a8b3-a5e0acb19a1c :  \"How do you know about that?\"\n",
      "7468cc42-01aa-425d-acd0-0abf8fe50f0b :  I was just coming to tell him that my work had been successful, beyond anything that I had dared to\n",
      "7b3b9425-eca8-44b4-a6d5-d741d0100b0a :  that I had dared to hope. He made no answer to my question, but he added:\n"
     ]
    }
   ],
   "source": [
    "responses = es_document_manager.get_documents_by_ids(index_name, ids[:10])\n",
    "\n",
    "for response in responses:\n",
    "    print(response[\"doc_id\"], \": \", response[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete\n",
    "- This code deletes documents from the specified Elasticsearch index using a list of document IDs, specifically retrieving the first 10 IDs. It then prints each document's ID along with its corresponding text for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/mfqx9ZQBuaU-CwHIDaXY [status:200 duration:0.190s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/mvqx9ZQBuaU-CwHIDaXY [status:200 duration:0.189s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/m_qx9ZQBuaU-CwHIDaXY [status:200 duration:0.194s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/nPqx9ZQBuaU-CwHIDaXY [status:200 duration:0.204s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/nfqx9ZQBuaU-CwHIDaXY [status:200 duration:0.188s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/nvqx9ZQBuaU-CwHIDaXY [status:200 duration:0.189s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/n_qx9ZQBuaU-CwHIDaXY [status:200 duration:0.185s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/oPqx9ZQBuaU-CwHIDaXY [status:200 duration:0.188s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/ofqx9ZQBuaU-CwHIDaXY [status:200 duration:0.187s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_doc/ovqx9ZQBuaU-CwHIDaXY [status:200 duration:0.188s]\n"
     ]
    }
   ],
   "source": [
    "es_document_manager.delete(index_name=index_name, ids=ids[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:POST https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es/_delete_by_query?conflicts=proceed [status:200 duration:0.374s]\n"
     ]
    }
   ],
   "source": [
    "# Delete all documents\n",
    "es_document_manager.delete(index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:elastic_transport.transport:HEAD https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.186s]\n",
      "INFO:elastic_transport.transport:DELETE https://e638d39188c94d828a30ae87af1733ce.us-central1.gcp.cloud.es.io:443/langchain_tutorial_es [status:200 duration:0.215s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"‚úÖ Index 'langchain_tutorial_es' deleted successfully.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete index\n",
    "es_connection_manager.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove a **Huggingface Cache**  , `embeddings` and `client` .\n",
    "\n",
    "If you created a **vectordb** directory, please **remove** it at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCacheStrategy(expected_freed_size=0, blobs=frozenset(), refs=frozenset(), repos=frozenset(), snapshots=frozenset())"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "del embedded_documents\n",
    "del es_connection_manager\n",
    "del es_document_manager\n",
    "scan = scan_cache_dir()\n",
    "scan.delete_revisions()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "langchain-opentutorial-gmgjIYR5-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

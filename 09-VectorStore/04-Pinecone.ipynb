{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pinecone\n",
    "\n",
    "- Author: [ro__o_jun](https://github.com/ro-jun)\n",
    "- Design: []()\n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/08-Embeeding/01-OpenAIEmbeddings.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/08-Embeeding/01-OpenAIEmbeddings.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial provides a comprehensive guide to integrating `Pinecone` with `LangChain` for creating and managing high-performance vector databases.  \n",
    "\n",
    "It explains how to set up `Pinecone` , `preprocess documents` , and utilize Pinecone's APIs for vector indexing and `document retrieval` .  \n",
    "\n",
    "Additionally, it demonstrates advanced features like `hybrid search` using `dense` and `sparse embeddings` , `metadata filtering` , and `dynamic reranking` to build efficient and scalable search systems.  \n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [What is Pinecone?](#what-is-pinecone)\n",
    "- [Pinecone setup guide](#Pinecone-setup-guide)\n",
    "- [Data preprocessing](#data-preprocessing)\n",
    "- [Pinecone and LangChain Integration Guide: Step by Step](#pinecone-and-langchain-integration-guide-step-by-step)\n",
    "- [Pinecone: Add to DB Index (Upsert)](#pinecone-add-to-db-index-upsert)\n",
    "- [Index inquiry/delete](#index-inquirydelete)\n",
    "- [Create HybridRetrieve](#create-hybridretrieve)\n",
    "\n",
    "### References\n",
    "\n",
    "- [Langchain-PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html)\n",
    "- [Langchain-Retrievers](https://python.langchain.com/docs/integrations/retrievers/pinecone_hybrid_search/)\n",
    "- [Pinecone-Docs](https://docs.pinecone.io/guides/get-started/overview)\n",
    "- [Pinecone-Docs-integrations](https://docs.pinecone.io/integrations/langchain)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain-pinecone\",\n",
    "        \"pinecone[grpc]\",\n",
    "        \"nltk\",\n",
    "        \"langchain_community\",\n",
    "        \"pymupdf\",\n",
    "        \"langchain-openai\",\n",
    "        \"pinecone-text\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"PINECONE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Pinecone\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Note] If you are using a `.env` file, proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pinecone?\n",
    "\n",
    "`Pinecone` is a **cloud-based** , high-performance vector database for **efficient vector storage and retrieval** in AI and machine learning applications.\n",
    "\n",
    "**Features** :\n",
    "1. **Supports SDKs** for Python, Node.js, Java, and Go.\n",
    "2. **Fully managed** : Reduces the burden of infrastructure management.\n",
    "3. **Real-time updates** : Supports real-time insertion, updates, and deletions.\n",
    "\n",
    "**Advantages** :\n",
    "1. Scalability for large datasets.\n",
    "2. Real-time data processing.\n",
    "3. High availability with cloud infrastructure.\n",
    "\n",
    "**Disadvantages** :\n",
    "1. Relatively higher cost compared to other vector databases.\n",
    "2. Limited customization options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone setup guide\n",
    "\n",
    "This section explains how to set up `Pinecone` , including `API key` creation.\n",
    "\n",
    "**[steps]**\n",
    "\n",
    "1. Log in to [Pinecone](https://www.pinecone.io/)\n",
    "2. Create an API key under the `API Keys` tab.\n",
    "\n",
    "![example](./assets/04-pinecone-api-01.png)  \n",
    "![example](./assets/04-pinecone-api-02.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Below is the preprocessing process for general documents.  \n",
    "Reads all `data/*.pdf` files under `ROOT_DIR` and saves them in `document_lsit.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 414 documents from 1 files.\n",
      "Number of processed documents: 414\n"
     ]
    }
   ],
   "source": [
    "from utils.pinecone import DocumentProcessor\n",
    "\n",
    "directory_path = \"data/*.pdf\"\n",
    "doc_processor = DocumentProcessor(\n",
    "    directory_path=directory_path,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    use_basename=True,\n",
    ")\n",
    "split_docs = doc_processor.process_pdf_files(directory_path)\n",
    "\n",
    "print(f\"Number of processed documents: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'up. I have a serious reason: he is the best friend I have in the world. I have another reason: this grown-up understands everything, even books about children. I have a third reason: he lives in France where he is hungry and cold. He needs cheering up. If all these'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[12].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'TheLittlePrince.pdf',\n",
       " 'file_path': 'data\\\\TheLittlePrince.pdf',\n",
       " 'page': 2,\n",
       " 'total_pages': 64,\n",
       " 'format': 'PDF 1.3',\n",
       " 'title': '',\n",
       " 'author': 'Paula MacDowell',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'Safari',\n",
       " 'producer': 'Mac OS X 10.10.5 Quartz PDFContext',\n",
       " 'creationDate': \"D:20160209011144Z00'00'\",\n",
       " 'modDate': \"D:20160209011144Z00'00'\",\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[12].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs document processing to save DB in Pinecone. You can select `metadata_keys` during this process.\n",
    "\n",
    "You can additionally tag metadata and, if desired, add and process metadata ahead of time in a preprocessing task.\n",
    "\n",
    "- `split_docs` : List[Document] containing the results of document splitting.\n",
    "- `metadata_keys` : List containing metadata keys to be added to the document.\n",
    "- `min_length` : Specifies the minimum length of the document. Documents shorter than this length are excluded.\n",
    "- `use_basename` : Specifies whether to use the file name based on the source path. The default is `False` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing of documents**\n",
    "\n",
    "- Extract the required `metadata` information.\n",
    "- Filters only data longer than the minimum length.\n",
    "- Specifies whether to use the document's `basename` . The default is `False` .\n",
    "- Here, `basename` refers to the very last part of the file.\n",
    "- For example, `/data/TheLittlePrince.pdf` becomes `TheLittlePrince.pdf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing documents: 100%|██████████| 414/414 [00:00<00:00, 31331.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processed documents: 414\n",
      "Metadata keys: ['source', 'page', 'author']\n",
      "Sample 'source' metadata: ['TheLittlePrince.pdf', 'TheLittlePrince.pdf', 'TheLittlePrince.pdf', 'TheLittlePrince.pdf', 'TheLittlePrince.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "contents, metadatas = doc_processor.preprocess_documents(docs=split_docs, min_length=10)\n",
    "\n",
    "print(f\"Number of processed documents: {len(contents)}\")\n",
    "print(f\"Metadata keys: {list(metadatas.keys())}\")\n",
    "print(f\"Sample 'source' metadata: {metadatas['source'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 414, 414, 414)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of documents, check number of sources, check number of pages\n",
    "len(contents), len(metadatas[\"source\"]), len(metadatas[\"page\"]), len(\n",
    "    metadatas[\"author\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone and LangChain Integration Guide: Step by Step\n",
    "\n",
    "This guide outlines the integration of Pinecone and LangChain to set up and utilize a vector database. \n",
    "\n",
    "Below are the key steps to complete the integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone client initialization and vector database setup\n",
    "\n",
    "The provided code performs the initialization of a Pinecone client, sets up an index in Pinecone, and defines a vector database to store embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[caution]**    \n",
    "\n",
    "If you are considering HybridSearch, specify the metric as dotproduct.  \n",
    "Basic users cannot use PodSpec.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pinecone index settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This explains how to create and check indexes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.pinecone.PineconeDocumentManager at 0x18a2d5fffd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from utils.pinecone import PineconeDocumentManager\n",
    "\n",
    "# Initialize Pinecone client with API key from environment variables\n",
    "pc_db = PineconeDocumentManager(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "pc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Indexes: [{\n",
      "    \"name\": \"langchain-opentutorial-index\",\n",
      "    \"dimension\": 3072,\n",
      "    \"metric\": \"dotproduct\",\n",
      "    \"host\": \"langchain-opentutorial-index-9v46jum.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"cloud\": \"aws\",\n",
      "            \"region\": \"us-east-1\"\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"deletion_protection\": \"disabled\"\n",
      "}, {\n",
      "    \"name\": \"langchain-opentutorial-multimodal-1024\",\n",
      "    \"dimension\": 1024,\n",
      "    \"metric\": \"dotproduct\",\n",
      "    \"host\": \"langchain-opentutorial-multimodal-1024-9v46jum.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"cloud\": \"aws\",\n",
      "            \"region\": \"us-east-1\"\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"deletion_protection\": \"disabled\"\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "# Check existing index names\n",
    "pc_db.check_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing index: langchain-opentutorial-index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pinecone.grpc.index_grpc.GRPCIndex at 0x18a2dea3c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec, PodSpec\n",
    "\n",
    "# Create or reuse the index\n",
    "index_name = \"langchain-opentutorial-index\"\n",
    "\n",
    "# Set to True when using the serverless method, and False when using the PodSpec method.\n",
    "use_serverless = True\n",
    "if use_serverless:\n",
    "    spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "else:\n",
    "    spec = PodSpec(environment=\"us-west1-gcp\", pod_type=\"p1.x1\", pods=1)\n",
    "\n",
    "pc_db.create_index(\n",
    "    index_name=index_name,\n",
    "    dimension=3072,\n",
    "    metric=\"dotproduct\",\n",
    "    spec=spec,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how to check the inside of an index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414}},\n",
      " 'total_vector_count': 414}\n"
     ]
    }
   ],
   "source": [
    "index = pc_db.get_index(index_name)\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![04-pinecone-index.png](./assets/04-pinecone-index.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is how to clear an index.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** If you want to delete the index, uncomment the lines below and run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_name = \"langchain-opentutorial-index2\"\n",
    "\n",
    "# pc_db.delete_index(index_name)\n",
    "# print(pc_db.list_indexes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sparse Encoder\n",
    "\n",
    "- Create a sparse encoder.\n",
    "\n",
    "- Perform stopword processing.\n",
    "\n",
    "- Learn contents using Sparse Encoder. The encode learned here is used to create a Sparse Vector when storing documents in VectorStore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplified NLTK-based BM25 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Downloading NLTK stopwords and punkt tokenizer...\n",
      "[INFO] NLTK setup completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\thdgh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thdgh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.pinecone import NLTKBM25Tokenizer\n",
    "\n",
    "tokenizer = NLTKBM25Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before stop words modification: ['example', 'text', 'contains', 'punctuation', 'stop', 'words']\n",
      "\n",
      "After adding stop words: ['example', 'contains', 'punctuation', 'words']\n",
      "\n",
      "After removing stop words: ['example', 'text', 'contains', 'punctuation', 'stop', 'words']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example text, and it contains some punctuation and stop words.\"\n",
    "tokens = tokenizer(text)\n",
    "\n",
    "print(\"Before stop words modification:\", tokenizer(text))\n",
    "tokenizer.add_stop_words([\"text\", \"stop\"])\n",
    "print(\"\\nAfter adding stop words:\", tokenizer(text))\n",
    "tokenizer.remove_stop_words([\"text\", \"stop\"])\n",
    "print(\"\\nAfter removing stop words:\", tokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Sparse Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone_text.sparse import BM25Encoder\n",
    "\n",
    "sparse_encoder = BM25Encoder()\n",
    "\n",
    "# Connect custom tokenizer\n",
    "sparse_encoder._tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01b87838ee442458749ba656f950ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indices': [3127628307, 3368723024], 'values': [0.49504950495049505, 0.49504950495049505]}\n"
     ]
    }
   ],
   "source": [
    "# sparse_encoder test\n",
    "test_corpus = [\"This is a text document.\", \"Another document for testing.\"]\n",
    "sparse_encoder.fit(test_corpus)\n",
    "\n",
    "print(sparse_encoder.encode_documents(\"Test document.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the corpus on Sparse Encoder.\n",
    "\n",
    "- `save_path` : Path to save Sparse Encoder. Later, the Sparse Encoder saved in pickle format will be loaded and used for query embedding. Therefore, specify the path to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41a856696a34ac1a4dd849cfc5ed99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_sparse_encoder]\n",
      "Saved Sparse Encoder to: ./sparse_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "save_path = \"./sparse_encoder.pkl\"\n",
    "\n",
    "# Learn and save Sparse Encoder.\n",
    "sparse_encoder.fit(contents)\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(sparse_encoder, f)\n",
    "print(f\"[fit_sparse_encoder]\\nSaved Sparse Encoder to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional]  \n",
    "Below is the code to use when you need to reload the learned and saved Sparse Encoder later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_sparse_encoder]\n",
      "Loaded Sparse Encoder from: ./sparse_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./sparse_encoder.pkl\"\n",
    "\n",
    "# It is used later to load the learned sparse encoder.\n",
    "try:\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        loaded_file = pickle.load(f)\n",
    "    print(f\"[load_sparse_encoder]\\nLoaded Sparse Encoder from: {file_path}\")\n",
    "    sparse_encoder = loaded_file\n",
    "except Exception as e:\n",
    "    print(f\"[load_sparse_encoder]\\n{e}\")\n",
    "    sparse_encoder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone: Add to DB Index (Upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![04-pinecone-upsert](./assets/04-pinecone-upsert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `context`: This is the context of the document.\n",
    "- `page` : The page number of the document.\n",
    "- `source` : This is the source of the document.\n",
    "- `values` : This is an embedding of a document obtained through Embedder.\n",
    "- `sparse values` : This is an embedding of a document obtained through Sparse Encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsert documents in batches without distributed processing.\n",
    "If the amount of documents is not large, use the method below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [00:59<00:00,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414}},\n",
      " 'total_vector_count': 414}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Please set\n",
    "embedder = openai_embeddings\n",
    "batch_size = 32\n",
    "namespace = \"langchain-opentutorial-01\"\n",
    "\n",
    "# Running upsert on Pinecone\n",
    "pc_db.upsert_documents(\n",
    "    index=index,\n",
    "    contents=contents,\n",
    "    metadatas=metadatas,\n",
    "    embedder=openai_embeddings,\n",
    "    sparse_encoder=sparse_encoder,\n",
    "    namespace=namespace,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, distributed processing is performed to quickly upsert large documents. Use this for large uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches in Parallel: 100%|██████████| 13/13 [00:12<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414},\n",
      "                'langchain-opentutorial-02': {'vector_count': 0}},\n",
      " 'total_vector_count': 414}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "embedder = openai_embeddings\n",
    "# Set batch size and number of workers\n",
    "batch_size = 32\n",
    "max_workers = 8\n",
    "namespace = \"langchain-opentutorial-02\"\n",
    "\n",
    "# Running Upsert in Parallel on Pinecone\n",
    "pc_db.upsert_documents_parallel(\n",
    "    index=index,\n",
    "    contents=contents,\n",
    "    metadatas=metadatas,\n",
    "    embedder=openai_embeddings,\n",
    "    sparse_encoder=sparse_encoder,\n",
    "    namespace=namespace,\n",
    "    batch_size=batch_size,\n",
    "    max_workers=max_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414},\n",
      "                'langchain-opentutorial-02': {'vector_count': 414}},\n",
      " 'total_vector_count': 828}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![04-pinecone-namespaces-01.png](./assets/04-pinecone-namespaces-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index inquiry/delete\n",
    "\n",
    "The `describe_index_stats` method provides statistical information about the contents of an index. This method allows you to obtain information such as the number of vectors and dimensions per namespace.\n",
    "\n",
    "**Parameter** * `filter` (Optional[Dict[str, Union[str, float, int, bool, List, dict]]]): A filter that returns statistics only for vectors that meet certain conditions. Default is None * `**kwargs`: Additional keyword arguments\n",
    "\n",
    "**Return value** * `DescribeIndexStatsResponse`: Object containing statistical information about the index\n",
    "\n",
    "**Usage example** * Default usage: `index.describe_index_stats()` * Apply filter: `index.describe_index_stats(filter={'key': 'value'})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414},\n",
       "                'langchain-opentutorial-02': {'vector_count': 414}},\n",
       " 'total_vector_count': 828}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index lookup\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search for documents in the index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'doc-303',\n",
      "              'metadata': {'author': 'Paula MacDowell',\n",
      "                           'context': \"o'clock in the afternoon, then at three \"\n",
      "                                      \"o'clock I shall begin to be happy. I \"\n",
      "                                      'shall feel happier and happier as the '\n",
      "                                      \"hour advances. At four o'clock, I shall \"\n",
      "                                      'already be worrying and jumping about. '\n",
      "                                      'I shall show you how',\n",
      "                           'page': 46.0,\n",
      "                           'source': 'TheLittlePrince.pdf'},\n",
      "              'score': 1.3499277,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []},\n",
      "             {'id': 'doc-304',\n",
      "              'metadata': {'author': 'Paula MacDowell',\n",
      "                           'context': 'happy I am! But if you come at just any '\n",
      "                                      'time, I shall never know at what hour '\n",
      "                                      'my heart is to be ready to greet you . '\n",
      "                                      '. . One must observe the proper rites . '\n",
      "                                      '. .\" \"What is a rite?\" asked the little '\n",
      "                                      'prince.',\n",
      "                           'page': 46.0,\n",
      "                           'source': 'TheLittlePrince.pdf'},\n",
      "              'score': 1.1850042,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []},\n",
      "             {'id': 'doc-98',\n",
      "              'metadata': {'author': 'Paula MacDowell',\n",
      "                           'context': '\"I am very fond of sunsets. Come, let '\n",
      "                                      'us go look at a sunset now.\" \"But we '\n",
      "                                      'must wait,\" I said. \"Wait? For what?\" '\n",
      "                                      '\"For the sunset. We must wait until it '\n",
      "                                      'is time.\" At first you seemed to be '\n",
      "                                      'very much surprised. And then you '\n",
      "                                      'laughed to yourself. You said to me:',\n",
      "                           'page': 15.0,\n",
      "                           'source': 'TheLittlePrince.pdf'},\n",
      "              'score': 0.84356034,\n",
      "              'sparse_values': {'indices': [], 'values': []},\n",
      "              'values': []}],\n",
      " 'namespace': 'langchain-opentutorial-01',\n",
      " 'usage': {'read_units': 11}}\n"
     ]
    }
   ],
   "source": [
    "# Define your query\n",
    "question = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "\n",
    "# Convert the query into dense and sparse vectors\n",
    "dense_vector = embedder.embed_query(question)\n",
    "sparse_vector = sparse_encoder.encode_documents(question)\n",
    "\n",
    "results = pc_db.search(\n",
    "    index = index,\n",
    "    namespace=\"langchain-opentutorial-01\",\n",
    "    query=dense_vector,\n",
    "    sparse_vector=sparse_vector,\n",
    "    top_k=3,\n",
    "    include_metadata=True,\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delete namespace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(delete_all=True, namespace=\"langchain-opentutorial-02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![04-pinecone-namespaces-02.png](./assets/04-pinecone-namespaces-02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414}},\n",
       " 'total_vector_count': 414}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are features exclusive to paid users. Metadata filtering is available to paid users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while deleting using filter:\n",
      "UNKNOWN:Error received from peer  {grpc_message:\"Invalid request.\", grpc_status:3, created_time:\"2025-02-15T15:26:54.3610786+00:00\"}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dimension': 3072,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'langchain-opentutorial-01': {'vector_count': 414}},\n",
       " 'total_vector_count': 414}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone.exceptions import PineconeException\n",
    "\n",
    "try:\n",
    "    index.delete(\n",
    "        filter={\"source\": {\"$eq\": \"TheLittlePrince.pdf\"}},\n",
    "        namespace=\"langchain-opentutorial-01\",\n",
    "    )\n",
    "except PineconeException as e:\n",
    "    print(f\"Error while deleting using filter:\\n{e}\")\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create HybridRetrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PineconeHybridRetriever initialization parameter settings**\n",
    "\n",
    "The `init_pinecone_index` function and the `PineconeHybridRetriever` class implement a hybrid search system using Pinecone. This system combines dense and sparse vectors to perform effective document retrieval.\n",
    "\n",
    "Pinecone index initialization\n",
    "\n",
    "The `init_pinecone_index` function initializes the Pinecone index and sets up the necessary components.\n",
    "\n",
    "Parameters \n",
    "* `index_name` (str): Pinecone index name \n",
    "* `namespace` (str): Namespace to use \n",
    "* `api_key` (str): Pinecone API key \n",
    "* `sparse_encoder_pkl_path` (str): Sparse encoder pickle file path \n",
    "* `stopwords` (List[str]): List of stop words \n",
    "* `tokenizer` (str): Tokenizer to use (default: \"nltk\") \n",
    "* `embeddings` (Embeddings): Embedding model \n",
    "* `alpha` (float): Weight of dense and sparse vectors Adjustment parameter (default: 0.5)\n",
    "* `top_k` (int): Maximum number of documents to return (default: 4) \n",
    "\n",
    "**Main features** \n",
    "1. Pinecone index initialization and statistical information output\n",
    "2. Sparse encoder (BM25) loading and tokenizer settings\n",
    "3. Specify namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Hybrid Search Retriever initialized for index 'langchain-opentutorial-index'.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from utils.pinecone import PineconeDocumentManager\n",
    "import os\n",
    "\n",
    "pc_db = PineconeDocumentManager(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Settings\n",
    "index_name = \"langchain-opentutorial-index\"\n",
    "namespace = \"langchain-opentutorial-01\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "sparse_encoder = sparse_encoder  # Pre-initialized BM25Encoder\n",
    "\n",
    "# Create Hybrid Search Retriever\n",
    "retriever = pc_db.create_hybrid_search_retriever(\n",
    "    index_name=index_name,\n",
    "    embeddings=embeddings,\n",
    "    sparse_encoder=sparse_encoder,\n",
    "    namespace=namespace,\n",
    "    alpha=0.5,\n",
    "    top_k=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main properties** \n",
    "* `embeddings` : Embedding model for dense vector transformations \n",
    "* `sparse_encoder:` Encoder for sparse vector transformations \n",
    "* `index` : Pinecone index object \n",
    "* `top_k` : Maximum number of documents to return \n",
    "* `alpha` : Weight adjustment parameters for dense and sparse vectors \n",
    "* `namespace` : Namespace within the Pinecone index.\n",
    "\n",
    "**Features** \n",
    "* HybridSearch Retriever combining dense and sparse vectors \n",
    "* Search strategy can be optimized through weight adjustment \n",
    "* Various dynamic metadata filtering can be applied (using `search_kwargs` : `filter` , `top_k` , `alpha` , etc.)\n",
    "\n",
    "**Use example** \n",
    "1. Initialize required components with the `init_pinecone_index` function   \n",
    "2. Create a `PineconeHybridRetriever` instance with initialized components.  \n",
    "3. Perform a hybrid search using the generated retriever to create a `PineconeHybridRetriever`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**general search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\n",
      "Metadata: {'context': \"o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\", 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: happy I am! But if you come at just any time, I shall never know at what hour my heart is to be ready to greet you . . . One must observe the proper rites . . .\" \"What is a rite?\" asked the little prince.\n",
      "Metadata: {'context': 'happy I am! But if you come at just any time, I shall never know at what hour my heart is to be ready to greet you . . . One must observe the proper rites . . .\" \"What is a rite?\" asked the little prince.', 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: of misunderstandings. But you will sit a little closer to me, every day . . .\" The next day the little prince came back. \"It would have been better to come back at the same hour,\" said the fox. \"If, for example, you come at four\n",
      "Metadata: {'context': 'of misunderstandings. But you will sit a little closer to me, every day . . .\" The next day the little prince came back. \"It would have been better to come back at the same hour,\" said the fox. \"If, for example, you come at four', 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: \"I am very fond of sunsets. Come, let us go look at a sunset now.\" \"But we must wait,\" I said. \"Wait? For what?\" \"For the sunset. We must wait until it is time.\" At first you seemed to be very much surprised. And then you laughed to yourself. You said to me:\n",
      "Metadata: {'context': '\"I am very fond of sunsets. Come, let us go look at a sunset now.\" \"But we must wait,\" I said. \"Wait? For what?\" \"For the sunset. We must wait until it is time.\" At first you seemed to be very much surprised. And then you laughed to yourself. You said to me:', 'page': 15.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "search_results = retriever(query)\n",
    "\n",
    "for result in search_results:\n",
    "    print(\"Page Content:\", result[\"metadata\"][\"context\"])\n",
    "    print(\"Metadata:\", result[\"metadata\"])\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dynamic search_kwargs - k: specify maximum number of documents to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\n",
      "Metadata: {'context': \"o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\", 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: happy I am! But if you come at just any time, I shall never know at what hour my heart is to be ready to greet you . . . One must observe the proper rites . . .\" \"What is a rite?\" asked the little prince.\n",
      "Metadata: {'context': 'happy I am! But if you come at just any time, I shall never know at what hour my heart is to be ready to greet you . . . One must observe the proper rites . . .\" \"What is a rite?\" asked the little prince.', 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "\n",
    "search_kwargs = {\"top_k\": 2}\n",
    "search_results = retriever(query, **search_kwargs)\n",
    "\n",
    "for result in search_results:\n",
    "    print(\"Page Content:\", result[\"metadata\"][\"context\"])\n",
    "    print(\"Metadata:\", result[\"metadata\"])\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use dynamic `search_kwargs` - `alpha` : Weight adjustment parameters for dense and sparse vectors. Specify a value between 0 and 1. `0.5` is the default, the closer it is to 1, the higher the weight of the dense vector is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\n",
      "Metadata: {'context': \"o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\", 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: of misunderstandings. But you will sit a little closer to me, every day . . .\" The next day the little prince came back. \"It would have been better to come back at the same hour,\" said the fox. \"If, for example, you come at four\n",
      "Metadata: {'context': 'of misunderstandings. But you will sit a little closer to me, every day . . .\" The next day the little prince came back. \"It would have been better to come back at the same hour,\" said the fox. \"If, for example, you come at four', 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "\n",
    "search_kwargs = {\"alpha\": 1, \"top_k\": 2}\n",
    "search_results = retriever(query, **search_kwargs)\n",
    "\n",
    "for result in search_results:\n",
    "    print(\"Page Content:\", result[\"metadata\"][\"context\"])\n",
    "    print(\"Metadata:\", result[\"metadata\"])\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\n",
      "Metadata: {'context': \"o'clock in the afternoon, then at three o'clock I shall begin to be happy. I shall feel happier and happier as the hour advances. At four o'clock, I shall already be worrying and jumping about. I shall show you how\", 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: happy I am! But if you come at just any time, I shall never know at what hour my heart is to be ready to greet you . . . One must observe the proper rites . . .\" \"What is a rite?\" asked the little prince.\n",
      "Metadata: {'context': 'happy I am! But if you come at just any time, I shall never know at what hour my heart is to be ready to greet you . . . One must observe the proper rites . . .\" \"What is a rite?\" asked the little prince.', 'page': 46.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "\n",
    "search_kwargs = {\"alpha\": 0, \"top_k\": 2}\n",
    "search_results = retriever(query, **search_kwargs)\n",
    "\n",
    "for result in search_results:\n",
    "    print(\"Page Content:\", result[\"metadata\"][\"context\"])\n",
    "    print(\"Metadata:\", result[\"metadata\"])\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metadata filtering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![04-pinecone-filter](./assets/04-pinecone-filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using dynamic search_kwargs - filter: Apply metadata filtering\n",
    "\n",
    "(Example) Search with a value less than 25 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: \"I am very fond of sunsets. Come, let us go look at a sunset now.\" \"But we must wait,\" I said. \"Wait? For what?\" \"For the sunset. We must wait until it is time.\" At first you seemed to be very much surprised. And then you laughed to yourself. You said to me:\n",
      "Metadata: {'context': '\"I am very fond of sunsets. Come, let us go look at a sunset now.\" \"But we must wait,\" I said. \"Wait? For what?\" \"For the sunset. We must wait until it is time.\" At first you seemed to be very much surprised. And then you laughed to yourself. You said to me:', 'page': 15.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: Hum! That will be about--about--that will be this evening about twenty minutes to eight. And you will see how well I am obeyed!\" The little prince yawned. He was regretting his lost sunset. And then, too, he was already beginning to be a little bored.\n",
      "Metadata: {'context': 'Hum! That will be about--about--that will be this evening about twenty minutes to eight. And you will see how well I am obeyed!\" The little prince yawned. He was regretting his lost sunset. And then, too, he was already beginning to be a little bored.', 'page': 24.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: \"I am always thinking that I am at home!\" Just so. Everybody knows that when it is noon in the United States the sun is setting over France. If you could fly to France in one minute, you could go straight into the sunset, right from noon.\n",
      "Metadata: {'context': '\"I am always thinking that I am at home!\" Just so. Everybody knows that when it is noon in the United States the sun is setting over France. If you could fly to France in one minute, you could go straight into the sunset, right from noon.', 'page': 15.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "\n",
    "search_kwargs = {\"alpha\": 1, \"top_k\": 3, \"filter\": {\"page\": {\"$lt\": 25}}}\n",
    "search_results = retriever(query, **search_kwargs)\n",
    "\n",
    "for result in search_results:\n",
    "    print(\"Page Content:\", result[\"metadata\"][\"context\"])\n",
    "    print(\"Metadata:\", result[\"metadata\"])\n",
    "    print(\"\\n====================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Content: He should be able, for example, to order me to be gone by the end of one minute. It seems to me that conditions are favorable . . .\" As the king made no answer, the little prince hesitated a moment. Then, with a sigh, he took his leave.\n",
      "Metadata: {'context': 'He should be able, for example, to order me to be gone by the end of one minute. It seems to me that conditions are favorable . . .\" As the king made no answer, the little prince hesitated a moment. Then, with a sigh, he took his leave.', 'page': 25.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: way.\" \"No,\" said the king. But the little prince, having now completed his preparations for departure, had no wish to grieve the old monarch. \"If Your Majesty wishes to be promptly obeyed,\" he said, \"he should be able to give me a reasonable order.\n",
      "Metadata: {'context': 'way.\" \"No,\" said the king. But the little prince, having now completed his preparations for departure, had no wish to grieve the old monarch. \"If Your Majesty wishes to be promptly obeyed,\" he said, \"he should be able to give me a reasonable order.', 'page': 25.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: \"And you actually believe that the flowers--\" \"Oh, no!\" I cried. \"No, no, no! I don't believe anything. I answered you with the first thing that came into my head. Don't you see--I am very busy with matters of consequence!\" He stared at me, thunderstruck. \"Matters of consequence!\"\n",
      "Metadata: {'context': '\"And you actually believe that the flowers--\" \"Oh, no!\" I cried. \"No, no, no! I don\\'t believe anything. I answered you with the first thing that came into my head. Don\\'t you see--I am very busy with matters of consequence!\" He stared at me, thunderstruck. \"Matters of consequence!\"', 'page': 16.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n",
      "Page Content: I did not answer. At that instant I was saying to myself: \"If this bolt still won't turn, I am going to knock it out with the hammer.\" Again the little prince disturbed my thoughts: \"And you actually believe that the flowers--\"\n",
      "Metadata: {'context': 'I did not answer. At that instant I was saying to myself: \"If this bolt still won\\'t turn, I am going to knock it out with the hammer.\" Again the little prince disturbed my thoughts: \"And you actually believe that the flowers--\"', 'page': 16.0, 'author': 'Paula MacDowell', 'source': 'TheLittlePrince.pdf'}\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"If you come at 4 PM, I will be happy from 3 PM. As time goes by, I will become happier.\"\n",
    "\n",
    "search_kwargs = {\"alpha\": 1, \"top_k\": 4, \"filter\": {\"page\": {\"$in\": [25, 16]}}}\n",
    "search_results = retriever(query, **search_kwargs)\n",
    "\n",
    "for result in search_results:\n",
    "    print(\"Page Content:\", result[\"metadata\"][\"context\"])\n",
    "    print(\"Metadata:\", result[\"metadata\"])\n",
    "    print(\"\\n====================\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-XrZComUd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

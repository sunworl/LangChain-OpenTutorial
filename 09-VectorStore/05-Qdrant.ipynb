{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant\n",
    "\n",
    "- Author: [HyeonJong Moon](https://github.com/hj0302)\n",
    "- Design: \n",
    "- Peer Review: \n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to utilize the features related to the `Qdrant` vector database.\n",
    "\n",
    "[`Qdrant`](https://python.langchain.com/docs/integrations/vectorstores/qdrant/) is an open-source vector similarity search engine designed to store, search, and manage high-dimensional vectors with additional payloads. It offers a production-ready service with a user-friendly API, suitable for applications such as semantic search, recommendation systems, and more.\n",
    "\n",
    "**Qdrant's architecture** is optimized for efficient vector similarity searches, employing advanced indexing techniques like **Hierarchical Navigable Small World (HNSW)** graphs to enable fast and scalable retrieval of relevant data.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Credentials](#credentials)\n",
    "- [Installation](#installation)\n",
    "- [Initialization](#initialization)\n",
    "- [Manage Vector Store](#manage-vector-store)\n",
    "  - [Create a Collection](#create-a-collection)\n",
    "  - [List Collections](#list-collections)\n",
    "  - [Delete a Collection](#delete-a-collection)\n",
    "  - [Add Items to the Vector Store](#add-items-to-the-vector-store)\n",
    "  - [Delete Items from the Vector Store](#delete-items-from-the-vector-store)\n",
    "  - [Upsert Items to Vector Store (Parallel)](#upsert-items-to-vector-store-parallel)\n",
    "- [Query Vector Store](#query-vector-store)\n",
    "  - [Query Directly](#query-directly)\n",
    "  - [Similarity Search with Score](#similarity-search-with-score)\n",
    "  - [Query by Turning into Retriever](#query-by-turning-into-retriever)\n",
    "  - [Search with Filtering](#search-with-filtering)\n",
    "  - [Delete with Filtering](#delete-with-filtering)\n",
    "  - [Filtering and Updating Records](#filtering-and-updating-records)\n",
    "\n",
    "### References\n",
    "\n",
    "- [LangChain Qdrant Reference](https://python.langchain.com/docs/integrations/vectorstores/qdrant/)\n",
    "- [Qdrant Official Reference](https://qdrant.tech/documentation/frameworks/langchain/)\n",
    "- [Qdrant Install Reference](https://qdrant.tech/documentation/guides/installation/)\n",
    "- [Qdrant Cloud Reference](https://cloud.qdrant.io)\n",
    "- [Qdrant Cloud Quickstart Reference](https://qdrant.tech/documentation/quickstart-cloud/)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to Environment Setup for more details.\n",
    "\n",
    "[Note]\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_qdrant\",\n",
    "        \"qdrant_client\",\n",
    "        \"langchain_core\",\n",
    "        \"fastembed\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPEN_API_KEY\": \"\",\n",
    "        \"QDRANT_API_KEY\": \"\",\n",
    "        \"QDRANT_URL\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"Qdrant\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "**[Note]** If you are using a `.env` file, proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Credentials**\n",
    "\n",
    "Create a new account or sign in to your existing one, and generate an API key for use in this notebook.\n",
    "\n",
    "1. **Log in to Qdrant Cloud** : Go to the [Qdrant Cloud](https://cloud.qdrant.io) website and log in using your email, Google account, or GitHub account.\n",
    "\n",
    "2. **Create a Cluster** : After logging in, navigate to the **\"Clusters\"** section and click the **\"Create\"** button. Choose your desired configurations and region, then click **\"Create\"** to start building your cluster. Once the cluster is created, an API key will be generated for you.\n",
    "\n",
    "3. **Retrieve and Store Your API Key** : When your cluster is created, you will receive an API key. Ensure you save this key in a secure location, as you will need it later. If you lose it, you will have to generate a new one.\n",
    "\n",
    "4. **Manage API Keys** : To create additional API keys or manage existing ones, go to the **\"Access Management\"** section in the Qdrant Cloud dashboard and select *\"Qdrant Cloud API Keys\"* Here, you can create new keys or delete existing ones.\n",
    "\n",
    "```\n",
    "QDRANT_API_KEY=\"YOUR_QDRANT_API_KEY\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installation**\n",
    "\n",
    "There are several main options for initializing and using the **Qdrant** vector store:\n",
    "\n",
    "- **Local Mode** : This mode doesn't require a separate server.\n",
    "    - **In-memory storage** (data is not persisted)\n",
    "    - **On-disk storage** (data is saved to your local machine)\n",
    "- **Docker Deployments** : You can run **Qdrant** using **Docker**.\n",
    "- **Qdrant Cloud** : Use **Qdrant** as a managed cloud service.\n",
    "\n",
    "For detailed instructions, see the [installation instructions](https://qdrant.tech/documentation/guides/installation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Memory\n",
    "\n",
    "For simple tests or quick experiments, you might choose to store data directly in memory. This means the data is automatically removed when your client terminates, typically at the end of your script or notebook session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'demo_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'demo_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with in-memory storage\n",
    "db = QdrantDocumentManager(\n",
    "    location=\":memory:\",  # Use in-memory database for temporary storage\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Disk Storage\n",
    "\n",
    "With **on-disk storage**, you can store your vectors directly on your hard drive without requiring a **Qdrant server**. This ensures that your data persists even when you restart the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'demo_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'demo_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the path for Qdrant storage\n",
    "qdrant_path = \"./qdrant_memory\"\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    path=qdrant_path,  # Specify the path for Qdrant storage\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker Deployments\n",
    "\n",
    "You can deploy `Qdrant` in a **production environment** using [`Docker`](https://qdrant.tech/documentation/guides/installation/#docker) and [`Docker Compose`](https://qdrant.tech/documentation/guides/installation/#docker-compose). Refer to the `Docker` and `Docker Compose` setup instructions in the development section for detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.qdrant import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the URL for Qdrant server\n",
    "url = \"http://localhost:6333\"\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=url,  # Specify the path for Qdrant storage\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qdrant Cloud\n",
    "\n",
    "For a **production environment**, you can use [**Qdrant Cloud**](https://cloud.qdrant.io/). It offers fully managed `Qdrant` databases with features such as **horizontal and vertical scaling**, **one-click setup and upgrades**, **monitoring**, **logging**, **backups**, and **disaster recovery**. For more information, refer to the [**Qdrant Cloud documentation**](https://qdrant.tech/documentation/cloud/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# Fetch the Qdrant server URL from environment variables or prompt for input\n",
    "if not os.getenv(\"QDRANT_URL\"):\n",
    "    os.environ[\"QDRANT_URL\"] = getpass.getpass(\"Enter your Qdrant Cloud URL key: \")\n",
    "QDRANT_URL = os.environ.get(\"QDRANT_URL\")\n",
    "\n",
    "# Fetch the Qdrant API key from environment variables or prompt for input\n",
    "if not os.getenv(\"QDRANT_API_KEY\"):\n",
    "    os.environ[\"QDRANT_API_KEY\"] = getpass.getpass(\"Enter your Qdrant API key: \")\n",
    "QDRANT_API_KEY = os.environ.get(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.qdrant import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Once you've established your **vector store**, you'll likely need to manage the **collections** within it. Here are some common operations you can perform:\n",
    "\n",
    "- **Create a collection**\n",
    "- **List collections**\n",
    "- **Delete a collection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To proceed with the tutorial, we will use **Qdrant Cloud** for the next steps. This approach ensures that your data is securely stored in the cloud, allowing for seamless access, comprehensive testing, and experimentation across different environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Collection\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to create a new **collection** in `Qdrant`. It can automatically create a collection if it doesn't exist or if you want to **recreate** it. You can specify configurations for **dense** and **sparse vectors** to meet different search needs. Use the `_ensure_collection_exists` method for **automatic creation** or call `create_collection` directly when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'test_collection' does not exist or force recreate is enabled. Creating new collection...\n",
      "Collection 'test_collection' created successfully with configuration: {'vectors_config': VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None)}\n"
     ]
    }
   ],
   "source": [
    "from utils.qdrant import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client.http.models import Distance\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    "    metric=Distance.COSINE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Collections\n",
    "\n",
    "The `QdrantDocumentManager` class lets you list all **collections** in your `Qdrant` instance using the `get_collections` method. This retrieves and displays the **names** of all existing collections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection Name: test_collection\n",
      "Collection Name: sparse_collection\n",
      "Collection Name: dense_collection\n",
      "Collection Name: insta_image_search_test\n",
      "Collection Name: insta_image_search\n",
      "Collection Name: demo_collection\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the list of collections from the Qdrant client\n",
    "collections = db.client.get_collections()\n",
    "\n",
    "# Iterate over each collection and print its details\n",
    "for collection in collections.collections:\n",
    "    print(f\"Collection Name: {collection.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete a Collection\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to delete a **collection** using the `delete_collection` method. This method removes the specified collection from your `Qdrant` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'test_collection' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "# Define collection name\n",
    "collection_name = \"test_collection\"\n",
    "\n",
    "# Delete the collection\n",
    "if db.client.delete_collection(collection_name=collection_name):\n",
    "    print(f\"Collection '{collection_name}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage VectorStore\n",
    "\n",
    "After you've created your **vector store**, you can interact with it by **adding** or **deleting** items. Here are some common operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Items to the Vector Store\n",
    "\n",
    "The `QdrantDocumentManager` class lets you add items to your **vector store** using the `upsert` method. This method **updates** existing documents with new data if their IDs already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from uuid import uuid4\n",
    "\n",
    "# Load the text file\n",
    "loader = TextLoader(\"./data/the_little_prince.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600, chunk_overlap=100, length_function=len\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Generate unique IDs for documents\n",
    "uuids = [str(uuid4()) for _ in split_docs[:30]]\n",
    "page_contents = [doc.page_content for doc in split_docs[:30]]\n",
    "metadatas = [doc.metadata for doc in split_docs[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22417c4f-bf11-4e92-978a-6c436dec39ca',\n",
       " '28f56a01-34af-46ae-aeb4-ea6e0fcacb62',\n",
       " 'c6d06501-9595-4272-80b5-f0747cb145fc',\n",
       " 'b4b901bf-6e83-4658-b5e9-a1d5a80c767d',\n",
       " '21b1b98d-0707-4128-a0bd-78c94db6cbf3',\n",
       " 'c49b5d7c-c330-4d59-9097-25c3c52510b9',\n",
       " '36ddc677-4fa9-47ee-b2e0-284bdb9062a1',\n",
       " '32fde659-84c6-4679-b4df-d4b1d11e645f',\n",
       " 'caf0b611-4a38-4a94-84a9-c3a98ac0b2a1',\n",
       " '0e655834-9a6c-48a8-8a3b-5d5e2b1d6c2c',\n",
       " '493aaa5c-b89d-429b-a425-57f20f3564ed',\n",
       " '6f7f0755-d226-4aec-a714-a53d7a705e51',\n",
       " '8b68a39b-f990-4ce1-9fbd-675f5103d3ff',\n",
       " '73ef217b-9114-48a4-a447-0deb916b3d5a',\n",
       " '63b99932-4e84-4cb2-a5ef-1d83fdbc4e6a',\n",
       " '45fd3628-ca2f-439d-97ba-cc34da564f36',\n",
       " '876f59dd-a9ae-4af7-84e8-5d8fe78cf7d3',\n",
       " '5aa82f42-534f-447f-94b5-9ed4f3571091',\n",
       " 'eb69cc2a-8899-4d9e-ad8f-adebea281ff0',\n",
       " '1defc340-16b4-4ee0-94de-0dabc23e5d07',\n",
       " '368d5f90-75d2-406c-8dd2-c7d8736b6944',\n",
       " '842812f6-ee9f-43ae-8f6d-53015a5e57af',\n",
       " '61031399-09ed-4c88-bc93-1018b942df71',\n",
       " 'a6ac25f2-2dd5-445f-95dd-6a4d9fc4081c',\n",
       " '08215031-2393-4d0c-82a2-53a6a90d169f',\n",
       " 'f41de48c-1e7d-4036-a75e-a10ac579081d',\n",
       " 'a2d6b6d1-5bbc-4f17-9b95-c917021614f0',\n",
       " '3603a2e7-6021-46c9-8f4c-d53056849c1a',\n",
       " 'e1fb95a1-7c1c-4aed-a628-b39e0907b744',\n",
       " '2a42fbb6-9450-4d86-a5f8-65f333c10d4c']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.qdrant import QdrantDocumentManager\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the collection name for storing documents\n",
    "collection_name = \"demo_collection\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Create an instance of QdrantDocumentManager with specified storage path\n",
    "db = QdrantDocumentManager(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=collection_name,\n",
    "    embedding=embedding,\n",
    ")\n",
    "\n",
    "db.upsert(texts=page_contents, metadatas=metadatas, ids=uuids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Items from the Vector Store\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to delete items from your **vector store** using the `delete` method. You can specify items to delete by providing **IDs** or **filters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_ids = [uuids[0]]\n",
    "\n",
    "db.delete(ids=delete_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsert Items to Vector Store (Parallel)\n",
    "\n",
    "The `QdrantDocumentManager` class supports **parallel upserts** using the `upsert_parallel` method. This efficiently **adds** or **updates** multiple items with unique **IDs**, **data**, and **metadata**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['286d99ae-019b-41ed-962a-c1a26bf41c4a',\n",
       " 'e17ce584-3576-45bb-8d82-36cfdd4c89d1',\n",
       " 'aed142fa-a13a-421f-9e60-ab1af13a8b15',\n",
       " '14337336-edb2-4ea1-880c-2f4613f1f999',\n",
       " '91d47b16-4a1f-4f1f-ba07-78f9b2db06d8',\n",
       " '6b58d2d9-1a4b-4e03-97fd-d584d502b606',\n",
       " 'e7b6f4b5-27e0-4787-a74c-b8d17a7038ea',\n",
       " '01579e1a-9935-443d-a7a5-b9ffdd1e07f9',\n",
       " '4d516f16-09cf-4b7e-8d65-455eced738e7',\n",
       " '7fd284a3-5f10-407f-a8fe-44a923263748',\n",
       " '55fae9b6-046a-4f09-9cf0-08568efde43c',\n",
       " 'b4386ade-1590-41fa-94e7-cc34d4f4c9da',\n",
       " 'd27d8f98-349a-4c45-9f82-31e983edfa8c',\n",
       " '20537c5d-80d1-4d72-8507-73fd21e3f11a',\n",
       " 'ae418ede-69f6-4703-9d9d-2e31d59441b2',\n",
       " '975d663d-f825-446d-9824-7997058ca24a',\n",
       " 'c8086e33-6345-4403-a98c-a4cd46375cd1',\n",
       " 'ec887b4f-eecf-4325-8117-293e6fd8dfd6',\n",
       " 'c5fa1381-e30d-47d8-aad3-d46cc8520953',\n",
       " '1b20e891-e44f-4640-ab24-03d692627265',\n",
       " '0d37a3dd-329f-4901-a828-71a704f7a35e',\n",
       " '170420dc-b02c-42f3-a36d-c56973784fb7',\n",
       " 'f11893c3-20c5-43e4-9c0f-905d91c7a668',\n",
       " '37327ff1-7f17-43b0-89ca-65ab69c14df6',\n",
       " '92a4e2ec-7418-4241-a1e3-3bf2668a9fd6',\n",
       " 'ea018faa-293f-4329-b8ae-92dc3fcdd909',\n",
       " '09c78d94-0b4c-41cc-b530-7504f3d62dc4',\n",
       " '907ad8d0-427d-4f29-b801-aea90a6a86aa',\n",
       " '86508b0c-4ff7-422f-b13e-1443e47ef5d3',\n",
       " 'b12e4c37-50a1-4257-80ae-de372a4a77ce']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate unique IDs for documents\n",
    "uuids = [str(uuid4()) for _ in split_docs[30:60]]\n",
    "page_contents = [doc.page_content for doc in split_docs[30:60]]\n",
    "metadatas = [doc.metadata for doc in split_docs[30:60]]\n",
    "\n",
    "db.upsert_parallel(\n",
    "    texts=page_contents,\n",
    "    metadatas=metadatas,\n",
    "    ids=uuids,\n",
    "    batch_size=32,\n",
    "    workers=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query VectorStore\n",
    "\n",
    "Once your **vector store** has been created and the relevant **documents** have been added, you will most likely wish to **query** it during the running of your `chain` or `agent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Directly\n",
    "\n",
    "The `QdrantDocumentManager` class allows direct **querying** using the `search` method. It performs **similarity searches** by converting queries into **vector embeddings** to find similar **documents**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "response = db.search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in response:\n",
    "    payload = res[\"payload\"]\n",
    "    print(f\"* {payload['page_content'][:200]}\\n [{payload['metadata']}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search with Score\n",
    "\n",
    "The `QdrantDocumentManager` class enables **similarity searches** with **scores** using the `search` method. This provides a **relevance score** for each **document** found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [SIM=0.527] for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* [SIM=0.527] for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n",
      "* [SIM=0.527] for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query to search in the database\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Perform the search with the specified query and number of results\n",
    "response = db.search(query=query, k=3)\n",
    "\n",
    "for res in response:\n",
    "    payload = res[\"payload\"]\n",
    "    score = res[\"score\"]\n",
    "    print(\n",
    "        f\"* [SIM={score:.3f}] {payload['page_content'][:200]}\\n [{payload['metadata']}]\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query by Turning into Retriever\n",
    "\n",
    "The `QdrantDocumentManager` class can transform the **vector store** into a `retriever`. This allows for easier **integration** into **workflows** or **chains**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'c49b5d7c-c330-4d59-9097-25c3c52510b9', '_collection_name': 'demo_collection'}]\n",
      "\n",
      "\n",
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': '9567e6cf-2f89-4c3b-8a41-7167770fbcd3', '_collection_name': 'demo_collection'}]\n",
      "\n",
      "\n",
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'e2a0d06a-9ccd-4e9e-8d4a-4e1292b6ccef', '_collection_name': 'demo_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# Initialize QdrantVectorStore with the client, collection name, and embedding\n",
    "vector_store = QdrantVectorStore(\n",
    "    client=db.client, collection_name=db.collection_name, embedding=db.embedding\n",
    ")\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Transform the vector store into a retriever with specific search parameters\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\": 0.3},\n",
    ")\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with Filtering\n",
    "\n",
    "The `QdrantDocumentManager` class allows **searching with filters** to retrieve records based on specific **metadata values**. This is done using the `scroll` method with a defined **filter query**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Record(id='09c78d94-0b4c-41cc-b530-7504f3d62dc4', payload={'page_content': '[ Chapter 7 ]\\n- the narrator learns about the secret of the little prince‘s life \\nOn the fifth day-- again, as always, it was thanks to the sheep-- the secret of the little prince‘s life was revealed to me. Abruptly, without anything to lead up to it, and as if the question had been born of long and silent meditation on his problem, he demanded: \\n\"A sheep-- if it eats little bushes, does it eat flowers, too?\"\\n\"A sheep,\" I answered, \"eats anything it finds in its reach.\"\\n\"Even flowers that have thorns?\"\\n\"Yes, even flowers that have thorns.\" \\n\"Then the thorns-- what use are they?\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='0e655834-9a6c-48a8-8a3b-5d5e2b1d6c2c', payload={'page_content': '[ Chapter 1 ]\\n- we are introduced to the narrator, a pilot, and his ideas about grown-ups\\nOnce when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest. It was a picture of a boa constrictor in the act of swallowing an animal. Here is a copy of the drawing. \\n(picture)\\nIn the book it said: \"Boa constrictors swallow their prey whole, without chewing it. After that they are not able to move, and they sleep through the six months that they need for digestion.\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='286d99ae-019b-41ed-962a-c1a26bf41c4a', payload={'page_content': '[ Chapter 4 ]\\n- the narrator speculates as to which asteroid from which the little prince came\\u3000\\u3000\\nI had thus learned a second fact of great importance: this was that the planet the little prince came from was scarcely any larger than a house!', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='45fd3628-ca2f-439d-97ba-cc34da564f36', payload={'page_content': '[ Chapter 2 ]\\n- the narrator crashes in the desert and makes the acquaintance of the little prince\\nSo I lived my life alone, without anyone that I could really talk to, until I had an accident with my plane in the Desert of Sahara, six years ago. Something was broken in my engine. And as I had with me neither a mechanic nor any passengers, I set myself to attempt the difficult repairs all alone. It was a question of life or death for me: I had scarcely enough drinking water to last a week.', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='d27d8f98-349a-4c45-9f82-31e983edfa8c', payload={'page_content': '[ Chapter 5 ]\\n- we are warned as to the dangers of the baobabs\\nAs each day passed I would learn, in our talk, something about the little prince‘s planet, his departure from it, his journey. The information would come very slowly, as it might chance to fall from his thoughts. It was in this way that I heard, on the third day, about the catastrophe of the baobabs.\\nThis time, once more, I had the sheep to thank for it. For the little prince asked me abruptly-- as if seized by a grave doubt-- \"It is true, isn‘t it, that sheep eat little bushes?\" \\n\"Yes, that is true.\" \\n\"Ah! I am glad!\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='f11893c3-20c5-43e4-9c0f-905d91c7a668', payload={'page_content': '[ Chapter 6 ]\\n- the little prince and the narrator talk about sunsets\\nOh, little prince! Bit by bit I came to understand the secrets of your sad little life... For a long time you had found your only entertainment in the quiet pleasure of looking at the sunset. I learned that new detail on the morning of the fourth day, when you said to me: \\n\"I am very fond of sunsets. Come, let us go look at a sunset now.\" \\n\"But we must wait,\" I said. \\n\"Wait? For what?\" \\n\"For the sunset. We must wait until it is time.\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None),\n",
       " Record(id='f41de48c-1e7d-4036-a75e-a10ac579081d', payload={'page_content': '[ Chapter 3 ]\\n- the narrator learns more about from where the little prince came\\nIt took me a long time to learn where he came from. The little prince, who asked me so many questions, never seemed to hear the ones I asked him. It was from words dropped by chance that, little by little, everything was revealed to me. \\nThe first time he saw my airplane, for instance (I shall not draw my airplane; that would be much too complicated for me), he asked me: \\n\"What is that object?\"\\n\"That is not an object. It flies. It is an airplane. It is my airplane.\"', 'metadata': {'source': './data/the_little_prince.txt'}}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "# Define a filter query to match documents containing the text \"Chapter\" in the page content\n",
    "filter_query = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"page_content\",\n",
    "            match=models.MatchText(text=\"Chapter\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve records from the collection that match the filter query\n",
    "db.scroll(\n",
    "    scroll_filter=filter_query,\n",
    "    k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete with Filtering\n",
    "\n",
    "The `QdrantDocumentManager` class allows you to **delete records** using **filters** based on specific **metadata values**. This is achieved with the `delete` method and a **filter query**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=31, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.http.models import Filter, FieldCondition, MatchText\n",
    "\n",
    "# Define a filter query to match documents containing the text \"Chapter\" in the page content\n",
    "filter_query = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"page_content\",\n",
    "            match=models.MatchText(text=\"Chapter\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Delete records from the collection that match the filter query\n",
    "db.client.delete(collection_name=db.collection_name, points_selector=filter_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering and Updating Records\n",
    "\n",
    "The `QdrantDocumentManager` class supports **filtering and updating records** based on specific **metadata values**. This is done by **retrieving records** with **filters** and **updating** them as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models\n",
    "\n",
    "# Define a filter query to match documents with a specific metadata source\n",
    "filter_query = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"metadata.source\",\n",
    "            match=models.MatchValue(value=\"./data/the_little_prince.txt\"),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve records matching the filter query, including their vectors\n",
    "response = db.scroll(scroll_filter=filter_query, k=10, with_vectors=True)\n",
    "new_source = \"the_little_prince.txt\"\n",
    "\n",
    "# Update the point IDs and set new metadata for the records\n",
    "for point in response:  # response[0] returns a list of points\n",
    "    payload = point.payload\n",
    "\n",
    "    # Check if metadata exists in the payload\n",
    "    if \"metadata\" in payload:\n",
    "        payload[\"metadata\"][\"source\"] = new_source\n",
    "    else:\n",
    "        payload[\"metadata\"] = {\n",
    "            \"source\": new_source\n",
    "        }  # Add new metadata if it doesn't exist\n",
    "\n",
    "    # Update the point with new metadata\n",
    "    db.client.upsert(\n",
    "        collection_name=db.collection_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=point.id,\n",
    "                payload=payload,\n",
    "                vector=point.vector,\n",
    "            )\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search Options\n",
    "\n",
    "When using `QdrantVectorStore`, you have three options for performing **similarity searches**. You can select the desired search mode using the `retrieval_mode` parameter when you set up the class. The available modes are:\n",
    "\n",
    "- **Dense Vector Search** (Default)\n",
    "- **Sparse Vector Search**\n",
    "- **Hybrid Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Vector Search\n",
    "\n",
    "To perform a search using only **dense vectors**:\n",
    "\n",
    "- The `retrieval_mode` parameter must be set to `RetrievalMode.DENSE`. This is also the **default setting**.\n",
    "- You need to provide a [dense embeddings](https://python.langchain.com/docs/integrations/text_embedding/) value through the `embedding` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': '3cc041d5-2700-498f-8114-85f3c96e26b9', '_collection_name': 'dense_collection'}]\n",
      "\n",
      "\n",
      "* for decades. In the book, a pilot is stranded in the midst of the Sahara where he meets a tiny prince from another world traveling the universe in order to understand life. In the book, the little pri\n",
      " [{'source': './data/the_little_prince.txt', '_id': '24d766ea-3383-40e5-bd0e-051d51de88a3', '_collection_name': 'dense_collection'}]\n",
      "\n",
      "\n",
      "* Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets-- good plants and bad plants. In consequence, there were good seeds from good plants, and bad seeds fro\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'd25ba992-e54d-4e8a-9572-438c78d0288b', '_collection_name': 'dense_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Initialize QdrantVectorStore with documents, embeddings, and configuration\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=split_docs[:50],\n",
    "    embedding=embedding,\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=\"dense_collection\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "# Perform similarity search in the vector store\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Vector Search\n",
    "\n",
    "To search with only **sparse vectors**:\n",
    "\n",
    "- The `retrieval_mode` parameter should be set to `RetrievalMode.SPARSE`.\n",
    "- An implementation of the [SparseEmbeddings](https://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/langchain_qdrant/sparse_embeddings.py) interface using any **sparse embeddings provider** has to be provided as a value to the `sparse_embedding` parameter.\n",
    "- The `langchain-qdrant` package provides a **FastEmbed** based implementation out of the box.\n",
    "\n",
    "To use it, install the [FastEmbed](https://github.com/qdrant/fastembed) package:\n",
    "\n",
    "```bash\n",
    "pip install fastembed\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* [ Chapter 20 ]\n",
      "- the little prince discovers a garden of roses\n",
      "But it happened that after walking for a long time through sand, and rocks, and snow, the little prince at last came upon a road. And all\n",
      " [{'source': './data/the_little_prince.txt', '_id': '30d70339-4233-427b-b839-208c7618ae82', '_collection_name': 'sparse_collection'}]\n",
      "\n",
      "\n",
      "* [ Chapter 20 ]\n",
      "- the little prince discovers a garden of roses\n",
      "But it happened that after walking for a long time through sand, and rocks, and snow, the little prince at last came upon a road. And all\n",
      " [{'source': './data/the_little_prince.txt', '_id': '45ad1b0e-45cd-46f0-b6cd-d8e2b19ea8fa', '_collection_name': 'sparse_collection'}]\n",
      "\n",
      "\n",
      "* And he went back to meet the fox. \n",
      "\"Goodbye,\" he said. \n",
      "\"Goodbye,\" said the fox. \"And now here is my secret, a very simple secret: It is only with the heart that one can see rightly; what is essential\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'ab098119-c45f-4e33-b105-a6c6e01a918b', '_collection_name': 'sparse_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# Initialize sparse embeddings using FastEmbedSparse\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# Initialize QdrantVectorStore with documents, embeddings, and configuration\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=\"sparse_collection\",\n",
    "    retrieval_mode=RetrievalMode.SPARSE,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "# Perform similarity search in the vector store\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Vector Search\n",
    "\n",
    "To perform a **hybrid search** using **dense** and **sparse vectors** with **score fusion**:\n",
    "\n",
    "- The `retrieval_mode` parameter should be set to `RetrievalMode.HYBRID`.\n",
    "- A [`dense embeddings`](https://python.langchain.com/docs/integrations/text_embedding/) value should be provided to the `embedding` parameter.\n",
    "- An implementation of the [`SparseEmbeddings`](https://github.com/langchain-ai/langchain/blob/master/libs/partners/qdrant/langchain_qdrant/sparse_embeddings.py) interface using any **sparse embeddings provider** has to be provided as a value to the `sparse_embedding` parameter.\n",
    "\n",
    "**Note**: If you've added documents with the `HYBRID` mode, you can switch to any **retrieval mode** when searching, since both the **dense** and **sparse vectors** are available in the **collection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* \"Go and look again at the roses. You will understand now that yours is unique in all the world. Then come back to say goodbye to me, and I will make you a present of a secret.\" \n",
      "The little prince went\n",
      " [{'source': './data/the_little_prince.txt', '_id': '447a916c-d8a9-46f2-b035-d0ac4c7ea901', '_collection_name': 'hybrid_collection'}]\n",
      "\n",
      "\n",
      "* [ Chapter 20 ]\n",
      "- the little prince discovers a garden of roses\n",
      "But it happened that after walking for a long time through sand, and rocks, and snow, the little prince at last came upon a road. And all\n",
      " [{'source': './data/the_little_prince.txt', '_id': '894a9222-ef0c-4e28-b736-8a334cbdc83b', '_collection_name': 'hybrid_collection'}]\n",
      "\n",
      "\n",
      "* [ Chapter 8 ]\n",
      "- the rose arrives at the little prince‘s planet\n",
      " [{'source': './data/the_little_prince.txt', '_id': 'a3729fa0-b734-4316-ad18-83ea16263a2f', '_collection_name': 'hybrid_collection'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_qdrant import FastEmbedSparse, RetrievalMode\n",
    "from langchain_qdrant import RetrievalMode\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "query = \"What is the significance of the rose in The Little Prince?\"\n",
    "\n",
    "# Initialize the embedding model with a specific OpenAI model\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "# Initialize sparse embeddings using FastEmbedSparse\n",
    "sparse_embeddings = FastEmbedSparse(model_name=\"Qdrant/bm25\")\n",
    "\n",
    "# Initialize QdrantVectorStore with documents, embeddings, and configuration\n",
    "vector_store = QdrantVectorStore.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding,\n",
    "    sparse_embedding=sparse_embeddings,\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    collection_name=\"hybrid_collection\",\n",
    "    retrieval_mode=RetrievalMode.HYBRID,\n",
    "    batch_size=10,\n",
    ")\n",
    "\n",
    "# Perform similarity search in the vector store\n",
    "results = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content[:200]}\\n [{res.metadata}]\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-6aJyhYW2-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

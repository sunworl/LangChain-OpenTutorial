{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e00f269d",
      "metadata": {},
      "source": [
        "# SemanticChunker\n",
        "\n",
        "- Author: [Wonyoung Lee](https://github.com/BaBetterB)\n",
        "- Design: []()\n",
        "- Peer Review : [Wooseok Jeong](https://github.com/jeong-wooseok), [sohyunwriter](https://github.com/sohyunwriter)\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/07-TextSplitter/04-SemanticChunker.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial dives into a Text Splitter that uses semantic similarity to split text.\n",
        "\n",
        "LangChain's `SemanticChunker` is a powerful tool that takes document chunking to a whole new level. Unlike traiditional methods that split text at fixed intervals, the `SemanticChunker` analyzes the meaning of the content to create more logical divisions.\n",
        "\n",
        "This approach relies on **OpenAI's embedding model** , calculating how similar different pieces of text are by converting them into numerical representations. The tool offers various splitting options to suit your needs. You can choose from methods based on percentiles, standard deviation, or interquartile range.\n",
        "\n",
        "What sets the `SemanticChunker` apart is its ability to preserve context by identifying natural breaks. This ultimately leads to better performance when working with large language models. \n",
        "\n",
        "Since the `SemanticChunker` understands the actual content, it generates chunks that are more useful and maintain the flow and context of the original document.\n",
        "\n",
        "See [Greg Kamradt's notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
        "\n",
        "\n",
        "The method breaks down the text into individual sentences first. Then, it groups sementically similar sentences into chunks (e.g., 3 sentences), and finally merges similar sentences in the embedding space.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [Creating a Semantic Chunker](#creating-a-semanticchunker)\n",
        "- [Text Splitting](#text-splitting)\n",
        "- [Breakpoints](#breakpoints)\n",
        "\n",
        "### References\n",
        "\n",
        "- [Greg Kamradt's notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n",
        "- [Greg Kamradt's video](https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580)\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68b06978",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90b48cce",
      "metadata": {},
      "source": [
        "Load sample text and output the content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3a8858",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e18ec26d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain_experimental\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a1683c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"SemanticChunker\",  # title\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54038451",
      "metadata": {},
      "source": [
        "Alternatively, you can set and load `OPENAI_API_KEY` from a `.env` file.\n",
        "\n",
        "**[Note]** This is only necessary if you haven't already set `OPENAI_API_KEY` in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295e7ad6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration File for Managing API Keys as Environment Variables\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load API Key Information\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3926396",
      "metadata": {},
      "source": [
        "Load the sample text and output its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c170dd43",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Open the data/appendix-keywords.txt file to create a file object called f.\n",
        "with open(\"./data/appendix-keywords.txt\", encoding=\"utf-8\") as f:\n",
        "\n",
        "    file = f.read()  # Read the contents of the file and save it in the file variable.\n",
        "\n",
        "# Print part of the content read from the file.\n",
        "print(file[:350])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d165846",
      "metadata": {},
      "source": [
        "## Creating a `SemanticChunker`\n",
        "\n",
        "The `SemanticChunker` is an experimental LangChain feature, that splits text into semantically similar chunks.\n",
        "\n",
        "This approach allows for more effective processing and analysis of text data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339f032d",
      "metadata": {},
      "source": [
        "Use the `SemanticChunker` to divide the text into semantically related chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "312e3aae",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# Initialize a semantic chunk splitter using OpenAI embeddings.\n",
        "text_splitter = SemanticChunker(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab515b0",
      "metadata": {},
      "source": [
        "## Text Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c9b20b",
      "metadata": {},
      "source": [
        "Use the `text_splitter` with your loaded file (`file`) to split the text into smallar, more manageable unit documents. This process is often referred to as chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb5870d",
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = text_splitter.split_text(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14a777bc",
      "metadata": {},
      "source": [
        "After splitting, you can examine the resulting chunks to see how the text has been divided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec69bff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the first chunk among the divided chunks.\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f03b26b",
      "metadata": {},
      "source": [
        "The `create_documents()` function allows you to convert the individual chunks ([`file`]) into proper document objects (`docs`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadaf823",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split using text_splitter\n",
        "docs = text_splitter.create_documents([file])\n",
        "print(\n",
        "    docs[0].page_content\n",
        ")  # Print the content of the first document among the divided documents."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1633cf8e",
      "metadata": {},
      "source": [
        "## Breakpoints\n",
        "\n",
        "This chunking process works by indentifying natural breaks between sentences.\n",
        "\n",
        "Here's how it decides where to split the text:\n",
        "1. It calculates the difference between these embeddings for each pair of sentences.\n",
        "2. When the difference between two sentences exceeds a certain threshold (breakpoint), the `text_splitter` identifies this as a natural break and splits the text at that point.\n",
        "\n",
        "Check out [Greg Kamradt's video](https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580) for more details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f295b680",
      "metadata": {},
      "source": [
        "### Percentile-Based Splitting\n",
        "\n",
        "This method sorts all embedding differences between sentences. Then, it splits the text at a specific percentile (e.g. 70th percentile)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "744bbd95",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(\n",
        "    # Initialize the semantic chunker using OpenAI's embedding model\n",
        "    OpenAIEmbeddings(),\n",
        "    # Set the split breakpoint type to percentile\n",
        "    breakpoint_threshold_type=\"percentile\",\n",
        "    breakpoint_threshold_amount=70,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59aa8318",
      "metadata": {},
      "source": [
        "Examine the resulting document list (`docs`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7b3262",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = text_splitter.create_documents([file])\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
        "    print(\n",
        "        doc.page_content\n",
        "    )  # Print the content of the first document among the split documents.\n",
        "    print(\"===\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e83f74",
      "metadata": {},
      "source": [
        "Use the `len(docs)` function to get the number of chunks created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c0cbd0",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(docs))  # Print the length of docs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c1c9e8",
      "metadata": {},
      "source": [
        "### Standard Deviation Splitting\n",
        "\n",
        "This method sets a threshold based on a specified number of standard deviations (`breakpoint_threshold_amount`).\n",
        "\n",
        "To use standard deviation for your breakpoints, set the `breakpoint_threshold_type` parameter to `\"standard_deviation\"` when initializing the `text_splitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "16a8d823",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(\n",
        "    # Initialize the semantic chunker using OpenAI's embedding model.\n",
        "    OpenAIEmbeddings(),\n",
        "    # Use standard deviation as the splitting criterion.\n",
        "    breakpoint_threshold_type=\"standard_deviation\",\n",
        "    breakpoint_threshold_amount=1.25,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690db96c",
      "metadata": {},
      "source": [
        "After splitting, check the `docs` list and print its length (`len(docs)`) to see how many chunks were created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1764de39",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split using text_splitter.\n",
        "docs = text_splitter.create_documents([file])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0743d8f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = text_splitter.create_documents([file])\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
        "    print(\n",
        "        doc.page_content\n",
        "    )  # Print the content of the first document among the split documents.\n",
        "    print(\"===\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee9f46ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(docs))  # Print the length of docs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5b03d9b",
      "metadata": {},
      "source": [
        "### Interquartile Range Splitting\n",
        "\n",
        "This method utilizes the interquartile range (IQR) of the embedding differences to consider breaks, leading to a text split."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb408177",
      "metadata": {},
      "source": [
        "Set the `breakpoint_threshold_type` parameter to `\"interquartile\"` when initializing the `text_splitter` to use the IQR for splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f32f5fe8",
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(\n",
        "    # Initialize the semantic chunk splitter using OpenAI's embedding model.\n",
        "    OpenAIEmbeddings(),\n",
        "    # Set the breakpoint threshold type to interquartile range.\n",
        "    breakpoint_threshold_type=\"interquartile\",\n",
        "    breakpoint_threshold_amount=0.5,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e0d2d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split using text_splitter.\n",
        "docs = text_splitter.create_documents([file])\n",
        "\n",
        "# Print the results.\n",
        "for i, doc in enumerate(docs[:5]):\n",
        "    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n",
        "    print(\n",
        "        doc.page_content\n",
        "    )  # Print the content of the first document among the split documents.\n",
        "    print(\"===\" * 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d186bb7",
      "metadata": {},
      "source": [
        "Finally, print the length of `docs` list (`len(docs)`) to view the number of cunks created.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c693c11",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(docs))  # Print the length of docs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-9y5W8e20-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

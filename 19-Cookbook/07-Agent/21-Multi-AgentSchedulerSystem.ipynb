{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Scheduler System\n",
    "\n",
    "- Author: [Ilgyun Jeong](https://github.com/johnny9210)\n",
    "- Design: \n",
    "- Peer Review: [Mark()](https://github.com/obov), [Taylor(Jihyun Kim)](https://github.com/Taylor0819)\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/19-Cookbook/03-MultiAgentSystem/01-MultiAgentScheduler.ipynb)\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "The Multi-Agent Scheduler System represents an innovative approach to automating information retrieval and delivery through a coordinated network of specialized AI agents. At its core, this system transforms simple natural language requests into scheduled, automated search and delivery operations, making it particularly valuable for researchers, analysts, and anyone needing regular, scheduled information updates.\n",
    "\n",
    "Imagine asking \"Find me the latest RAG papers at 7 AM tomorrow.\" Instead of manually searching and compiling information early in the morning, the system automatically handles the entire process - from understanding your request to delivering a well-formatted email with relevant research papers at precisely 7 AM. This automation eliminates the need for manual intervention while ensuring timely delivery of crucial information.\n",
    "\n",
    "### System Architecture\n",
    "\n",
    "The system's architecture is built around five specialized agents, each handling a crucial aspect of the information retrieval and delivery process:\n",
    "\n",
    "1. `Query Analysis Agent`\n",
    "   This agent serves as the system's front door, interpreting natural language queries to extract critical information \n",
    "\n",
    "2. `Search Router`\n",
    "   Acting as the system's traffic controller, the Search Router directs queries to the most appropriate specialized search agent:\n",
    "   \n",
    "3. `Response Agent`\n",
    "   This agent transforms raw search results into well-structured, readable content by:\n",
    "\n",
    "4. `Scheduling System and Email Service`\n",
    "   The scheduling component manages the temporal aspects of the system:\n",
    "   This ensures that all operations occur at their specified times without conflicts.\n",
    "   The system implements a robust email delivery service using yagmail that provides:\n",
    "\n",
    "### System Flow\n",
    "\n",
    "The entire process follows this sequence:\n",
    "\n",
    "![Multi-Agent Scheduler System Flow](assets/21-Multi-AgentSchedulerSystem.png)\n",
    "\n",
    "This architecture ensures reliable, automated information retrieval and delivery, with each agent optimized for its specific role in the process.\n",
    "\n",
    "### Table of Contents\n",
    "- [Overview](#overview)\n",
    "- [System Architecture](#system-architecture)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Query Analysis Agent](#query-analysis-agent)\n",
    "- [Search Router and Specialized Agents](#Search-Router-and-Specialized-Agents)\n",
    "- [Response Agent](#response-agent)\n",
    "- [Scheduling System and Email Service](#Scheduling-System-and-Email-Service)  \n",
    "\n",
    "The system's modular design allows for easy expansion and customization, making it adaptable to various use cases while maintaining consistent performance and reliability. Whether you're tracking research papers, monitoring news, or gathering general information, the Multi-Agent Scheduler System automates the entire process from query to delivery, saving time and ensuring consistent, timely access to important information.\n",
    "\n",
    "### References\n",
    "- [How to get NewsAPI](https://newsapi.org)\n",
    "- [How to get SerpAPI](https://serpapi.com/)\n",
    "- [How to get Google password](https://support.google.com/accounts/answer/185833?visit_id=638745290390245053-2925662375&p=InvalidSecondFactor&rd=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials.\n",
    "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langsmith\",\n",
    "        \"langchain\",\n",
    "        \"chromadb\",\n",
    "        \"langchain_chroma\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"pytz\",\n",
    "        \"google-search-results\",\n",
    "        \"yagmail\",\n",
    "        \"schedule\",\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        \"OPENAI_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"21-Multi-AgentSchedulerSystem\",\n",
    "        \"NEWS_API_KEY\": \"\",\n",
    "        \"SERPAPI_API_KEY\": \"\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Analysis Agent\n",
    "The QueryAnalysisAgent serves as the initial interpreter in our multi-agent scheduler system, transforming natural language queries into structured data that other agents can process. This agent employs advanced language understanding capabilities through GPT-4 to accurately parse user intentions and timing requirements.\n",
    "\n",
    "### Core Components\n",
    "The agent is built around three essential classes:\n",
    "- Time Extraction Processor: Handles temporal information\n",
    "- Task Analysis Engine: Understands search requirements\n",
    "- Query Coordinator: Combines and validates results\n",
    "\n",
    "\n",
    "### Core Functionality\n",
    "The agent performs two primary functions:\n",
    "\n",
    "1. Time Extraction\n",
    "```python\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"Extracts time information from queries\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "    # Converts to standardized datetime format\n",
    "    return self._process_time(time_str)\n",
    "```\n",
    "\n",
    "2. Task Analysis\n",
    "```python\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"Analyzes query content for search parameters\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "    return self._parse_response(response)\n",
    "```\n",
    "\n",
    "### Usage Example\n",
    "\n",
    "```python\n",
    "# Initialize agent\n",
    "agent = QueryAnalysisAgent()\n",
    "\n",
    "# Process a sample query\n",
    "query = \"Find RAG papers tomorrow at 9 AM\"  \n",
    "result = agent.analyze_query(query)\n",
    "\n",
    "Expected output:\n",
    "```json\n",
    "{\n",
    "    \"target_time\": \"2025-02-06 09:00:00+0000\",\n",
    "    \"execution_time\": \"2025-02-06 08:55:00+0000\",\n",
    "    \"task_type\": \"search\",\n",
    "    \"search_type\": \"research_paper\",\n",
    "    \"keywords\": [\"RAG\", \"papers\"],  \n",
    "    \"requirements\": \"minimum 5 results\",\n",
    "    \"time_sensitivity\": \"normal\",\n",
    "    \"original_query\": \"Find RAG papers tomorrow at 9 AM\", \n",
    "    \"status\": \"success\"\n",
    "}\n",
    "```\n",
    "\n",
    "This structured approach ensures reliable query interpretation while maintaining flexibility for various query types and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Importing Required Libraries\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QueryAnalysisAgent class represents a specialized natural language query processor that utilizes OpenAI's language models. Let's break down its core components:\n",
    "The initialization method sets up the language model with temperature=0 to ensure consistent, deterministic responses:\n",
    "\n",
    "The setup_prompt_templates method defines two essential templates:\n",
    "\n",
    "1. Time Extraction Template\n",
    "This template focuses solely on extracting and standardizing time information from queries.\n",
    "\n",
    "2. Task Analysis Template\n",
    "This template structures the query analysis with specific rules:\n",
    "\n",
    "- Categorizes searches into three types: research_paper, news, or general\n",
    "- Distinguishes between normal and urgent time sensitivity\n",
    "- Separates search keywords from temporal terms\n",
    "- Maintains consistent task typing as \"search\"\n",
    "\n",
    "These templates work together to transform natural language queries into structured, actionable data that the rest of the system can process efficiently. The clear separation between time extraction and task analysis allows for precise handling of each aspect of the query.\n",
    "\n",
    "For example, a query like \"Find RAG papers at 7 AM\" would be processed to extract both the time (07:00) and the search parameters (research papers about RAG), while filtering out temporal terms from the actual search keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Class Definition and __init__, setup_prompt_templates Methods\n",
    "class QueryAnalysisAgent:\n",
    "    def __init__(self, model_name=\"gpt-4o\"):\n",
    "        self.llm = ChatOpenAI(model_name=model_name, temperature=0)\n",
    "        self.setup_prompt_templates()\n",
    "\n",
    "    def setup_prompt_templates(self):\n",
    "        self.time_extraction_prompt = PromptTemplate.from_template(\n",
    "            \"Extract time and convert to 24h format from: {query}\\nReturn HH:MM only\"\n",
    "        )\n",
    "\n",
    "        self.task_analysis_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Analyze the query and return only a JSON object. For the query: {query}\n",
    "\n",
    "        Return this exact format:\n",
    "        {{\n",
    "            \"task_type\": \"search\",\n",
    "            \"search_type\": \"research_paper\",\n",
    "            \"keywords\": [\"rag\", \"papers\"],\n",
    "            \"requirements\": \"minimum 5 results\",\n",
    "            \"time_sensitivity\": \"normal\",\n",
    "            \"search_terms\": [\"rag\", \"papers\"]  # Actual keywords to use for search\n",
    "        }}\n",
    "\n",
    "        Rules:\n",
    "        - search_type must be one of: \"research_paper\", \"news\", \"general\"\n",
    "        - time_sensitivity must be one of: \"normal\", \"urgent\"\n",
    "        - keywords should include all words from query including time-related terms\n",
    "        - search_terms should exclude time-related terms and only include actual search keywords\n",
    "        - task_type should always be \"search\"\n",
    "        \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 1\n",
    "\n",
    "`extract_time()`\n",
    "- Functionality: Extracts and processes time information from natural language queries\n",
    "- Features:\n",
    "  - Converts various time formats (e.g., \"morning 7\", \"afternoon 3:30\") to standardized datetime objects\n",
    "  - Maintains timezone awareness using pytz for accurate scheduling\n",
    "  - Automatically schedules for next day if requested time has already passed\n",
    "  - Strips unnecessary time components (seconds, microseconds) for cleaner scheduling\n",
    "- Error Handling: Raises ValueError with detailed error messages for invalid time formats\n",
    "- Returns: UTC-aware datetime object representing the target execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Adding extract_time Method\n",
    "def extract_time(self, query: str) -> datetime:\n",
    "    \"\"\"Extracts time information from the query and returns a datetime object.\"\"\"\n",
    "    time_extraction_chain = self.time_extraction_prompt | self.llm\n",
    "    time_str = time_extraction_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # Extract the actual time string from the ChatCompletion response\n",
    "        time_str = time_str.content.strip()\n",
    "\n",
    "        # Calculate the next scheduled time based on the current time\n",
    "        current_time = datetime.now(pytz.utc)\n",
    "        hour, minute = map(int, time_str.split(\":\"))\n",
    "\n",
    "        target_time = current_time.replace(\n",
    "            hour=hour, minute=minute, second=0, microsecond=0\n",
    "        )\n",
    "\n",
    "        # If the extracted time has already passed, set it for the next day\n",
    "        if target_time <= current_time:\n",
    "            target_time += timedelta(days=1)\n",
    "\n",
    "        return target_time\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Time extraction failed: {e}\")\n",
    "\n",
    "\n",
    "# After executing this cell, the method should be added to the class\n",
    "QueryAnalysisAgent.extract_time = extract_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 2\n",
    "\n",
    "`analyze_task()`\n",
    "- Functionality: Breaks down queries into structured task components\n",
    "- Features:\n",
    "  - Identifies search type (research_paper, news, general)\n",
    "  - Extracts relevant keywords while filtering temporal terms\n",
    "  - Determines task urgency (normal vs urgent)\n",
    "  - Identifies specific requirements (e.g., minimum result count)\n",
    "- Error Handling: Handles JSON parsing errors and invalid query formats\n",
    "- Returns: Dictionary containing parsed task information and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Adding analyze_task Method\n",
    "def analyze_task(self, query: str) -> dict:\n",
    "    \"\"\"Extracts task intent and keywords from the query.\"\"\"\n",
    "    task_analysis_chain = self.task_analysis_prompt | self.llm\n",
    "    response = task_analysis_chain.invoke({\"query\": query})\n",
    "\n",
    "    try:\n",
    "        # Clean response content to ensure valid JSON format\n",
    "        content = response.content.strip()\n",
    "        content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "        return json.loads(content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Original response: {response.content}\")\n",
    "        raise ValueError(f\"Failed to parse task analysis result: {e}\")\n",
    "\n",
    "\n",
    "# Adding the method to the class\n",
    "QueryAnalysisAgent.analyze_task = analyze_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 3\n",
    "\n",
    "`analyze_query()`\n",
    "- Functionality: Combines time extraction and task analysis into a complete query interpretation\n",
    "- Features:\n",
    "  - Coordinates between time extraction and task analysis\n",
    "  - Sets execution time 5 minutes before target time\n",
    "  - Validates and combines all query components\n",
    "- Error Handling: Catches and reports errors from both time and task processing\n",
    "- Returns: Combined dictionary with timing and task information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(self, query: str) -> dict:\n",
    "    \"\"\"Performs a full query analysis and returns the results.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the analysis results.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract time information\n",
    "        target_time = self.extract_time(query)\n",
    "\n",
    "        # Analyze task information\n",
    "        task_info = self.analyze_task(query)\n",
    "\n",
    "        # Return the results including all necessary details\n",
    "        return {\n",
    "            \"target_time\": target_time,\n",
    "            \"execution_time\": target_time - timedelta(minutes=5),\n",
    "            \"task_type\": task_info[\"task_type\"],\n",
    "            \"search_type\": task_info[\"search_type\"],  # Newly added\n",
    "            \"keywords\": task_info[\"keywords\"],\n",
    "            \"requirements\": task_info[\"requirements\"],\n",
    "            \"time_sensitivity\": task_info[\"time_sensitivity\"],  # Newly added\n",
    "            \"original_query\": query,\n",
    "            \"status\": \"success\",\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error_message\": str(e), \"original_query\": query}\n",
    "\n",
    "\n",
    "# Adding the method to the class\n",
    "QueryAnalysisAgent.analyze_query = analyze_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Methods 4\n",
    "\n",
    "`datetime_handler(obj)`\n",
    "* Functionality: Converts datetime objects to JSON-serializable string format\n",
    "* Features:\n",
    "   * Accepts any object and checks if it's a datetime instance\n",
    "   * Converts datetime to standardized string format (YYYY-MM-DD HH:MM:SS+ZZZZ)\n",
    "   * Maintains timezone information in the output string\n",
    "* Error Handling: Raises TypeError with descriptive message for non-datetime objects\n",
    "* Returns: String representation of datetime in consistent format\n",
    "* Use Cases:\n",
    "   * JSON serialization for API responses\n",
    "   * Database storage of temporal data\n",
    "   * Logging and debugging timestamp formatting\n",
    "* Examples:\n",
    "   * Input: `datetime(2024, 2, 6, 15, 30, tzinfo=pytz.UTC)`\n",
    "   * Output: `\"2024-02-06 15:30:00+0000\"`\n",
    "\n",
    "The function serves as a critical utility for converting Python's datetime objects into a standardized string format that can be easily stored, transmitted, and later reconstructed. This is particularly important in our scheduling system where accurate time representation and timezone awareness are essential for reliable task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Testing QueryAnalysisAgent\n",
    "def datetime_handler(obj):\n",
    "    \"\"\"Handler to convert datetime objects into JSON serializable strings.\n",
    "\n",
    "    Args:\n",
    "        obj: The object to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted datetime string (Format: YYYY-MM-DD HH:MM:SS+ZZZZ).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Raised if the object is not a datetime instance.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QueryAnalysisAgent Test Result ===\n",
      "{\n",
      "  \"target_time\": \"2025-02-08 07:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-08 06:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"news\",\n",
      "  \"keywords\": [\n",
      "    \"rag\",\n",
      "    \"7 AM\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"Find and recommend news related to RAG at 7 AM.\",\n",
      "  \"status\": \"success\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def run_query_test():\n",
    "    \"\"\"Runs a test for QueryAnalysisAgent and returns the results.\n",
    "\n",
    "    Returns:\n",
    "        str: The analysis result in JSON format.\n",
    "    \"\"\"\n",
    "    # Create an instance of QueryAnalysisAgent\n",
    "    agent = QueryAnalysisAgent()\n",
    "\n",
    "    # Test query\n",
    "    test_query = \"Find and recommend news related to RAG at 7 AM.\"\n",
    "\n",
    "    # Execute query analysis\n",
    "    result = agent.analyze_query(test_query)\n",
    "\n",
    "    # Convert the result to JSON format\n",
    "    return json.dumps(result, indent=2, ensure_ascii=False, default=datetime_handler)\n",
    "\n",
    "\n",
    "# Execute test and print results\n",
    "if __name__ == \"__main__\":\n",
    "    test_result = run_query_test()\n",
    "    print(\"\\n=== QueryAnalysisAgent Test Result ===\")\n",
    "    print(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Router and Specialized Agents\n",
    "\n",
    "The Search Router system acts as an intelligent traffic controller, directing queries to the most appropriate specialized search agent based on the query analysis. This architecture ensures that each type of search request is handled by an agent specifically optimized for that domain.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "```python\n",
    "class SearchRouter:\n",
    "    def __init__(self):\n",
    "        # Initialize specialized search agents\n",
    "        self.paper_search_agent = PaperSearchAgent()\n",
    "        self.news_search_agent = NewsSearchAgent()\n",
    "        self.general_search_agent = GeneralSearchAgent()\n",
    "```\n",
    "Each specialized agent is designed to handle specific types of searches:\n",
    "\n",
    "1. `Paper Search Agent`\n",
    "This agent specializes in academic paper searches, interfacing with arXiv's API to retrieve scholarly articles and research papers.\n",
    "\n",
    "2. `News Search Agent`\n",
    "This agent handles news-related searches, connecting to NewsAPI to gather current events and news articles.\n",
    "\n",
    "3. `General Search Agent`\n",
    "This agent manages general web searches using SerpAPI, handling broader information gathering needs.\n",
    "\n",
    "This routing system ensures that each query is handled by the most appropriate agent while maintaining consistent error handling and result formatting across all search types. The modular design allows for easy addition of new specialized agents as needed, making the system highly extensible and maintainable.\n",
    "\n",
    "Each agent provides standardized outputs despite their different data sources and search methodologies, enabling seamless integration with the rest of the system components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PaperSearchAgent`\n",
    "\n",
    "This agent focuses on academic content retrieval. It interfaces with the arXiv API to fetch scholarly papers and research documents. Key features include filtering papers by relevance, date ranges, and processing XML responses into structured data. The agent is particularly useful for researchers and academics needing current papers in their field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class PaperSearchAgent:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    def perform_search(self, query_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        try:\n",
    "            keywords = self._process_keywords(query_info[\"keywords\"])\n",
    "            max_results = self._extract_max_results(query_info.get(\"requirements\", \"\"))\n",
    "\n",
    "            url = f\"{self.base_url}?search_query=all:{keywords}&start=0&max_results={max_results}\"\n",
    "            response = urllib.request.urlopen(url)\n",
    "            data = response.read().decode(\"utf-8\")\n",
    "\n",
    "            results = self._parse_arxiv_results(data)\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": results,\n",
    "                \"total_found\": len(results),\n",
    "                \"returned_count\": len(results),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def _process_keywords(self, keywords: List[str]) -> str:\n",
    "        # Remove time-related keywords\n",
    "        filtered_keywords = [\n",
    "            k\n",
    "            for k in keywords\n",
    "            if not any(\n",
    "                time in k.lower()\n",
    "                for time in [\"hour\", \"morning\", \"afternoon\", \"evening\"]\n",
    "            )\n",
    "        ]\n",
    "        return \"+\".join(filtered_keywords)\n",
    "\n",
    "    def _extract_max_results(self, requirements: str) -> int:\n",
    "        import re\n",
    "\n",
    "        # extracting numbers\n",
    "        numbers = re.findall(r\"\\d+\", requirements)\n",
    "        return int(numbers[0]) if numbers else 5\n",
    "\n",
    "    def _parse_arxiv_results(self, data: str) -> List[Dict[str, Any]]:\n",
    "        root = ET.fromstring(data)\n",
    "        results = []\n",
    "\n",
    "        for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "            title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text\n",
    "            url = entry.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "            published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text\n",
    "            summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"type\": \"research_paper\",\n",
    "                    \"title\": title,\n",
    "                    \"url\": url,\n",
    "                    \"published_date\": published[:10],\n",
    "                    \"summary\": summary,\n",
    "                    \"source\": \"arxiv\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NewsSearchAgent`\n",
    "\n",
    "This agent handles current events and news article searches. It connects to NewsAPI to access a wide range of news sources. The agent supports features like language filtering, date range specification, and source selection. It's especially valuable for users needing real-time information or tracking specific topics in the news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import urlencode\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "class NewsSearchAgent:\n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initializes a news search agent using NewsAPI.\n",
    "\n",
    "        NewsAPI follows a REST API structure with the base URL 'https://newsapi.org/v2'.\n",
    "        It provides two main endpoints:\n",
    "        - /everything: Searches the entire news archive.\n",
    "        - /top-headlines: Retrieves the latest top headlines.\n",
    "        \"\"\"\n",
    "\n",
    "        self.api_key = os.environ[\"NEWS_API_KEY\"]\n",
    "\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"NewsAPI key is required\")\n",
    "\n",
    "        self.base_url = \"https://newsapi.org/v2\"\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Performs a news search based on the given query information.\n",
    "\n",
    "        Args:\n",
    "            query_info (Dict[str, Any]): Dictionary containing search parameters.\n",
    "            max_results (int): Maximum number of results to return.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing search results or an error message.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Extract actual search terms (excluding time-related keywords)\n",
    "            search_terms = query_info.get(\n",
    "                \"search_terms\", query_info.get(\"keywords\", [])\n",
    "            )\n",
    "\n",
    "            # Check if the search is for real-time news\n",
    "            is_realtime = query_info.get(\"time_sensitivity\") == \"urgent\"\n",
    "            from_date = datetime.now() - timedelta(hours=1 if is_realtime else 24)\n",
    "\n",
    "            # Configure parameters for the 'everything' endpoint\n",
    "            params = {\n",
    "                \"q\": \" \".join(search_terms),  # Exclude time-related keywords\n",
    "                \"from\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"sortBy\": \"publishedAt\",\n",
    "                \"language\": \"en\",\n",
    "                \"apiKey\": self.api_key,\n",
    "            }\n",
    "\n",
    "            # Construct API request URL\n",
    "            url = f\"{self.base_url}/everything?{urlencode(params)}\"\n",
    "\n",
    "            # Send API request\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "\n",
    "            # Check response status\n",
    "            if response.status_code != 200:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": data.get(\"message\", \"Unknown error\"),\n",
    "                    \"query_info\": query_info,\n",
    "                }\n",
    "\n",
    "            # Process and format results\n",
    "            articles = data.get(\"articles\", [])\n",
    "            formatted_results = []\n",
    "\n",
    "            for article in articles[:max_results]:\n",
    "                formatted_results.append(\n",
    "                    {\n",
    "                        \"title\": article.get(\"title\"),\n",
    "                        \"description\": article.get(\"description\"),\n",
    "                        \"url\": article.get(\"url\"),\n",
    "                        \"published_at\": article.get(\"publishedAt\"),\n",
    "                        \"source\": article.get(\"source\", {}).get(\"name\"),\n",
    "                        \"content\": article.get(\"content\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": formatted_results,\n",
    "                \"total_results\": data.get(\"totalResults\", 0),\n",
    "                \"returned_count\": len(formatted_results),\n",
    "                \"search_parameters\": {\n",
    "                    \"keywords\": query_info[\"keywords\"],\n",
    "                    \"from_date\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"language\": \"en\",\n",
    "                },\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def get_top_headlines(\n",
    "        self, country: str = \"us\", category: str = None\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Fetches top news headlines.\n",
    "\n",
    "        Args:\n",
    "            country (str): Country code (default: 'us' for the United States).\n",
    "            category (str, optional): News category (e.g., business, technology).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing top headlines.\n",
    "        \"\"\"\n",
    "        params = {\"country\": country, \"apiKey\": self.api_key}\n",
    "\n",
    "        if category:\n",
    "            params[\"category\"] = category\n",
    "\n",
    "        url = f\"{self.base_url}/top-headlines?{urlencode(params)}\"\n",
    "        response = requests.get(url)\n",
    "\n",
    "        return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GeneralSearchAgent`\n",
    "\n",
    "This agent manages broader web searches through SerpAPI. It handles diverse information needs that don't fit strictly into academic or news categories. The agent includes features like language-specific searches, result ranking, and content type filtering. It's particularly useful for general research, product information, or any broad information gathering needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "\n",
    "class GeneralSearchAgent:\n",
    "    def __init__(self, serpapi_key: str = None):\n",
    "        if serpapi_key:\n",
    "            os.environ[\"SERPAPI_API_KEY\"] = serpapi_key\n",
    "        self.search = SerpAPIWrapper()\n",
    "\n",
    "    def setup_search_parameters(self, query_info: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Constructs search queries for general search.\"\"\"\n",
    "        keywords = \" \".join(query_info[\"keywords\"])\n",
    "\n",
    "        # Set up base search queries\n",
    "        search_queries = [\n",
    "            f\"{keywords} lang:ko\",\n",
    "            keywords,\n",
    "        ]  # Korean results  # General search\n",
    "\n",
    "        return search_queries\n",
    "\n",
    "    def perform_search(\n",
    "        self, query_info: Dict[str, Any], max_results: int = 5\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Performs general search and returns results.\"\"\"\n",
    "        try:\n",
    "            search_queries = self.setup_search_parameters(query_info)\n",
    "            all_results = []\n",
    "\n",
    "            for query in search_queries:\n",
    "                raw_results = self.search.run(query)\n",
    "                parsed_results = self._parse_general_results(raw_results)\n",
    "                all_results.extend(parsed_results)\n",
    "\n",
    "            # Sort by relevance score\n",
    "            sorted_results = sorted(\n",
    "                all_results, key=lambda x: x.get(\"relevance_score\", 0), reverse=True\n",
    "            )[:max_results]\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"results\": sorted_results,\n",
    "                \"total_found\": len(all_results),\n",
    "                \"returned_count\": len(sorted_results),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"query_info\": query_info,\n",
    "            }\n",
    "\n",
    "    def _parse_general_results(self, raw_results: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Parses general search results.\"\"\"\n",
    "        parsed_results = []\n",
    "\n",
    "        for result in raw_results.split(\"\\n\"):\n",
    "            if not result.strip():\n",
    "                continue\n",
    "\n",
    "            parsed_results.append(\n",
    "                {\n",
    "                    \"type\": \"general\",\n",
    "                    \"title\": self._extract_title(result),\n",
    "                    \"content\": result,\n",
    "                    \"url\": self._extract_url(result),\n",
    "                    \"relevance_score\": self._calculate_relevance(result),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return parsed_results\n",
    "\n",
    "    def _extract_title(self, result: str) -> str:\n",
    "        \"\"\"Extracts title from the result.\"\"\"\n",
    "        return result.split(\".\")[0].strip()[:100]\n",
    "\n",
    "    def _extract_url(self, result: str) -> str:\n",
    "        \"\"\"Extracts URL from the result.\"\"\"\n",
    "        import re\n",
    "\n",
    "        urls = re.findall(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+[^\\s]*\", result)\n",
    "        return urls[0] if urls else \"\"\n",
    "\n",
    "    def _calculate_relevance(self, result: str) -> float:\n",
    "        \"\"\"Calculates relevance score for the search result.\"\"\"\n",
    "        relevance_score = 0.5  # Base score\n",
    "\n",
    "        # Calculate score based on keyword matching\n",
    "        keywords = [\"official\", \"guide\", \"tutorial\", \"review\", \"recommendation\"]\n",
    "        lower_result = result.lower()\n",
    "\n",
    "        for keyword in keywords:\n",
    "            if keyword in lower_result:\n",
    "                relevance_score += 0.1\n",
    "\n",
    "        return min(1.0, relevance_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SearchRouter`: The System's Traffic Controller\n",
    "\n",
    "The `SearchRouter` acts as the central coordinator for our multi-agent search system, intelligently directing queries to specialized search agents based on the type of information needed. Think of it as an expert traffic controller at a busy airport, making sure each \"flight\" (query) goes to the right \"runway\" (search agent).\n",
    "\n",
    "The `SearchRouter`'s modular design allows for easy expansion - new specialized search agents can be added without modifying the existing code, making the system highly adaptable to evolving search needs.\n",
    "\n",
    "Through this central coordination, the `SearchRouter` ensures efficient and reliable information retrieval across different types of searches while maintaining a consistent interface for the rest of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchRouter:\n",
    "    def __init__(self):\n",
    "        # Get API key directly or from environment variables\n",
    "        # Initialize agents for each search type\n",
    "        self.paper_search_agent = PaperSearchAgent()\n",
    "        self.news_search_agent = NewsSearchAgent()\n",
    "        self.general_search_agent = GeneralSearchAgent()\n",
    "\n",
    "    def route_and_search(self, query_analysis: dict) -> dict:\n",
    "        \"\"\"Routes the search request to appropriate search agent based on query analysis\n",
    "\n",
    "        Args:\n",
    "            query_analysis (dict): Query analysis results from QueryAnalysisAgent\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing search results, including success/failure status and related info\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check search type\n",
    "            search_type = query_analysis.get(\"search_type\")\n",
    "\n",
    "            # Record start time for logging\n",
    "            start_time = datetime.now(pytz.utc)\n",
    "\n",
    "            # Perform search\n",
    "            if search_type == \"research_paper\":\n",
    "                print(\"Performing research paper search...\")\n",
    "                result = self.paper_search_agent.perform_search(query_analysis)\n",
    "            elif search_type == \"news\":\n",
    "                print(\"Performing news search...\")\n",
    "                result = self.news_search_agent.perform_search(query_analysis)\n",
    "            elif search_type == \"general\":\n",
    "                print(\"Performing general search...\")\n",
    "                result = self.general_search_agent.perform_search(query_analysis)\n",
    "            else:\n",
    "                return {\n",
    "                    \"status\": \"error\",\n",
    "                    \"error_message\": f\"Unsupported search type: {search_type}\",\n",
    "                    \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                }\n",
    "\n",
    "            # Calculate search duration\n",
    "            end_time = datetime.now(pytz.utc)\n",
    "            search_duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Add metadata to results\n",
    "            result.update(\n",
    "                {\n",
    "                    \"search_type\": search_type,\n",
    "                    \"search_duration\": search_duration,\n",
    "                    \"search_timestamp\": end_time.isoformat(),\n",
    "                    \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error_message\": str(e),\n",
    "                \"original_query\": query_analysis.get(\"original_query\"),\n",
    "                \"search_type\": query_analysis.get(\"search_type\"),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Agent\n",
    "Crafting User-Friendly Information Delivery\n",
    "\n",
    "The ResponseAgent serves as our system's expert communicator, transforming raw search results into well-structured, readable content that meets users' needs. This agent is particularly crucial as it represents the final step in our information delivery pipeline, ensuring that complex search results are presented in a clear, digestible format.\n",
    "\n",
    "The agent maintains three specialized prompt templates for different types of content:\n",
    "\n",
    "Key Features of the Response Agent:\n",
    "\n",
    "1. Content Customization\n",
    "   - Adapts formatting based on content type (papers, news, general)\n",
    "   - Maintains consistent structure while accommodating different information types\n",
    "   - Ensures appropriate context and explanations are included\n",
    "\n",
    "2. Email Optimization\n",
    "   - Creates clear, professional email subjects\n",
    "   - Structures content for easy scanning and reading\n",
    "   - Includes all necessary context and source information\n",
    "\n",
    "The ResponseAgent represents the crucial final step in our information delivery pipeline, ensuring that users receive not just raw data, but well-organized, contextually relevant information that directly addresses their queries. Through its careful formatting and organization, it helps users quickly understand and act upon the information they've requested.\n",
    "\n",
    "This agent demonstrates how automated systems can maintain a human touch in their communications, making complex information accessible and actionable for end users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseAgent:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "        self.setup_prompts()\n",
    "\n",
    "    def setup_prompts(self):\n",
    "        # Research paper search prompt\n",
    "        self.paper_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following research paper search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Research Paper Search Results for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the organized research paper results for your search.\"\n",
    "               - Number each paper and format as follows:\n",
    "                 1. Paper title: [title]\n",
    "                    - Summary: [core research content and key findings]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # News search prompt\n",
    "        self.news_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following news search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Latest News Updates for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the latest news articles related to your search topic.\"\n",
    "               - Number each news item and format as follows:\n",
    "                 1. [title] - [news source]\n",
    "                    - Main content: [key content summary]\n",
    "                    - Published date: [date]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        # General search prompt\n",
    "        self.general_prompt = PromptTemplate.from_template(\n",
    "            \"\"\"Please organize the following search results in email format.\n",
    "            Search term: {query}\n",
    "            Search results: {results}\n",
    "            \n",
    "            Format as follows:\n",
    "            1. Email subject: \"Search Results for [search term]\"\n",
    "            2. Body:\n",
    "               - Greeting\n",
    "               - \"Here are the organized results for your search.\"\n",
    "               - Number each result and format as follows:\n",
    "                 1. [title]\n",
    "                    - Content: [main content summary]\n",
    "                    - Source: [website or platform name]\n",
    "                    - URL: [link]\n",
    "               - Closing remarks\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def format_results(self, search_results):\n",
    "        try:\n",
    "            # Handle cases with no results or errors\n",
    "            if search_results.get(\"status\") == \"error\":\n",
    "                return {\n",
    "                    \"subject\": \"Search Error Notification\",\n",
    "                    \"body\": f\"An error occurred during search: {search_results.get('error_message', 'Unknown error')}\",\n",
    "                }\n",
    "\n",
    "            # Handle cases with no results\n",
    "            if not search_results.get(\"results\"):\n",
    "                return {\n",
    "                    \"subject\": \"No Search Results\",\n",
    "                    \"body\": f\"No results found for search term '{search_results.get('original_query', '')}'.\",\n",
    "                }\n",
    "\n",
    "            # Select prompt based on search type\n",
    "            search_type = search_results.get(\"search_type\", \"general\")\n",
    "            if search_type == \"research_paper\":\n",
    "                prompt = self.paper_prompt\n",
    "            elif search_type == \"news\":\n",
    "                prompt = self.news_prompt\n",
    "            else:\n",
    "                prompt = self.general_prompt\n",
    "\n",
    "            # Prepare input for result formatting\n",
    "            formatted_input = {\n",
    "                \"query\": search_results.get(\"original_query\", \"\"),\n",
    "                \"results\": json.dumps(\n",
    "                    search_results.get(\"results\", []), ensure_ascii=False, indent=2\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # Generate response through LLM\n",
    "            response = prompt.format(**formatted_input)\n",
    "            response = self.llm.invoke(response)\n",
    "\n",
    "            try:\n",
    "                # Attempt JSON parsing\n",
    "                return json.loads(response.content)\n",
    "            except json.JSONDecodeError:\n",
    "                # Return default format if JSON parsing fails\n",
    "                return {\n",
    "                    \"subject\": f\"Search Results for [{formatted_input['query']}]\",\n",
    "                    \"body\": response.content,\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"subject\": \"Result Processing Error\",\n",
    "                \"body\": f\"An error occurred while processing results: {str(e)}\\n\\nOriginal query: {search_results.get('original_query', '')}\",\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Test Query: recommend place to eat at seoul in 7 pm\n",
      "============================================================\n",
      "\n",
      "1. Query Analysis Results:\n",
      "{\n",
      "  \"target_time\": \"2025-02-08 19:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-08 18:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"general\",\n",
      "  \"keywords\": [\n",
      "    \"recommend\",\n",
      "    \"place\",\n",
      "    \"to\",\n",
      "    \"eat\",\n",
      "    \"at\",\n",
      "    \"seoul\",\n",
      "    \"in\",\n",
      "    \"7\",\n",
      "    \"pm\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"recommend place to eat at seoul in 7 pm\",\n",
      "  \"status\": \"success\"\n",
      "}\n",
      "Performing general search...\n",
      "\n",
      "2. Search Results:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"type\": \"general\",\n",
      "      \"title\": \"['1\",\n",
      "      \"content\": \"['1. Daol Charcoal Grill. (441). Open now · 2. Han-gong-gan. (145). Open now · 3. Bogwangjung Itaewon. (89). Open now · 4. Wolhwa Galbi. (61).', \\\"The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean style pastries and coffee. Also some cafes.\\\", \\\"Folks, let me tell you, when I stumbled upon Seoul's vibrant 24-hour food scene, I thought I'd died and gone to midnight snacking heaven.\\\", 'This place is a 4 minute walk from Hongik station and open 24hours. The short rib gimbap was my favorite food this entire trip and I probably ate there 7 times ...', \\\"Gwangjang Market – This old market is one of the greatest culinary destinations in all of Seoul, and if you love food, there's absolutely no way you're going to ...\\\", 'Myeongdong Kyoja 명동 교자 - serves the most delicious kalguksu in my opinion. The mandu is delicious, too. · Shinseon Seollongtang 신선 설농탕 - ...', \\\"We've collected the most-often-mentioned 50 places from other articles, including favorites like Mingles, Gwangjang Market, and MOSU Seoul.\\\", \\\"I'm even more excited to share my Insider tips on where I think you should go to eat delicious Korean foods.\\\", \\\"Try Star Chef, a fusion place. I think it's in Gangnam. Reviews here from Seoul Eats and ZenKimchi, which are good blogs to find other suggestions.\\\", 'Most hotels and hostels are within walking distance of restaurants. Here are my recommendations though: Itaewon - Foreign food, clubs, and bars.']\",\n",
      "      \"url\": \"\",\n",
      "      \"relevance_score\": 0.7\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"general\",\n",
      "      \"title\": \"[\\\"The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean\",\n",
      "      \"content\": \"[\\\"The only thing you'll find open will be bakeries like Paris Baguette and Tous Les Jours for Korean style pastries and coffee. Also some cafes.\\\", \\\"If you wanna eat like a king, I suggest the Bulgogi Whopper but the one with 3 patties. Many locations are open 24 hours. If you're feeling ...\\\", '1. Daol Charcoal Grill. (441). Open now · 2. Han-gong-gan. (145). Open now · 3. Bogwangjung Itaewon. (89). Open now · 4. Wolhwa Galbi. (61).', 'With favorites like Gwangjang Market, Tosokchon Samgyetang, and Myeongdong Kyoja Main Restaurant and more, get ready to experience the best flavors around ...', \\\"Folks, let me tell you, when I stumbled upon Seoul's vibrant 24-hour food scene, I thought I'd died and gone to midnight snacking heaven.\\\", \\\"We've collected the most-often-mentioned 50 places from other articles, including favorites like Mingles, La Yeon, and Soigné.\\\", \\\"Gwangjang Market – This old market is one of the greatest culinary destinations in all of Seoul, and if you love food, there's absolutely no way you're going to ...\\\", 'Cafe Onion Seongsu manages to balance old and new perfectly, and in doing this, makes for an extremely photogenic hang out. There are multiple ...', 'Traditional Korean soup restaurant specializing in sul lung tang, a beef bone broth soup, with options like beef brisket and a mixed selection excluding tongue.']\",\n",
      "      \"url\": \"\",\n",
      "      \"relevance_score\": 0.5\n",
      "    }\n",
      "  ],\n",
      "  \"total_found\": 2,\n",
      "  \"returned_count\": 2,\n",
      "  \"query_info\": {\n",
      "    \"target_time\": \"2025-02-08 19:00:00+0000\",\n",
      "    \"execution_time\": \"2025-02-08 18:55:00+0000\",\n",
      "    \"task_type\": \"search\",\n",
      "    \"search_type\": \"general\",\n",
      "    \"keywords\": [\n",
      "      \"recommend\",\n",
      "      \"place\",\n",
      "      \"to\",\n",
      "      \"eat\",\n",
      "      \"at\",\n",
      "      \"seoul\",\n",
      "      \"in\",\n",
      "      \"7\",\n",
      "      \"pm\"\n",
      "    ],\n",
      "    \"requirements\": \"minimum 5 results\",\n",
      "    \"time_sensitivity\": \"normal\",\n",
      "    \"original_query\": \"recommend place to eat at seoul in 7 pm\",\n",
      "    \"status\": \"success\"\n",
      "  },\n",
      "  \"search_type\": \"general\",\n",
      "  \"search_duration\": 5.684706,\n",
      "  \"search_timestamp\": \"2025-02-08T05:01:11.843234+00:00\",\n",
      "  \"original_query\": \"recommend place to eat at seoul in 7 pm\"\n",
      "}\n",
      "\n",
      "3. Formatted Results:\n",
      "{\n",
      "  \"subject\": \"Search Results for [recommend place to eat at seoul in 7 pm]\",\n",
      "  \"body\": \"Subject: Search Results for \\\"recommend place to eat at Seoul in 7 pm\\\"\\n\\n---\\n\\nHello,\\n\\nHere are the organized results for your search:\\n\\n1. Daol Charcoal Grill, Han-gong-gan, Bogwangjung Itaewon, Wolhwa Galbi\\n   - Content: Popular dining places open now include Daol Charcoal Grill, Han-gong-gan, Bogwangjung Itaewon, and Wolhwa Galbi. Seoul's vibrant 24-hour food scene offers options beyond regular hours, with places like Gwangjang Market and Myeongdong Kyoja also recommended for their culinary delights.\\n   - Source: General information\\n   - URL: Not provided\\n\\n2. Bakeries and 24-hour dining options\\n   - Content: Besides popular dining places, bakeries like Paris Baguette and Tous Les Jours are open for Korean-style pastries and coffee. For a 24-hour option, consider the Bulgogi Whopper at specific locations. Gwangjang Market and traditional Korean soup restaurants are also notable mentions.\\n   - Source: General information\\n   - URL: Not provided\\n\\nThank you for using our service. Should you have any further questions or need additional information, feel free to reach out.\\n\\nBest regards,\\n\\n[Your Name]\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Test Query: find rag persona paper at 3 pm\n",
      "============================================================\n",
      "\n",
      "1. Query Analysis Results:\n",
      "{\n",
      "  \"target_time\": \"2025-02-08 15:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-08 14:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"research_paper\",\n",
      "  \"keywords\": [\n",
      "    \"rag\",\n",
      "    \"persona\",\n",
      "    \"paper\",\n",
      "    \"3\",\n",
      "    \"pm\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"find rag persona paper at 3 pm\",\n",
      "  \"status\": \"success\"\n",
      "}\n",
      "Performing research paper search...\n",
      "\n",
      "2. Search Results:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging\\n  Narratives\",\n",
      "      \"url\": \"http://arxiv.org/abs/2305.02364v2\",\n",
      "      \"published_date\": \"2023-05-03\",\n",
      "      \"summary\": \"  Sustaining coherent and engaging narratives requires dialogue or storytelling\\nagents to understand how the personas of speakers or listeners ground the\\nnarrative. Specifically, these agents must infer personas of their listeners to\\nproduce statements that cater to their interests. They must also learn to\\nmaintain consistent speaker personas for themselves throughout the narrative,\\nso that their counterparts feel involved in a realistic conversation or story.\\n  However, personas are diverse and complex: they entail large quantities of\\nrich interconnected world knowledge that is challenging to robustly represent\\nin general narrative systems (e.g., a singer is good at singing, and may have\\nattended conservatoire). In this work, we construct a new large-scale persona\\ncommonsense knowledge graph, PeaCoK, containing ~100K human-validated persona\\nfacts. Our knowledge graph schematizes five dimensions of persona knowledge\\nidentified in previous studies of human interactive behaviours, and distils\\nfacts in this schema from both existing commonsense knowledge graphs and\\nlarge-scale pretrained language models. Our analysis indicates that PeaCoK\\ncontains rich and precise world persona inferences that help downstream systems\\ngenerate more consistent and engaging narratives.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"In Defense of RAG in the Era of Long-Context Language Models\",\n",
      "      \"url\": \"http://arxiv.org/abs/2409.01666v1\",\n",
      "      \"published_date\": \"2024-09-03\",\n",
      "      \"summary\": \"  Overcoming the limited context limitations in early-generation LLMs,\\nretrieval-augmented generation (RAG) has been a reliable solution for\\ncontext-based answer generation in the past. Recently, the emergence of\\nlong-context LLMs allows the models to incorporate much longer text sequences,\\nmaking RAG less attractive. Recent studies show that long-context LLMs\\nsignificantly outperform RAG in long-context applications. Unlike the existing\\nworks favoring the long-context LLM over RAG, we argue that the extremely long\\ncontext in LLMs suffers from a diminished focus on relevant information and\\nleads to potential degradation in answer quality. This paper revisits the RAG\\nin long-context answer generation. We propose an order-preserve\\nretrieval-augmented generation (OP-RAG) mechanism, which significantly improves\\nthe performance of RAG for long-context question-answer applications. With\\nOP-RAG, as the number of retrieved chunks increases, the answer quality\\ninitially rises, and then declines, forming an inverted U-shaped curve. There\\nexist sweet points where OP-RAG could achieve higher answer quality with much\\nless tokens than long-context LLM taking the whole context as input. Extensive\\nexperiments on public benchmark demonstrate the superiority of our OP-RAG.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"Speaker Profiling in Multiparty Conversations\",\n",
      "      \"url\": \"http://arxiv.org/abs/2304.08801v2\",\n",
      "      \"published_date\": \"2023-04-18\",\n",
      "      \"summary\": \"  In conversational settings, individuals exhibit unique behaviors, rendering a\\none-size-fits-all approach insufficient for generating responses by dialogue\\nagents. Although past studies have aimed to create personalized dialogue agents\\nusing speaker persona information, they have relied on the assumption that the\\nspeaker's persona is already provided. However, this assumption is not always\\nvalid, especially when it comes to chatbots utilized in industries like\\nbanking, hotel reservations, and airline bookings. This research paper aims to\\nfill this gap by exploring the task of Speaker Profiling in Conversations\\n(SPC). The primary objective of SPC is to produce a summary of persona\\ncharacteristics for each individual speaker present in a dialogue. To\\naccomplish this, we have divided the task into three subtasks: persona\\ndiscovery, persona-type identification, and persona-value extraction. Given a\\ndialogue, the first subtask aims to identify all utterances that contain\\npersona information. Subsequently, the second task evaluates these utterances\\nto identify the type of persona information they contain, while the third\\nsubtask identifies the specific persona values for each identified type. To\\naddress the task of SPC, we have curated a new dataset named SPICE, which comes\\nwith specific labels. We have evaluated various baselines on this dataset and\\nbenchmarked it with a new neural model, SPOT, which we introduce in this paper.\\nFurthermore, we present a comprehensive analysis of SPOT, examining the\\nlimitations of individual modules both quantitatively and qualitatively.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"Personalized Dialogue Generation with Persona-Adaptive Attention\",\n",
      "      \"url\": \"http://arxiv.org/abs/2210.15088v4\",\n",
      "      \"published_date\": \"2022-10-27\",\n",
      "      \"summary\": \"  Persona-based dialogue systems aim to generate consistent responses based on\\nhistorical context and predefined persona. Unlike conventional dialogue\\ngeneration, the persona-based dialogue needs to consider both dialogue context\\nand persona, posing a challenge for coherent training. Specifically, this\\nrequires a delicate weight balance between context and persona. To achieve\\nthat, in this paper, we propose an effective framework with Persona-Adaptive\\nAttention (PAA), which adaptively integrates the weights from the persona and\\ncontext information via our designed attention. In addition, a dynamic masking\\nmechanism is applied to the PAA to not only drop redundant information in\\ncontext and persona but also serve as a regularization mechanism to avoid\\noverfitting. Experimental results demonstrate the superiority of the proposed\\nPAA framework compared to the strong baselines in both automatic and human\\nevaluation. Moreover, the proposed PAA approach can perform equivalently well\\nin a low-resource regime compared to models trained in a full-data setting,\\nwhich achieve a similar result with only 20% to 30% of data compared to the\\nlarger models trained in the full-data setting. To fully exploit the\\neffectiveness of our design, we designed several variants for handling the\\nweighted information in different ways, showing the necessity and sufficiency\\nof our weighting and masking designs.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"research_paper\",\n",
      "      \"title\": \"Improved prompting and process for writing user personas with LLMs,\\n  using qualitative interviews: Capturing behaviour and personality traits of\\n  users\",\n",
      "      \"url\": \"http://arxiv.org/abs/2310.06391v1\",\n",
      "      \"published_date\": \"2023-10-10\",\n",
      "      \"summary\": \"  This draft paper presents a workflow for creating User Personas with Large\\nLanguage Models, using the results of a Thematic Analysis of qualitative\\ninterviews. The proposed workflow uses improved prompting and a larger pool of\\nThemes, compared to previous work conducted by the author for the same task.\\nThis is possible due to the capabilities of a recently released LLM which\\nallows the processing of 16 thousand tokens (GPT3.5-Turbo-16k) and also due to\\nthe possibility to offer a refined prompting for the creation of Personas. The\\npaper offers details of performing Phase 2 and 3 of Thematic Analysis, and then\\ndiscusses the improved workflow for creating Personas. The paper also offers\\nsome reflections on the relationship between the proposed process and existing\\napproaches to Personas such as the data-driven and qualitative Personas.\\nMoreover, the paper offers reflections on the capacity of LLMs to capture user\\nbehaviours and personality traits, from the underlying dataset of qualitative\\ninterviews used for the analysis.\\n\",\n",
      "      \"source\": \"arxiv\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_found\": 5,\n",
      "  \"returned_count\": 5,\n",
      "  \"query_info\": {\n",
      "    \"target_time\": \"2025-02-08 15:00:00+0000\",\n",
      "    \"execution_time\": \"2025-02-08 14:55:00+0000\",\n",
      "    \"task_type\": \"search\",\n",
      "    \"search_type\": \"research_paper\",\n",
      "    \"keywords\": [\n",
      "      \"rag\",\n",
      "      \"persona\",\n",
      "      \"paper\",\n",
      "      \"3\",\n",
      "      \"pm\"\n",
      "    ],\n",
      "    \"requirements\": \"minimum 5 results\",\n",
      "    \"time_sensitivity\": \"normal\",\n",
      "    \"original_query\": \"find rag persona paper at 3 pm\",\n",
      "    \"status\": \"success\"\n",
      "  },\n",
      "  \"search_type\": \"research_paper\",\n",
      "  \"search_duration\": 1.331628,\n",
      "  \"search_timestamp\": \"2025-02-08T05:01:30.714947+00:00\",\n",
      "  \"original_query\": \"find rag persona paper at 3 pm\"\n",
      "}\n",
      "\n",
      "3. Formatted Results:\n",
      "{\n",
      "  \"subject\": \"Search Results for [find rag persona paper at 3 pm]\",\n",
      "  \"body\": \"Subject: Research Paper Search Results for \\\"find rag persona paper at 3 pm\\\"\\n\\nDear [Recipient's Name],\\n\\nHere are the organized research paper results for your search.\\n\\n1. Paper title: \\\"PeaCoK: Persona Commonsense Knowledge for Consistent and Engaging Narratives\\\"\\n   - Summary: This paper explores the construction of a large-scale persona commonsense knowledge graph, PeaCoK, with around 100K human-validated persona facts. It aims to sustain coherent and engaging narratives by enabling dialogue agents to infer personas of listeners and maintain consistent speaker personas. The PeaCoK graph schematizes five dimensions of persona knowledge, which helps in generating more consistent and engaging narratives.\\n   - URL: [PeaCoK Paper](http://arxiv.org/abs/2305.02364v2)\\n\\n2. Paper title: \\\"In Defense of RAG in the Era of Long-Context Language Models\\\"\\n   - Summary: This paper revisits retrieval-augmented generation (RAG) in the context of long-context language models (LLMs). It introduces an order-preserve retrieval-augmented generation (OP-RAG) mechanism that enhances RAG's performance in long-context question-answer applications. The study finds that OP-RAG can achieve higher answer quality with fewer tokens than long-context LLMs.\\n   - URL: [RAG Defense Paper](http://arxiv.org/abs/2409.01666v1)\\n\\n3. Paper title: \\\"Speaker Profiling in Multiparty Conversations\\\"\\n   - Summary: The research explores Speaker Profiling in Conversations (SPC), aiming to summarize persona characteristics for individual speakers in dialogues. It introduces a new dataset, SPICE, and evaluates various baselines with a neural model named SPOT. The paper discusses the tasks of persona discovery, persona-type identification, and persona-value extraction.\\n   - URL: [Speaker Profiling Paper](http://arxiv.org/abs/2304.08801v2)\\n\\n4. Paper title: \\\"Personalized Dialogue Generation with Persona-Adaptive Attention\\\"\\n   - Summary: This paper presents a Persona-Adaptive Attention (PAA) framework for persona-based dialogue systems, which balances weights between context and persona. The framework uses dynamic masking to drop redundant information and prevent overfitting. The PAA approach excels in low-resource settings and performs equivalently well compared to models trained on full data.\\n   - URL: [Persona-Adaptive Attention Paper](http://arxiv.org/abs/2210.15088v4)\\n\\n5. Paper title: \\\"Improved prompting and process for writing user personas with LLMs, using qualitative interviews\\\"\\n   - Summary: The paper proposes a workflow for creating User Personas with Large Language Models, using thematic analysis of qualitative interviews. It improves prompting and expands the pool of themes using a recently released LLM. The paper discusses the workflow's phases and provides reflections on the LLMs' ability to capture user behaviors and personality traits.\\n   - URL: [User Personas with LLMs Paper](http://arxiv.org/abs/2310.06391v1)\\n\\nPlease let me know if there is anything else you need.\\n\\nBest regards,\\n\\n[Your Name]\"\n",
      "}\n",
      "\n",
      "============================================================\n",
      "Test Query: find news us president speech in 7 am\n",
      "============================================================\n",
      "\n",
      "1. Query Analysis Results:\n",
      "{\n",
      "  \"target_time\": \"2025-02-08 07:00:00+0000\",\n",
      "  \"execution_time\": \"2025-02-08 06:55:00+0000\",\n",
      "  \"task_type\": \"search\",\n",
      "  \"search_type\": \"news\",\n",
      "  \"keywords\": [\n",
      "    \"news\",\n",
      "    \"us\",\n",
      "    \"president\",\n",
      "    \"speech\",\n",
      "    \"in\",\n",
      "    \"7\",\n",
      "    \"am\"\n",
      "  ],\n",
      "  \"requirements\": \"minimum 5 results\",\n",
      "  \"time_sensitivity\": \"normal\",\n",
      "  \"original_query\": \"find news us president speech in 7 am\",\n",
      "  \"status\": \"success\"\n",
      "}\n",
      "Performing news search...\n",
      "\n",
      "2. Search Results:\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"title\": \"Pelosi: Trump Not Sophisticated Enough to Understand What Is Happening\",\n",
      "      \"description\": \"On Thursday on MSNBC’s “Andrea Mitchell Reports,” Rep. Nancy Pelosi (D-CA) said President Donald Trump is not sophisticated enough in terms of the intelligence community to understand what is happening when they comply with his executive orders. Pelosi said, …\",\n",
      "      \"url\": \"https://freerepublic.com/focus/f-news/4295616/posts\",\n",
      "      \"published_at\": \"2025-02-07T01:16:20Z\",\n",
      "      \"source\": \"Freerepublic.com\",\n",
      "      \"content\": \"Skip to comments.\\r\\nPelosi: Trump Not Sophisticated Enough to Understand What Is HappeningBreitbart ^\\r\\n | 02/06/2025\\r\\n | Pam Key\\r\\nPosted on 02/06/2025 5:16:20 PM PST by ChicagoConservative27\\r\\nOn Thurs… [+5045 chars]\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Share Market Highlights 7 February 2025: Sensex, Nifty settle lower after RBI cuts repo rate to 6.25%; FIIs continue selling spree\",\n",
      "      \"description\": \"Sensex, Nifty updates on 7 February 2025: Equity markets closed lower on Wednesday despite the Reserve Bank of India’s first rate cut since May 2020, as investors remained concerned about global trade tensions and continued foreign institutional investor (FII…\",\n",
      "      \"url\": \"https://www.thehindubusinessline.com/markets/stock-market-updates-february-7-2025/article69188765.ece\",\n",
      "      \"published_at\": \"2025-02-07T01:15:20Z\",\n",
      "      \"source\": \"BusinessLine\",\n",
      "      \"content\": \"<li></li>\\r\\nFebruary 07, 2025 17:07Stock market live updates today: Markets end lower as RBIs rate cut fails to impress; FIIs continue selling spree \\r\\nEquity markets closed lower on Wednesday despite … [+141894 chars]\"\n",
      "    }\n",
      "  ],\n",
      "  \"total_results\": 2,\n",
      "  \"returned_count\": 2,\n",
      "  \"search_parameters\": {\n",
      "    \"keywords\": [\n",
      "      \"news\",\n",
      "      \"us\",\n",
      "      \"president\",\n",
      "      \"speech\",\n",
      "      \"in\",\n",
      "      \"7\",\n",
      "      \"am\"\n",
      "    ],\n",
      "    \"from_date\": \"2025-02-07\",\n",
      "    \"language\": \"en\"\n",
      "  },\n",
      "  \"search_type\": \"news\",\n",
      "  \"search_duration\": 0.462204,\n",
      "  \"search_timestamp\": \"2025-02-08T05:01:55.353650+00:00\",\n",
      "  \"original_query\": \"find news us president speech in 7 am\"\n",
      "}\n",
      "\n",
      "3. Formatted Results:\n",
      "{\n",
      "  \"subject\": \"Search Results for [find news us president speech in 7 am]\",\n",
      "  \"body\": \"Subject: Latest News Updates for \\\"find news us president speech in 7 am\\\"\\n\\n---\\n\\nHello,\\n\\nHere are the latest news articles related to your search topic.\\n\\n1. Pelosi: Trump Not Sophisticated Enough to Understand What Is Happening - Freerepublic.com  \\n   - Main content: Rep. Nancy Pelosi commented on President Donald Trump's understanding of the intelligence community and his executive orders, suggesting he lacks the sophistication required to fully grasp the implications.\\n   - Published date: February 7, 2025\\n   - URL: [https://freerepublic.com/focus/f-news/4295616/posts](https://freerepublic.com/focus/f-news/4295616/posts)\\n\\n2. Share Market Highlights 7 February 2025: Sensex, Nifty settle lower after RBI cuts repo rate to 6.25%; FIIs continue selling spree - BusinessLine  \\n   - Main content: Despite the Reserve Bank of India's rate cut, the Sensex and Nifty closed lower, with investor concerns over global trade tensions and ongoing foreign institutional investor selling.\\n   - Published date: February 7, 2025\\n   - URL: [https://www.thehindubusinessline.com/markets/stock-market-updates-february-7-2025/article69188765.ece](https://www.thehindubusinessline.com/markets/stock-market-updates-february-7-2025/article69188765.ece)\\n\\nThank you for using our service. If you have any other search queries, feel free to reach out.\\n\\nBest regards,  \\n[Your Name]\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def test_search_system():\n",
    "    query_analyzer = QueryAnalysisAgent()\n",
    "    search_router = SearchRouter()\n",
    "    response_agent = ResponseAgent()  # Added\n",
    "\n",
    "    test_queries = [\n",
    "        \"recommend place to eat at seoul in 7 pm\",\n",
    "        \"find rag persona paper at 3 pm\",\n",
    "        \"find news us president speech in 7 am\",\n",
    "    ]\n",
    "\n",
    "    for query in test_queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Test Query: {query}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        try:\n",
    "            query_analysis = query_analyzer.analyze_query(query)\n",
    "            print(\"\\n1. Query Analysis Results:\")\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    query_analysis,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                    default=datetime_handler,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if query_analysis[\"status\"] != \"success\":\n",
    "                print(\"Query analysis failed!\")\n",
    "                continue\n",
    "\n",
    "            search_results = search_router.route_and_search(query_analysis)\n",
    "            print(\"\\n2. Search Results:\")\n",
    "            print(\n",
    "                json.dumps(\n",
    "                    search_results,\n",
    "                    indent=2,\n",
    "                    ensure_ascii=False,\n",
    "                    default=datetime_handler,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Added: Result formatting\n",
    "            print(\"\\n3. Formatted Results:\")\n",
    "            formatted_results = response_agent.format_results(search_results)\n",
    "            print(json.dumps(formatted_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during test: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_search_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduling System and Email Service\n",
    "\n",
    "The `ScheduledSearchSystem` manages the complete lifecycle of search tasks, from scheduling to result delivery. Here's its core structure and functionality:\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. Collection Management\n",
    "2. Task Scheduling\n",
    "3. Search Execution\n",
    "4. Email Delivery\n",
    "\n",
    "\n",
    "The system uses threading for non-blocking operation and includes comprehensive logging for monitoring task progress and debugging issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "import yagmail\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "\n",
    "class ScheduledSearchSystem:\n",
    "    def __init__(self, email_config: Dict[str, Any]):\n",
    "        print(f\"\\n[{self._get_current_time()}] Initializing system...\")\n",
    "        self.query_analyzer = QueryAnalysisAgent()\n",
    "        self.search_router = SearchRouter()\n",
    "        self.response_agent = ResponseAgent()\n",
    "        self.client = chromadb.PersistentClient(path=\"./search_data\")\n",
    "\n",
    "        # Email configuration\n",
    "        self.email_config = email_config\n",
    "        self.yag = yagmail.SMTP(email_config[\"username\"], email_config[\"password\"])\n",
    "        print(f\"[{self._get_current_time()}] Email client configuration complete\")\n",
    "\n",
    "        self.scheduled_tasks = {}\n",
    "        self.setup_collections()\n",
    "\n",
    "        # Add completion flag\n",
    "        self.is_completed = False\n",
    "        self.completion_event = threading.Event()\n",
    "\n",
    "        # Start scheduler\n",
    "        self.scheduler_thread = threading.Thread(target=self._run_scheduler)\n",
    "        self.scheduler_thread.daemon = True\n",
    "        self.scheduler_thread.start()\n",
    "        print(f\"[{self._get_current_time()}] System initialization complete\\n\")\n",
    "\n",
    "    def _get_current_time(self):\n",
    "        \"\"\"Return current time as string\"\"\"\n",
    "        return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    def setup_collections(self):\n",
    "        \"\"\"Set up ChromaDB collections\"\"\"\n",
    "        print(f\"[{self._get_current_time()}] Starting ChromaDB collection setup...\")\n",
    "        self.results_collection = self.client.get_or_create_collection(\n",
    "            name=\"search_results\", metadata={\"description\": \"Raw search results\"}\n",
    "        )\n",
    "\n",
    "        self.formatted_collection = self.client.get_or_create_collection(\n",
    "            name=\"formatted_responses\",\n",
    "            metadata={\"description\": \"Formatted responses for email delivery\"},\n",
    "        )\n",
    "\n",
    "        self.schedule_collection = self.client.get_or_create_collection(\n",
    "            name=\"scheduled_tasks\",\n",
    "            metadata={\"description\": \"Scheduled search and email tasks\"},\n",
    "        )\n",
    "        print(f\"[{self._get_current_time()}] ChromaDB collection setup complete\")\n",
    "\n",
    "    def schedule_task(self, query: str, user_email: str) -> Dict[str, Any]:\n",
    "        \"\"\"Schedule a task\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n[{self._get_current_time()}] Starting new task scheduling...\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Email: {user_email}\")\n",
    "\n",
    "            # Query analysis\n",
    "            query_analysis = self.query_analyzer.analyze_query(query)\n",
    "            execution_time = query_analysis[\"execution_time\"]\n",
    "            target_time = query_analysis[\"target_time\"]\n",
    "\n",
    "            print(f\"Scheduled search execution time: {execution_time}\")\n",
    "            print(f\"Scheduled email delivery time: {target_time}\")\n",
    "\n",
    "            # Generate task ID\n",
    "            schedule_id = f\"task_{datetime.now(pytz.UTC).timestamp()}\"\n",
    "            print(f\"Generated task ID: {schedule_id}\")\n",
    "\n",
    "            # Save task information\n",
    "            task_info = {\n",
    "                \"query\": query,\n",
    "                \"email\": user_email,\n",
    "                \"execution_time\": execution_time.isoformat(),\n",
    "                \"target_time\": target_time.isoformat(),\n",
    "                \"search_type\": query_analysis[\"search_type\"],\n",
    "                \"status\": \"scheduled\",\n",
    "            }\n",
    "\n",
    "            self.scheduled_tasks[schedule_id] = task_info\n",
    "\n",
    "            # Save to ChromaDB\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] Saving task information to ChromaDB...\"\n",
    "            )\n",
    "            self.schedule_collection.add(\n",
    "                documents=[json.dumps(task_info)],\n",
    "                metadatas=[{\"type\": \"schedule\", \"status\": \"pending\"}],\n",
    "                ids=[schedule_id],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Task information saved\")\n",
    "\n",
    "            # Schedule search execution\n",
    "            execution_time_str = execution_time.strftime(\"%H:%M\")\n",
    "            schedule.every().day.at(execution_time_str).do(\n",
    "                self.execute_search, schedule_id=schedule_id\n",
    "            ).tag(schedule_id)\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Search task scheduling complete\")\n",
    "\n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"message\": \"Task successfully scheduled\",\n",
    "                \"schedule_id\": schedule_id,\n",
    "                \"execution_time\": execution_time,\n",
    "                \"target_time\": target_time,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Task scheduling failed: {str(e)}\")\n",
    "            return {\"status\": \"error\", \"error_message\": str(e)}\n",
    "\n",
    "    def execute_search(self, schedule_id: str) -> bool:\n",
    "        \"\"\"Execute search\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\n[{self._get_current_time()}] Starting search execution (ID: {schedule_id})\"\n",
    "            )\n",
    "\n",
    "            task_info = self.scheduled_tasks.get(schedule_id)\n",
    "            if not task_info:\n",
    "                print(f\"[{self._get_current_time()}] Task information not found\")\n",
    "                return False\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Analyzing search query...\")\n",
    "            query_analysis = self.query_analyzer.analyze_query(task_info[\"query\"])\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Performing search...\")\n",
    "            search_results = self.search_router.route_and_search(query_analysis)\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Formatting search results...\")\n",
    "            formatted_response = self.response_agent.format_results(search_results)\n",
    "\n",
    "            # Save results\n",
    "            print(f\"[{self._get_current_time()}] Saving search results to ChromaDB...\")\n",
    "            response_id = f\"response_{schedule_id}\"\n",
    "            self.formatted_collection.add(\n",
    "                documents=[json.dumps(formatted_response)],\n",
    "                metadatas=[\n",
    "                    {\n",
    "                        \"schedule_id\": schedule_id,\n",
    "                        \"email\": task_info[\"email\"],\n",
    "                        \"target_time\": task_info[\"target_time\"],\n",
    "                    }\n",
    "                ],\n",
    "                ids=[response_id],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Search results saved\")\n",
    "\n",
    "            # Schedule email delivery\n",
    "            target_time = datetime.fromisoformat(task_info[\"target_time\"])\n",
    "            target_time_str = target_time.strftime(\"%H:%M\")\n",
    "\n",
    "            schedule.every().day.at(target_time_str).do(\n",
    "                self.send_email, schedule_id=schedule_id\n",
    "            ).tag(f\"email_{schedule_id}\")\n",
    "\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] Email delivery scheduled (Time: {target_time_str})\"\n",
    "            )\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Search execution failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def send_email(self, schedule_id: str) -> bool:\n",
    "        \"\"\"Send email\"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f\"\\n[{self._get_current_time()}] Starting email delivery (ID: {schedule_id})\"\n",
    "            )\n",
    "\n",
    "            response_id = f\"response_{schedule_id}\"\n",
    "            print(f\"[{self._get_current_time()}] Retrieving saved search results...\")\n",
    "            response_results = self.formatted_collection.get(ids=[response_id])\n",
    "\n",
    "            if not response_results[\"documents\"]:\n",
    "                print(f\"[{self._get_current_time()}] Search results not found\")\n",
    "                return False\n",
    "\n",
    "            formatted_response = json.loads(response_results[\"documents\"][0])\n",
    "            metadata = response_results[\"metadatas\"][0]\n",
    "\n",
    "            print(f\"[{self._get_current_time()}] Sending email...\")\n",
    "            print(f\"Recipient: {metadata['email']}\")\n",
    "            print(f\"Subject: {formatted_response['subject']}\")\n",
    "\n",
    "            self.yag.send(\n",
    "                to=metadata[\"email\"],\n",
    "                subject=formatted_response[\"subject\"],\n",
    "                contents=formatted_response[\"body\"],\n",
    "            )\n",
    "            print(f\"[{self._get_current_time()}] Email sent successfully\")\n",
    "\n",
    "            # Update task status\n",
    "            print(f\"[{self._get_current_time()}] Updating task status...\")\n",
    "            task_info = self.scheduled_tasks[schedule_id]\n",
    "            task_info[\"status\"] = \"completed\"\n",
    "            self.schedule_collection.update(\n",
    "                documents=[json.dumps(task_info)], ids=[schedule_id]\n",
    "            )\n",
    "\n",
    "            # Clear schedule\n",
    "            schedule.clear(f\"email_{schedule_id}\")\n",
    "            print(f\"[{self._get_current_time()}] Task completion processing complete\\n\")\n",
    "\n",
    "            # Set completion flag\n",
    "            self.is_completed = True\n",
    "            self.completion_event.set()\n",
    "            print(\n",
    "                f\"[{self._get_current_time()}] All tasks completed. Shutting down system.\\n\"\n",
    "            )\n",
    "\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[{self._get_current_time()}] Email delivery failed: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _run_scheduler(self):\n",
    "        \"\"\"Run scheduler\"\"\"\n",
    "        print(f\"[{self._get_current_time()}] Scheduler started...\")\n",
    "        while not self.is_completed:\n",
    "            schedule.run_pending()\n",
    "            time.sleep(1)  # Check every second\n",
    "\n",
    "    def wait_for_completion(self, timeout=None):\n",
    "        \"\"\"Wait for task completion\"\"\"\n",
    "        try:\n",
    "            completed = self.completion_event.wait(timeout=timeout)\n",
    "            if not completed:\n",
    "                print(f\"[{self._get_current_time()}] Task completion timeout\")\n",
    "            if hasattr(self, \"yag\"):\n",
    "                self.yag.close()\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n[{self._get_current_time()}] Terminated by user.\")\n",
    "            if hasattr(self, \"yag\"):\n",
    "                self.yag.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agent Scheduler System Usage Guide\n",
    "The work starts 5 minutes before the work request time.\n",
    "\n",
    "1. Email Configuration\n",
    "- Enable Gmail 2-Step Verification\n",
    "- Generate App Password: https://myaccount.google.com/security > 2-Step Verification > App passwords\n",
    "```python\n",
    "email_config = {\n",
    "    \"username\": \"your_email@gmail.com\",\n",
    "    \"password\": \"your_app_password\",  # Gmail app password\n",
    "}\n",
    "```\n",
    "\n",
    "2. Initialize System and Schedule Task\n",
    "```python\n",
    "# Initialize system\n",
    "system = ScheduledSearchSystem(email_config)\n",
    "\n",
    "# Schedule task\n",
    "result = system.schedule_task(\n",
    "    query=\"find AI papers at 9 AM\",  # Search query to execute\n",
    "    user_email=\"your_email@gmail.com\"  # Email to receive results\n",
    ")\n",
    "```\n",
    "\n",
    "3. Wait for Completion\n",
    "```python\n",
    "# Wait for task completion (max 4 hours)\n",
    "system.wait_for_completion(timeout=14400)\n",
    "```\n",
    "\n",
    "Search results will be automatically emailed to the specified address upon completion.\n",
    "\n",
    "The system supports various query types:\n",
    "- Research papers: \"find RAG papers at 7 AM\"\n",
    "- News: \"find AI news at 9 AM\"\n",
    "- General search: \"find restaurants in Seoul at 6 PM\"\n",
    "\n",
    "\n",
    "### Note\n",
    "You need to get app password from google account.\n",
    "- [How to get Google password](https://support.google.com/accounts/answer/185833?visit_id=638745290390245053-2925662375&p=InvalidSecondFactor&rd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Scheduling System Test ===\n",
      "\n",
      "\n",
      "[2025-02-08 14:08:50] Initializing system...\n",
      "[2025-02-08 14:08:50] Email client configuration complete\n",
      "[2025-02-08 14:08:50] Starting ChromaDB collection setup...\n",
      "[2025-02-08 14:08:50] ChromaDB collection setup complete\n",
      "[2025-02-08 14:08:50] Scheduler started...\n",
      "[2025-02-08 14:08:50] System initialization complete\n",
      "\n",
      "\n",
      "[2025-02-08 14:08:50] Starting new task scheduling...\n",
      "Query: find Modular RAG paper at 14:15 PM\n",
      "Email: jik9210@gmail.com\n",
      "Scheduled search execution time: 2025-02-08 14:10:00+00:00\n",
      "Scheduled email delivery time: 2025-02-08 14:15:00+00:00\n",
      "Generated task ID: task_1738991333.292786\n",
      "[2025-02-08 14:08:53] Saving task information to ChromaDB...\n",
      "[2025-02-08 14:08:53] Task information saved\n",
      "[2025-02-08 14:08:53] Search task scheduling complete\n",
      "\n",
      "=== Scheduling Result ===\n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"message\": \"Task successfully scheduled\",\n",
      "  \"schedule_id\": \"task_1738991333.292786\",\n",
      "  \"execution_time\": \"2025-02-08 14:10:00+00:00\",\n",
      "  \"target_time\": \"2025-02-08 14:15:00+00:00\"\n",
      "}\n",
      "\n",
      "=== Task will execute at scheduled time... ===\n",
      "\n",
      "\n",
      "[2025-02-08 14:10:00] Starting search execution (ID: task_1738991333.292786)\n",
      "[2025-02-08 14:10:00] Analyzing search query...\n",
      "[2025-02-08 14:10:03] Performing search...\n",
      "Performing research paper search...\n",
      "[2025-02-08 14:10:04] Formatting search results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: response_task_1738991333.292786\n",
      "Add of existing embedding ID: response_task_1738991333.292786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-08 14:10:18] Saving search results to ChromaDB...\n",
      "[2025-02-08 14:10:18] Search results saved\n",
      "[2025-02-08 14:10:18] Email delivery scheduled (Time: 14:15)\n",
      "\n",
      "[2025-02-08 14:15:00] Starting email delivery (ID: task_1738991333.292786)\n",
      "[2025-02-08 14:15:00] Retrieving saved search results...\n",
      "[2025-02-08 14:15:00] Sending email...\n",
      "Recipient: jik9210@gmail.com\n",
      "Subject: Search Results for [find Modular RAG paper at 14:15 PM]\n"
     ]
    }
   ],
   "source": [
    "# System configuration\n",
    "email_config = {\n",
    "    \"username\": \"@gmail.com\",\n",
    "    \"password\": \"\",\n",
    "}\n",
    "\n",
    "print(\"\\n=== Starting Scheduling System Test ===\\n\")\n",
    "\n",
    "# Initialize system\n",
    "system = ScheduledSearchSystem(email_config)\n",
    "\n",
    "# Schedule task\n",
    "result = system.schedule_task(\n",
    "    query=\"find Modular RAG paper at 14:15 PM\",  # Example: 7:45 PM\n",
    "    user_email=\"@gmail.com\",\n",
    ")\n",
    "\n",
    "print(\"\\n=== Scheduling Result ===\")\n",
    "print(json.dumps(result, indent=2, default=str))\n",
    "print(\"\\n=== Task will execute at scheduled time... ===\\n\")\n",
    "\n",
    "# Wait for task completion (maximum 4 hours)\n",
    "system.wait_for_completion(timeout=14400)  # 4 hours = 14400 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
